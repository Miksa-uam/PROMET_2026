I want to make forest plots that represent the likelihood of achieving 10% weight loss by weight gain causes using risk ratios. 
The pipeline should be built as a single, self-contained script at best relying on existing helpers (for config, fdr corrections or statistical comparisons)
that uses the timetoevent_wgc_compl table to stratify patients by weight gain causes, calculates the proportion of each cause group where 10%_wl_achieved = 1, calculates risk ratios based on these proportions and then plots them on forest plots based on the implementation plan below. 

### Implementation Plan: Forest Plot of Risk Ratios for 10% Weight Loss

**Objective**: To create a forest plot visualizing the risk ratio (and 95% confidence interval) of achieving >10% weight loss for patients with each specific Weight Gain Cause (WGC) compared to patients without that cause.

#### Step 1: Data Preparation (Input: Raw Data)
1.  **Define your cohort**: Start with your complete dataset of all patients included in the analysis.
2.  **Create the Outcome Variable**: Generate a binary column, let's call it `Achieved_10_Percent_WL`, where `1` = Yes and `0` = No.
3.  **Identify Predictor Variables**: Ensure you have a set of binary columns, one for each of the 12 WGCs (e.g., `WGC_Mental_Health`, `WGC_Physical_Inactivity`), where `1` = cause is present and `0` = cause is absent.

#### Step 2: Iterative Calculation and Statistical Testing
You will now loop through each of your 12 WGCs. For each WGC (e.g., for `WGC_Mental_Health`):

1.  **Create a 2x2 Contingency Table**: From your raw data, generate a table that cross-tabulates the WGC with the outcome:

| | Achieved >10% WL | Did Not Achieve >10% WL |
| :--- | :--- | :--- |
| **WGC Present** | a | b |
| **WGC Absent** | c | d |

2.  **Calculate Risks**:
    *   Risk in "Present" Group = `a / (a + b)`
    *   Risk in "Absent" Group (Reference) = `c / (c + d)`

3.  **Calculate the Risk Ratio (RR)**:
    *   `RR = (Risk in "Present" Group) / (Risk in "Absent" Group)`

4.  **Calculate the 95% Confidence Interval (CI)**:
    *   Calculate the standard error of the log risk ratio: `SE(log(RR)) = sqrt( (1/a) - (1/(a+b)) + (1/c) - (1/(c+d)) )`
    *   Calculate the CI on the log scale: `log(RR) Â± 1.96 * SE(log(RR))`
    *   Convert the CI back to the original scale by taking the exponential of the lower and upper bounds.

5.  **Perform a Statistical Test**: On the same 2x2 table, run a **Chi-squared test of independence**. If any cell (`a`, `b`, `c`, or `d`) has a count less than 5, use a **Fisher's exact test** instead. Store the p-value.

6: Adjust for Multiple Comparisons**:
    *   Take the list of 12 p-values obtained from the statistical tests for each WGC.
    *   Apply the Benjamini-Hochberg (FDR) correction to this list to generate a new list of "adjusted p-values" or "q-values."


After iterating through all 12 WGCs, you will have a summary table with the WGC name, its RR, the lower CI bound, the upper CI bound, and the p-value.

#### Step 3: Visualization (The Forest Plot)
1.  **Set up the Plot**: Use a plotting library like `matplotlib` or `seaborn` in Python, or `ggplot2` in R.
2.  **Y-axis**: List the names of the 12 WGCs.
3.  **X-axis**: This will represent the Risk Ratio. **Crucially, set the x-axis to a logarithmic scale.** This is standard practice for ratio measures, as it makes effects of the same magnitude (e.g., RR=0.5 and RR=2.0) appear equidistant from the center.[1]
4.  **Plot the Points and Intervals**: For each WGC, plot a point (e.g., a square) at its RR value. Draw a horizontal line through the point spanning from the lower to the upper CI bound.
5.  **Add the Line of No Effect**: Draw a solid vertical line at `RR = 1.0`. This is the most important reference on the plot.
6.  **Annotate Significance**: You can add asterisks next to the WGC names based on the p-values calculated in Step 2 (e.g., `*` for p<0.05).



Now that you have a general idea of my expectations (and can also see an AI-generated image of how the result could, approximately, look like),
please first ideate whether the best approach is to load raw data into the pipeline or to use the existing stratified comparison tables? 
Take time to understand the working directory structure, and that of the research i'm doing here. 
Most importantly, evaluate which scripts, functions, modules that already exist in the directory can be used in creating this pipeline. 