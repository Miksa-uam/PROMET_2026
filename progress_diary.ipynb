{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APRIL 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- comparing only baseline emotional with baseline weight - bigger sample size than in longitudinal\n",
    "- also, linking baseline emotional with dropping out, and whole program completion (percentage of completers, weight loss at completion)\n",
    "- use frequency of measurements and number of visits as indicators of adherence\n",
    "- 10% WL vs dropout as competing risks (events that prevent the outcome of interest) in a survival model (Fine-Gray model)\n",
    "- making correlation plot between the emotional variables themselves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### musts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- rowcleaned measurements has to go into the first paper sql pipeline, duplicates have to be removed \n",
    "    - this can be done in the subsetted SQL too. Dilemma: universally good to have row-cleaning, duplicate and outlier removal universally. But, identifying duplicates and outliers in the data frames other than measurements is an effort, and time loss as of mid april 2025. (*i know this is not the right attitude, but this is the leaner one*)\n",
    "- when checking if a baseline/final measurement is within the range of a medical record, don't look whether it is within/without the start and end dates. Rather, look if it is within a 10-15-day window of both dates. Sometimes the first measurement is months after the start of a prescription, and that can't be taken as a reliable baseline value. Identifying cases like this can also help spotting loopholes in the analysis pipeline. \n",
    "- correcting for GDPR\n",
    "- when using genetics and considering personalized recommendations, correcting for dates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18/4/25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[8.10] what i wanted to ask copilot about which revised (excel ideation) SA input data structure should be better for me:\n",
    "\n",
    "*the table contents* \n",
    "\n",
    "OG:\n",
    "\n",
    "patient_id  medical_record_id  baseline_date  end_date  followup_period baseline_weight  fil_weight  weight_loss_kg  weight_loss_pct  10pct_achieved  dropout  sex  age  height_m  baseline_bmi  hunger  satiety  emotiol_eating  emotiol_eating_value  quantity_control  impulse_control \n",
    "str str datetime datetime nr nr nr nr nr bool bool str, could be bool nr nr nr bool bool bool nr nr nr\n",
    "pacID mrID first mes last mes as of: days from first to last mes, as of described in D first mes weight, from measurements, last mes weight, as of fil weight as of fil weight within window if target &/ followthrough not achieved gender years from medical record from medical record, I think, maybe it should be fetched together with the measurement? bool bool bool likert likert likert\n",
    "last of window but maybe just out of curiosity, as of described in D\n",
    "last recorded fetch from records as well to see if there is mismatch?\n",
    "date of first over-target\n",
    "\n",
    "ALT:\n",
    "\n",
    "pID mID baseline_date last_aval_date total_followup baseline_weight last_aval_weight total_wl_kg total_wl_% Nd_weight wl_Nd_kg wl_Nd_% Nd_date days_to_Nd_measurement Nd_dropout X%_achieved X%wl% X%_wl_date days_to_X%_wl sex  age  height_m  baseline_bmi  hunger  satiety  emotiol_eating  emotiol_eating_value  quantity_control  impulse_control \n",
    "str str datetime datetime nr nr nr nr nr nr nr nr datetime nr bool bool nr datetime nr str, could be bool nr nr nr bool bool bool nr nr nr\n",
    "pacID mrID first mes last available mes as of: days from first to last mes first mes weight last aval mes weight as of last aval mes as of last aval mes weight at N days, for example, 40 weight loss at N days weight loss at N days exact date of measurement closest to N-day window, eg. 40 exact days until N-day measurement, eg. 40 is/isn't a dropout as of N days (for example 40) X% target weight loss achieved or not, for example 5% exact WL% of first over-target measurement date of first over-target measurement days from baseline to first over-target measurement other clinical factors to consider -- >\n",
    "the absolute last one of the patient-record group, even if followup is 2y\n",
    "\n",
    "EX WIDE:\n",
    "\n",
    "pID mID baseline_date last_aval_date total_followup baseline_weight last_aval_weight total_wl_kg total_wl_% 40d_weight wl_40d_kg wl_40d_% 40d_date days_to_40d_measurement 40d_dropout 60d_weight wl_60d_kg wl_60d_% 60d_date days_to_60d_measurement 60d_dropout 80d_weight wl_80d_kg wl_80d_% 80d_date days_to_80d_measurement 80d_dropout 5%_achieved 5%wl% 5%_wl_date days_to_5%_wl 10%_achieved 10%wl% 10%_wl_date days_to_10%_wl 15%_achieved 15%wl% 15%_wl_date days_to_15%_wl sex  age  height_m  baseline_bmi  hunger  satiety  emotiol_eating  emotiol_eating_value  quantity_control  impulse_control \n",
    "example1 id1 1-Jan 31-Dec 365 100 75 25 25 90 10 10 9-Feb 40 0 80 20 20 2-Feb 59 0 75 25 25 23-Mar 82 0 1 6 20-Jan 20 1 10 9-Feb 40 1 17 19-Feb 50 m 40 1,76 32 n n y 7 8 9\n",
    "example2 id2 1-Jan 30-Jan 30 100 93 7 7 1 1 1 1 5 20-Jan 20 0 1 d 60 1,54 42 n n y 1 7 7\n",
    "\n",
    "EXS LONGS:\n",
    "\n",
    "40d5p\n",
    "pID mID baseline_date last_aval_date total_followup baseline_weight last_aval_weight total_wl_kg total_wl_% 40d_weight wl_40d_kg wl_40d_% 40d_date days_to_40d_measurement 40d_dropout 5%_achieved 5%wl% 5%_wl_date days_to_5%_wl sex  age  height_m  baseline_bmi  hunger  satiety  emotiol_eating  emotiol_eating_value  quantity_control  impulse_control \n",
    "example1 id1 1-Jan 31-Dec 365 100 75 25 25 90 10 10 9-Feb 40 0 1 6 20-Jan 20 m 40 1,76 32 n n y 7 8 9\n",
    "example2 id2 1-Jan 30-Jan 30 100 93 7 7 1 1 5 20-Jan 20 d 60 1,54 42 n n y 1 7 7\n",
    "\n",
    "40d10p\n",
    "pID mID baseline_date last_aval_date total_followup baseline_weight last_aval_weight total_wl_kg total_wl_% 40d_weight wl_40d_kg wl_40d_% 40d_date days_to_40d_measurement 40d_dropout 10%_achieved 10%wl% 10%_wl_date days_to_10%_wl sex  age  height_m  baseline_bmi  hunger  satiety  emotiol_eating  emotiol_eating_value  quantity_control  impulse_control \n",
    "example1 id1 1-Jan 31-Dec 365 100 75 25 25 90 10 10 9-Feb 40 0 1 10 9-Feb 40 m 40 1,76 32 n n y 7 8 9\n",
    "example2 id2 1-Jan 30-Jan 30 100 93 7 7 1 0 d 60 1,54 42 n n y 1 7 7\n",
    "\n",
    "40d15p\n",
    "pID mID baseline_date last_aval_date total_followup baseline_weight last_aval_weight total_wl_kg total_wl_% 40d_weight wl_40d_kg wl_40d_% 40d_date days_to_40d_measurement 40d_dropout 15%_achieved 15%wl% 15%_wl_date days_to_15%_wl sex  age  height_m  baseline_bmi  hunger  satiety  emotiol_eating  emotiol_eating_value  quantity_control  impulse_control \n",
    "example1 id1 1-Jan 31-Dec 365 100 75 25 25 90 10 10 9-Feb 40 0 1 17 19-Feb 50 m 40 1,76 32 n n y 7 8 9\n",
    "example2 id2 1-Jan 30-Jan 30 100 93 7 7 1 1 d 60 1,54 42 n n y 1 7 7\n",
    "\n",
    "60d5p\n",
    "pID mID baseline_date last_aval_date total_followup baseline_weight last_aval_weight total_wl_kg total_wl_% 60d_weight wl_60d_kg wl_60d_% 60d_date days_to_60d_measurement 60d_dropout 5%_achieved 5%wl% 5%_wl_date days_to_5%_wl sex  age  height_m  baseline_bmi  hunger  satiety  emotiol_eating  emotiol_eating_value  quantity_control  impulse_control \n",
    "example1 id1 1-Jan 31-Dec 365 100 75 25 25 80 20 20 2-Feb 59 0 1 6 20-Jan 20 m 40 1,76 32 n n y 7 8 9\n",
    "example2 id2 1-Jan 30-Jan 30 100 93 7 7 1 1 5 20-Jan 20 d 60 1,54 42 n n y 1 7 7\n",
    "\n",
    "*a description*\n",
    "\n",
    " The idea is basically including BOTH the target weight loss and time window information in the same data unit. This unit can then be structured in a long format, or can be split to wide formats in an analysis-specific way.\n",
    "\n",
    "What do you think, which format is better for achieving the 4-question-based research goal we discussed earlier?\n",
    "Additionally, is there anything missing or not ideal about this structure for that proposed research report?\n",
    "\n",
    "*an important detail*\n",
    "\n",
    "There is a subtle but important distinction - that I was not using the wide and long expressions properly. \n",
    "\n",
    "Technically, both of the proposed layouts are wide, having one row per patient. However, the first one contains all target and window case observations in the same data frame, whereas the second layout separates the distinct target-window cases into distinct tables, much like the current SQL implementations. \n",
    "\n",
    "Practically, when executing the data analysis pipeline, using one vs the other would mean subsetting a large data frame for each scenario vs. using distinct data frames for each scenario. \n",
    "\n",
    "Considering this clarification, please revise your answer on which one should be the optimal choice for my use case. \n",
    "\n",
    "Also, comment on whether I am missing something potentially important, or misleading in this structure. \n",
    "\n",
    "[17.30] AI-gen data analysis plan for emotional values on weight loss success\n",
    "\n",
    "Proposed Analysis: Investigating the Impact of Baseline Eating Behaviors on Weight Loss Success and Dropout\n",
    "\n",
    "1. Research Questions:\n",
    "\n",
    "RQ1: Are baseline self-reported eating behaviors (hunger, satiety, emotional eating, quantity control, impulse control) associated with the magnitude of weight loss (e.g., % weight loss) at specific time points (e.g., ~60 days, ~80 days)?\n",
    "RQ2: Do baseline self-reported eating behaviors predict the likelihood of achieving clinically significant weight loss targets (e.g., 5%, 10%, 15%) by specific time points?\n",
    "RQ3: Are baseline self-reported eating behaviors associated with the time it takes to achieve weight loss targets?\n",
    "RQ4: Do baseline self-reported eating behaviors predict the likelihood of dropping out of the program before reaching specific time points?\n",
    "2. Primary Datasets for Analysis:\n",
    "\n",
    "Use the survival analysis datasets generated (e.g., sa_40d_5p, sa_60d_10p, sa_80d_15p, etc.). These tables are ideal because they:\n",
    "Focus on the first medical record per patient (addressing the multiple interventions issue for this specific analysis).\n",
    "Contain key outcome variables calculated for survival analysis:\n",
    "followup_period (time variable)\n",
    "X%_achieved (event status for achieving target)\n",
    "dropout (event status for dropout)\n",
    "wl% (magnitude of weight loss at the end measurement)\n",
    "Include the merged baseline characteristics and eating behavior variables.\n",
    "3. Key Variables:\n",
    "\n",
    "Outcomes:\n",
    "wl% (Continuous, for RQ1)\n",
    "X%_achieved (Binary, for RQ2 - e.g., 10%_achieved)\n",
    "followup_period & X%_achieved (Time-to-event, for RQ3)\n",
    "followup_period & dropout (Time-to-event, for RQ4)\n",
    "Predictors (Baseline Eating Behaviors):\n",
    "hunger (Categorical: yes/no)\n",
    "satiety (Categorical: yes/no)\n",
    "emotional_eating (Categorical: yes/no)\n",
    "emotional_eating_value (Numeric/Ordinal)\n",
    "quantity_control (Numeric/Ordinal)\n",
    "impulse_control (Numeric/Ordinal)\n",
    "Covariates (Baseline):\n",
    "age\n",
    "sex\n",
    "baseline_bmi (or baseline_weight and height_m)\n",
    "4. Analysis Steps:\n",
    "\n",
    "(Preprocessing)\n",
    "\n",
    "Select one or more primary analysis datasets (e.g., focus on sa_60d_10p first).\n",
    "Handle missing data in predictors/covariates (e.g., listwise deletion for simplicity initially, or consider imputation if significant).\n",
    "Convert categorical 'yes'/'no' predictors to 0/1 for regression models.\n",
    "(Descriptive Statistics)\n",
    "\n",
    "Summarize baseline characteristics (age, sex, BMI) and eating behavior variables for the cohort(s).\n",
    "Report overall weight loss outcomes (mean wl%, % achieving target, % dropout) for the chosen time point(s).\n",
    "(RQ1 & RQ2 - Associations at Specific Time Points)\n",
    "\n",
    "Bivariate:\n",
    "Compare mean wl% between groups based on categorical eating behaviors (e.g., emotional_eating yes vs. no) using t-tests.\n",
    "Correlate numeric eating behavior scores (emotional_eating_value, etc.) with wl% using Pearson correlation.\n",
    "Compare the proportion achieving the target (X%_achieved) between groups based on categorical eating behaviors using Chi-square tests.\n",
    "Compare mean numeric eating behavior scores between those who achieved the target and those who didn't using t-tests.\n",
    "Multivariable Regression:\n",
    "Linear Regression: Predict wl% using eating behavior variables, adjusting for age, sex, baseline_bmi.\n",
    "Logistic Regression: Predict X%_achieved (binary outcome) using eating behavior variables, adjusting for age, sex, baseline_bmi. (Note: Define the \"non-achieved\" group carefully - typically those who completed the period without achieving the target, excluding dropouts, or model dropout separately).\n",
    "(RQ3 & RQ4 - Time-to-Event Analysis)\n",
    "\n",
    "Define Events/Censoring:\n",
    "Time-to-Target: Event = X%_achieved is True. Time = followup_period. Censoring occurs if dropout is True OR if X%_achieved is False at the end of followup_period.\n",
    "Time-to-Dropout: Event = dropout is True. Time = followup_period. Censoring occurs if dropout is False at the end of followup_period.\n",
    "Kaplan-Meier Analysis:\n",
    "Plot survival curves (e.g., probability of not achieving target over time) stratified by key eating behavior variables (e.g., emotional_eating yes/no, high/low emotional_eating_value). Compare curves using log-rank tests.\n",
    "Plot cumulative incidence curves for dropout stratified by eating behavior variables. Compare using appropriate tests (e.g., Gray's test if competing risks are considered, though simpler log-rank might suffice initially).\n",
    "Cox Proportional Hazards Regression:\n",
    "Model 1: Hazard of achieving target. Predictors: Eating behavior variables + covariates. Report Hazard Ratios (HR).\n",
    "Model 2: Hazard of dropout. Predictors: Eating behavior variables + covariates. Report Hazard Ratios (HR).\n",
    "5. Interpretation & Next Steps:\n",
    "\n",
    "Summarize which, if any, baseline eating behaviors are significantly associated with weight loss magnitude, likelihood of success, time-to-success, and dropout risk, after adjusting for baseline factors.\n",
    "Discuss the clinical relevance of the findings.\n",
    "Acknowledge limitations: self-report bias, specific definitions used for time windows/outcomes, focus on the first intervention only, potential unmeasured confounders.\n",
    "Suggest further analyses: exploring interactions, incorporating genetics (linking alleles_clean_pivoted), analyzing subsequent interventions (using medical_records_complete and more complex modeling).\n",
    "\n",
    "Building a solid, step-by-step analysis plan is crucial, especially when aiming for more advanced techniques later. Let's break down your recap and questions.\n",
    "\n",
    "Recap Confirmation:\n",
    "\n",
    "Yes, your understanding seems spot on. The idea is to leverage the cleaned and structured data (especially medical_records_complete, measurements_with_metadata, and the derived survival analysis datasets like sa_60d_10p, etc.) to:\n",
    "\n",
    "Describe the Cohort: Characterize the patients who have complete eating behavior data, focusing on demographics, baseline anthropometrics, and their self-reported eating behaviors.\n",
    "Explore Associations: Investigate relationships between baseline eating behavior variables (both categorical like 'emotional_eating' and continuous like 'emotional_eating_value') and weight loss outcomes.\n",
    "Survival Analysis: Model the \"time-to-event,\" where the event is achieving a specific weight loss target (e.g., 5%, 10%, 15%). This allows comparing the rate at which different groups (e.g., based on eating behavior scores or demographics) achieve success, while accounting for varying follow-up times and dropouts (censoring).\n",
    "Kaplan-Meier: Visualize survival curves for different groups (e.g., emotional eaters vs. non-eaters) and use log-rank tests to see if the curves are statistically different.\n",
    "Cox Proportional Hazards: Build regression models to identify which baseline factors (including eating behaviors, age, sex, baseline BMI, etc.) are significant predictors of hazard (instantaneous risk/rate) of achieving the weight loss target. This quantifies the impact of each factor while adjusting for others.\n",
    "Foundation for Advanced Methods: The structure created for Cox models naturally lends itself to extensions like competing risks (if dropout is considered a competing event) or survival machine learning (using algorithms like Random Survival Forests to potentially find more complex interactions or improve prediction).\n",
    "Now, for your questions:\n",
    "\n",
    "1. Time and Effort Estimation (AI-Human Collaboration):\n",
    "\n",
    "This is always tricky to estimate precisely, but here's a breakdown based on the current state and the proposed analyses:\n",
    "\n",
    "Data Already Wrangled: You've done a massive amount of the heavy lifting already. Standardizing, cleaning, linking measurements, and creating the medical_records_complete and survival_analysis.sqlite database are significant achievements. This saves a huge amount of time.\n",
    "Survival Data Preparation: The code generating the survival_analysis.sqlite database looks quite robust and handles the core logic (defining baseline, time windows, success/censoring). Minor tweaks or validation might be needed, but the foundation is there. (Effort: Low-Medium, maybe a few hours to a day for thorough validation and potential adjustments).\n",
    "Basic Survival Analysis (Kaplan-Meier, Univariate Cox): With the data prepared, generating KM curves and running simple Cox models for individual predictors is relatively straightforward using libraries like lifelines or statsmodels/scikit-survival. The AI can generate the boilerplate code quickly. The main effort will be in:\n",
    "Defining the exact groups for KM curves.\n",
    "Interpreting the output (log-rank p-values, hazard ratios, confidence intervals).\n",
    "Creating publication-quality plots.\n",
    "(Effort: Medium, likely several days of focused work, iterating with the AI, interpreting results).\n",
    "Multivariate Cox Models: Building models with multiple predictors requires more thought about variable selection, checking assumptions (proportional hazards), and interpreting adjusted hazard ratios. The AI can help with code, but the statistical thinking and interpretation are key user tasks. (Effort: Medium-High, add another few days to a week, depending on complexity and number of models).\n",
    "Advanced Analyses (Competing Risks, Survival ML): This significantly increases complexity. Data structuring might need changes, and the conceptual/statistical understanding required is higher. The AI can assist with code implementation, but the user needs to guide the process strongly. (Effort: High, could add weeks, depending on depth).\n",
    "Report Writing & Visualization: Integrating results, creating tables, and writing the narrative takes time.\n",
    "Overall Estimate: For the core survival analysis (KM, Cox) leading to an elegant first report for your supervisor, assuming the current data structure holds up well:\n",
    "\n",
    "Optimistic: If everything flows smoothly with minimal debugging and clear interpretation, perhaps 1-2 weeks of focused effort (mixing coding/analysis days).\n",
    "Realistic: Accounting for inevitable debugging, data exploration detours, refining plots, and writing, 2-4 weeks is a more likely timeframe for a polished result.\n",
    "The AI collaboration significantly speeds up the coding part, but the analysis, interpretation, and validation parts still require substantial human effort and time.\n",
    "\n",
    "2. Workflow Recommendation (Current vs. New Notebook / Agentic vs. Ask):\n",
    "\n",
    "Based on the complexity and the iterative nature of analysis:\n",
    "\n",
    "Recommendation: Use new, focused notebooks loading cleaned data, primarily using the Copilot \"Ask\" mode.\n",
    "Why New Notebooks?\n",
    "Clarity & Focus: Your current notebook is excellent for wrangling but is becoming very long. A new notebook (e.g., paper1_survival_analysis.ipynb) dedicated only to the survival analysis for this specific paper keeps things clean and manageable.\n",
    "Performance: Avoids re-running all the wrangling steps. You can directly load the necessary tables from survival_analysis.sqlite or emotional_all_notna.sqlite.\n",
    "Isolation: Reduces the risk of accidentally modifying earlier cleaning code or dealing with namespace conflicts.\n",
    "Why Load Cleaned Data? Your SQLite databases (emotional_all_notna.sqlite, survival_analysis.sqlite) are perfect for this. Start your new analysis notebook by connecting to the relevant database and loading the specific tables (e.g., sa_60d_10p, medical_records_complete if needed for extra variables not in the SA tables).\n",
    "Why Primarily \"Ask\" Mode?\n",
    "Control: Analysis is iterative. You'll want to generate a plot, look at it, tweak it, run a model, check assumptions, modify it. \"Ask\" mode is better for these smaller, controlled steps and for debugging specific lines.\n",
    "Context: It works well within the existing code context of your notebook cell.\n",
    "Reliability: Generally more reliable for generating targeted code snippets or explaining existing ones.\n",
    "When to Consider Agentic (\"Generate\") Mode?\n",
    "Potentially useful for generating a first draft of a larger, self-contained analysis block in a new notebook if you have a very clear, detailed prompt (e.g., \"Using the dataframe sa_60d_10p, generate Python code with the lifelines library to: 1. Create a KaplanMeierFitter. 2. Fit the fitter using 'followup_period' as duration and '10pct_achieved' as event_observed. 3. Plot the overall survival curve. 4. Fit separate curves stratified by the 'sex' column. 5. Plot the stratified curves. 6. Perform a log-rank test comparing the curves stratified by 'sex'.\").\n",
    "Caveat: Be prepared to carefully review, debug, and potentially refactor the code generated by agentic mode. It might make assumptions or structure the code in a way you want to change.\n",
    "Additional Recommendations:\n",
    "\n",
    "Version Control (Git): Strongly consider using Git for your project directory. It allows you to track changes, revert if something breaks, and manage different analysis branches. Even working alone, it's invaluable.\n",
    "Save Intermediate Results: Continue saving key derived dataframes (like your survival analysis tables) to the SQLite database or separate files (.csv, .parquet). This makes reloading for specific tasks much faster.\n",
    "Modular Functions: Keep using functions for reusable analysis steps (e.g., a function to plot KM curves, a function to run and summarize a Cox model). This makes your analysis notebooks cleaner and easier to maintain.\n",
    "Start Simple: Begin with the core KM plots and univariate Cox models for your key eating behavior variables. Get those working and interpreted well before moving to complex multivariate models.\n",
    "In summary: you're in a great position. Leverage the cleaned data, start a fresh notebook for the survival analysis, primarily use \"Ask\" mode for iterative development, and be prepared for the human-intensive parts of interpretation and validation. Good luck!\n",
    "\n",
    "\n",
    "[7.50]\n",
    "- N day cutoff: datetime/NA\n",
    "- X % WL: number/NA\n",
    "\n",
    "can it be 1 table? or is that too chaotic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17/4/25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[8.10] this is the last question I wanted to ask copilot about which data frame structure is better for me: \n",
    "\n",
    "[**problems for future self**]\n",
    "Issue with the revised code: average calculations\n",
    "- averages are not calculated using a fixed time point, but the last measurement of everyone, which currently is defined by the achievement of target weight loss. So if someone has 10 measurements, but achieves target on the 7th, the last 3 measurements are excluded, and the 7th will be seen as the last one. However, this makes it look like the patient lost less weight than he actually did. \n",
    "- this made me think that maybe, additional columns should be included in the dataframe - like the first measurement, the first when target is achieved, and the last one. \n",
    "- this will create opportunities for better average calculations, logistic regression design, and whatnot, so it might be cool to add\n",
    "- but it will also create NA values, so I may want to use a fresher brain when thinking this through\n",
    "- below is the first ideation with AI about this\n",
    "- also, the new code does not contain number of measurements data!\n",
    "- [**and there is another problem:**] it would be important to do at least exploratory analyses \n",
    "    - we might be actually losing time by doing all this\n",
    "    - maybe a quick SA test run would help the efforts of seeing whether this is a right direction or not - tuesday is coming on!\n",
    "\n",
    "I'm thinking. I don't want to keep very different data in my dataset, this is very good for survival analysis looking at time-to-event - however, it is also interesting to see the exact weight lost at day X, the cutoff day.\n",
    "\n",
    "Would it be a good idea to include more results in the output datasets, that for the one part, can serve for accurate average calculations, and for the other, can be the basis of other types of analyses, like a logistic regression of having reached X days of followup, or X% weight loss on that followup length?\n",
    "\n",
    "Is this a good idea, to increase the complexity of this SQL, or should I actually create new tables, that contain different types of variables.\n",
    "\n",
    "\n",
    "GitHub Copilot\n",
    "Okay, let's break down this thought process. You're correctly identifying a nuance in survival analysis vs. fixed-timepoint analysis.\n",
    "\n",
    "Your Current Setup (Survival Focus):\n",
    "\n",
    "sa_... tables: Primarily designed for time-to-event.\n",
    "followup_period: Represents the time until the event (achieving X% weight loss) or the time until censoring (dropout or reaching the end of the observation window without success).\n",
    "weight_loss_pct: Represents the weight loss at the end_date, which corresponds to the event or censoring time.\n",
    "{target}%_achieved: Indicates if the event occurred within the observation period.\n",
    "Your New Requirement (Fixed-Timepoint Focus):\n",
    "\n",
    "You want to know the actual weight loss percentage specifically at the defined cutoff window (e.g., 60 +/- 10 days) for everyone who had a measurement within that window.\n",
    "This is useful for calculating accurate average weight loss at that specific time point and for other analyses like logistic regression (e.g., predicting target%_achieved based on baseline characteristics and the actual weight loss at day 60).\n",
    "Analysis & Recommendation:\n",
    "\n",
    "Is it a good idea? Yes, absolutely. Capturing the actual weight loss at the defined cutoff window provides valuable complementary information to the time-to-event data. It allows for different types of analysis and gives a clearer picture of the cohort's state at that specific time, regardless of when individuals might have hit the target before that window.\n",
    "\n",
    "Modify Existing Tables vs. Create New Tables?\n",
    "\n",
    "Argument for Modifying Existing Tables (sa_...):\n",
    "\n",
    "Cohesion: Keeps all information related to a specific analysis scenario (e.g., 10% target at 60 days) together in one place.\n",
    "Simplicity: Avoids doubling the number of tables in your database (9 -> 18).\n",
    "Direct Comparison: Makes it easy to compare the survival outcome (target%_achieved, followup_period) with the fixed-timepoint outcome (actual %WL at cutoff) for the same patients within the same table.\n",
    "Manageable Complexity: Adding a couple of columns is less complex to manage downstream than joining separate tables frequently.\n",
    "Argument for Creating New Tables (e.g., fixed_...):\n",
    "\n",
    "Purity: Keeps the sa_... tables strictly focused on survival metrics.\n",
    "Clarity: Might make the purpose of each table type slightly clearer initially.\n",
    "Recommendation: Modify the existing sa_... tables. The benefits of keeping related analysis data together and avoiding table proliferation outweigh the slight increase in table width. It aligns better with wanting to potentially use both types of information for related analyses (like your logistic regression idea).\n",
    "\n",
    "How to Implement (Conceptual):\n",
    "\n",
    "Inside your calculate_weight_loss_outcome function, after you've determined the baseline_row, measurements_within_window, and measurements_around_cutoff:\n",
    "\n",
    "Find the Measurement at Cutoff: Identify the last measurement that falls within the strict cutoff window (min_window_date to max_window_date). Let's call this cutoff_measurement_row.\n",
    "Calculate Actual WL% at Cutoff:\n",
    "If cutoff_measurement_row exists:\n",
    "Calculate actual_wl_pct_at_cutoff = ((baseline_weight - cutoff_measurement_row['weight_kg']) / baseline_weight) * 100\n",
    "Store cutoff_measurement_date = cutoff_measurement_row['measurement_date']\n",
    "If cutoff_measurement_row does not exist (patient dropped out before the window or had no measurement in the window):\n",
    "Set actual_wl_pct_at_cutoff = None (or NaN)\n",
    "Set cutoff_measurement_date = None (or NaT)\n",
    "Add to Results: Include actual_wl_pct_at_cutoff and cutoff_measurement_date in the result dictionary that gets appended to the results list.\n",
    "Code Snippet Idea (Inside calculate_weight_loss_outcome loop):\n",
    "\n",
    "Then, ensure these new columns are handled correctly when saving to the database in generate_survival_analysis_datasets.\n",
    "\n",
    "This approach gives you the best of both worlds within a single, manageable set of tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[19.10] was my daily progress really just revising where I was yesterday? \n",
    "- No, I actually did important code revisions, \n",
    "- identified important errors and gained a better understanding of my code\n",
    "- and probably, by the end of the day, got some nice ideas on how to best structure the dataframe, allowing for other analyses too!\n",
    "- i hope that today did good for the pipeline design, and that it will bear fruit\n",
    "\n",
    "[7.55] continuation of yesterday's SA debugging thread\n",
    "- days_to_final_measurement only counts BETWEEN days and not accounting for actual start and end date\n",
    "- incorrect dropout definition (only 0-days are considered for now)\n",
    "- target achievment % precision problems\n",
    "-> see all in the last cell from 16/4/25, correct the code accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**corrected, revised pseudocode for survival analysis input generation**]\n",
    "\n",
    "Define analysis parameters outside the functions\n",
    "\n",
    "weight_loss_targets = [10, 15]  # Weight loss target percentages to analyze\n",
    "time_windows = [60, 90, 120]    # Time windows in days to analyze\n",
    "window_span = 10               # Permissible span around target windows (+/- days)\n",
    "\n",
    "1 Data Loading\n",
    "FUNCTION load_measurements(connection):\n",
    "    Query all measurements\n",
    "    Convert dates to datetime\n",
    "    Return measurements dataframe\n",
    "\n",
    "FUNCTION load_medical_records(connection):\n",
    "    Query medical records data\n",
    "    Return medical records dataframe\n",
    "\n",
    "2 Data Preparation  \n",
    "FUNCTION prepare_patient_data(measurements, medical_records):\n",
    "    Identify earliest medical record for each patient\n",
    "    Filter measurements to only include measurements associated with the earliest medical record for each patient\n",
    "    For each patient:\n",
    "        Extract baseline data (first measurement)\n",
    "        Add demographic and behavior data from medical records\n",
    "    Return prepared patient dataframe\n",
    "\n",
    "3 Analysis Core\n",
    "FUNCTION calculate_weight_loss_outcome(patient_data, weight_loss_target, window_center, window_span):\n",
    "    Create empty results list\n",
    "    \n",
    "    For each patient:\n",
    "        Get baseline values\n",
    "        baseline_date = patient's first measurement date\n",
    "        baseline_weight = patient's first measurement weight\n",
    "        \n",
    "        Define time windows\n",
    "        min_window_date = baseline_date + (window_center - window_span) days\n",
    "        max_window_date = baseline_date + (window_center + window_span) days\n",
    "        \n",
    "        Check for measurements in window\n",
    "        measurements_within_window = get measurements between baseline and max_window_date\n",
    "        measurements_around_cutoff = get measurements between min_window_date and max_window_date\n",
    "        \n",
    "        Determine outcome based on survival analysis principles\n",
    "        IF no measurements after baseline:\n",
    "            Instant dropout\n",
    "            success = false\n",
    "            end_date = baseline_date\n",
    "            weight_loss = 0\n",
    "            dropout = true\n",
    "        ELSE:\n",
    "            Check if target achieved at any point\n",
    "            target_achieved = false\n",
    "            first_success_measurement = null\n",
    "            \n",
    "            FOR each measurement in measurements_within_window:\n",
    "                weight_loss = calculate percentage loss from baseline\n",
    "                IF ROUND(weight_loss, 2) >= weight_loss_target:\n",
    "                    target_achieved = true\n",
    "                    first_success_measurement = current measurement\n",
    "                    BREAK\n",
    "            \n",
    "            Determine final measurement using proper survival analysis principles\n",
    "            IF target_achieved:\n",
    "                Success case: use the measurement where target was first achieved\n",
    "                final_measurement = first_success_measurement\n",
    "                success = true\n",
    "                dropout = false\n",
    "            ELSE IF measurements_around_cutoff exist:\n",
    "                No success but followed through window: find measurement closest to center\n",
    "                target_date = baseline_date + window_center days\n",
    "                \n",
    "                Calculate distance to center for each measurement\n",
    "                FOR each measurement in measurements_around_cutoff:\n",
    "                    distance_to_center = abs(measurement_date - target_date)\n",
    "                \n",
    "                Find measurement with minimum distance to center\n",
    "                closest_measurement = measurement with minimum distance_to_center\n",
    "                final_measurement = closest_measurement\n",
    "                success = false\n",
    "                dropout = false\n",
    "            ELSE IF some followup exists but didn't reach window:\n",
    "                Delayed dropout: use the last available followup measurement\n",
    "                final_measurement = last measurement in measurements_within_window\n",
    "                success = false\n",
    "                dropout = true\n",
    "            \n",
    "            Calculate days properly (inclusive of both first and last date)\n",
    "            days_between = (final_measurement_date - baseline_date).days + 1\n",
    "            weight_loss = calculate percentage loss using final_measurement\n",
    "        \n",
    "        Add to results\n",
    "        Add patient outcome to results list\n",
    "    \n",
    "    Return results dataframe\n",
    "\n",
    "4 Main Workflow\n",
    "FUNCTION generate_survival_analysis_datasets(connection, weight_loss_targets, time_windows, window_span=10):\n",
    "    1 Load data once\n",
    "    measurements = load_measurements(connection)\n",
    "    medical_records = load_medical_records(connection)\n",
    "    \n",
    "    2 Prepare data once\n",
    "    prepared_data = prepare_patient_data(measurements, medical_records)\n",
    "    \n",
    "    3 Create results for each target/window combination\n",
    "    results = {}\n",
    "    summary = empty dataframe\n",
    "    \n",
    "    FOR each weight_loss_target in weight_loss_targets:\n",
    "        FOR each window in time_windows:\n",
    "            name = f\"sa_{window}d_{target}p\"\n",
    "            \n",
    "            Run the analysis\n",
    "            results[name] = calculate_weight_loss_outcome(\n",
    "                prepared_data, \n",
    "                weight_loss_target, \n",
    "                window, \n",
    "                window_span=window_span\n",
    "            )\n",
    "            \n",
    "            Add to summary\n",
    "            Add summary row with result stats\n",
    "    \n",
    "    4 Save results\n",
    "    Save results to database\n",
    "    \n",
    "    Return results, summary\n",
    "\n",
    "\n",
    "===VERY SIMPLY:===\n",
    "\n",
    "define analysis parameters\n",
    "load and prepare data\n",
    "    open measurements, open medical records, use only first medical record per patient\n",
    "analysis\n",
    "    get baseline measurements\n",
    "    define time windows and and measurement-window relationships\n",
    "    determine treatment outcome for survival analysis\n",
    "        check for instant dropout\n",
    "        else, check if target was achieved\n",
    "            determine final measurement\n",
    "                if target was achieved, take that measurement as final\n",
    "                elif there's a near-cutoff measurement, take that\n",
    "                elif, take the last measurement\n",
    "            calculate days passed and weight lost\n",
    "        update and reeturn resutls\n",
    "main\n",
    "    load and prepare data\n",
    "    create results for each target-window combination\n",
    "        name and run analyses\n",
    "        add to summary\n",
    "    return and save results and summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16/4/25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**problems for future self:**] keep debugging SA data preparation. Find AI summary of progress, issues, proposals and next steps: \n",
    "\n",
    "Problems for Future Self: Survival Analysis Code Restructuring\n",
    "Core Issues Identified\n",
    "After reviewing the weight loss analysis code that creates survival analysis input files, we found several issues:\n",
    "\n",
    "- Single Responsibility Violation: The current function does too many things at once (data loading, processing, calculations)\n",
    "- Poor Separation of Concerns: Database operations are mixed with analysis logic\n",
    "- Redundant Calculations: Baseline BMI is extracted separately rather than during the main processing\n",
    "- Complex Conditional Logic: The nested if/else structure is difficult to follow\n",
    "- Inconsistent Dropout Definition: Only patients with zero follow-up are flagged as dropouts, but proper definition should include those who neither achieve target weight loss nor complete the follow-up period\n",
    "- Floating-Point Comparison Issues: Weight loss target achievement has precision problems (e.g., 14.87% accepted as 15% but 14.61% not)\n",
    "\n",
    "Original Code Structure (Pseudocode)\n",
    "\n",
    "FUNCTION analyze_weight_loss(database_connection, weight_loss_target, time_window_center, time_window_span):\n",
    "\n",
    "    1. Query all measurements from database\n",
    "    2. Query medical record data from database\n",
    "    3. Convert dates to datetime format\n",
    "    \n",
    "    4. Filter to only include earliest medical record for each patient\n",
    "    \n",
    "    5. Extract baseline BMI separately by:\n",
    "       - Group by patient\n",
    "       - Take first record for each patient\n",
    "       - Create new dataframe with just patient_id and BMI\n",
    "    \n",
    "    6. For each (patient, medical_record) group:\n",
    "       a. Get baseline measurement (first measurement date and weight)\n",
    "       b. Calculate time window boundaries (center Â± span)\n",
    "       c. Find all measurements between baseline and window end\n",
    "       d. Initialize variables (end_date, final_weight, etc.)\n",
    "       \n",
    "       e. IF patient has no measurements after baseline:\n",
    "          - Set end_date = baseline_date (zero follow-up)\n",
    "          - Mark as dropout\n",
    "       ELSE:\n",
    "          - Check all measurements to see if target weight loss is achieved\n",
    "          - Find measurement in target window (or closest to it)\n",
    "          - Calculate weight loss percentage\n",
    "          - Update variables\n",
    "          \n",
    "       f. Add this patient's data to results list\n",
    "    \n",
    "    7. Convert results list to DataFrame\n",
    "    8. Merge with medical records data\n",
    "    9. Merge with baseline BMI data\n",
    "    \n",
    "    10. Return final dataframe\n",
    "\n",
    "Improved Code Structure (Pseudocode)\n",
    "\n",
    "1. Data Loading\n",
    "FUNCTION load_measurements(connection):\n",
    "    Query all measurements\n",
    "    Convert dates to datetime\n",
    "    Return measurements dataframe\n",
    "\n",
    "FUNCTION load_medical_records(connection):\n",
    "    Query medical records data\n",
    "    Return medical records dataframe\n",
    "\n",
    "2. Data Preparation  \n",
    "FUNCTION prepare_patient_data(measurements, medical_records):\n",
    "    Identify earliest medical record for each patient\n",
    "    Filter measurements to only include those medical records\n",
    "    For each patient:\n",
    "        Extract baseline data (first measurement)\n",
    "        Add demographic and behavior data from medical records\n",
    "    Return prepared patient dataframe\n",
    "\n",
    "3. Analysis Core\n",
    "FUNCTION calculate_weight_loss_outcome(patient_data, weight_loss_target, window_center, window_span):\n",
    "    Create empty results list\n",
    "    \n",
    "    For each patient:\n",
    "        Get baseline values\n",
    "        baseline_date = patient's first measurement date\n",
    "        baseline_weight = patient's first measurement weight\n",
    "        \n",
    "        Define time windows\n",
    "        min_window_date = baseline_date + (window_center - window_span) days\n",
    "        max_window_date = baseline_date + (window_center + window_span) days\n",
    "        \n",
    "        Check for measurements in window\n",
    "        measurements_within_window = get measurements between baseline and max_window_date\n",
    "        measurements_around_cutoff = get measurements between min_window_date and max_window_date\n",
    "        \n",
    "        Determine outcome based on survival analysis principles\n",
    "        IF no measurements after baseline:\n",
    "            Instant dropout\n",
    "            success = false\n",
    "            end_date = baseline_date\n",
    "            weight_loss = 0\n",
    "            dropout = true\n",
    "        ELSE:\n",
    "            Check if target achieved at any point\n",
    "            target_achieved = false\n",
    "            first_success_measurement = null\n",
    "            \n",
    "            FOR each measurement in measurements_within_window:\n",
    "                weight_loss = calculate percentage loss from baseline\n",
    "                IF ROUND(weight_loss, 2) >= weight_loss_target:\n",
    "                    target_achieved = true\n",
    "                    first_success_measurement = current measurement\n",
    "                    BREAK\n",
    "            \n",
    "            Determine final measurement using proper survival analysis principles\n",
    "            IF target_achieved:\n",
    "                # Success case: use the measurement where target was first achieved\n",
    "                final_measurement = first_success_measurement\n",
    "                success = true\n",
    "                dropout = false\n",
    "            ELSE IF measurements_around_cutoff exist:\n",
    "                # No success but followed through window: use measurement closest to center\n",
    "                final_measurement = measurement CLOSEST TO window_center in measurements_around_cutoff\n",
    "                success = false\n",
    "                dropout = false\n",
    "            ELSE IF some followup exists but didn't reach window:\n",
    "                # Delayed dropout: use the last available followup measurement\n",
    "                final_measurement = last measurement in measurements_within_window\n",
    "                success = false\n",
    "                dropout = true\n",
    "            \n",
    "            Calculate days properly (inclusive of both first and last date)\n",
    "            days_between = (final_measurement_date - baseline_date).days + 1\n",
    "            weight_loss = calculate percentage loss using final_measurement\n",
    "            \n",
    "            Determine if patient is a dropout (revised definition)\n",
    "            dropout = (NOT success) AND (days_between < (window_center - window_span))\n",
    "        \n",
    "        Add to results\n",
    "        Add patient outcome to results list\n",
    "    \n",
    "    Return results dataframe\n",
    "\n",
    "4. Main Workflow\n",
    "FUNCTION generate_survival_analysis_datasets(connection, weight_loss_targets, time_windows):\n",
    "    1. Load data once\n",
    "    measurements = load_measurements(connection)\n",
    "    medical_records = load_medical_records(connection)\n",
    "    \n",
    "    2. Prepare data once\n",
    "    prepared_data = prepare_patient_data(measurements, medical_records)\n",
    "    \n",
    "     3. Create results for each target/window combination\n",
    "    results = {}\n",
    "    summary = empty dataframe\n",
    "    \n",
    "    FOR each weight_loss_target in weight_loss_targets:\n",
    "        FOR each window in time_windows:\n",
    "            name = f\"sa_{window}d_{target}p\"\n",
    "            \n",
    "            Run the analysis\n",
    "            results[name] = calculate_weight_loss_outcome(\n",
    "                prepared_data, \n",
    "                weight_loss_target, \n",
    "                window, \n",
    "                window_span=10\n",
    "            )\n",
    "            \n",
    "            Add to summary\n",
    "            Add summary row with result stats\n",
    "    \n",
    "    4. Save results\n",
    "    Save results to database\n",
    "    \n",
    "    Return results, summary\n",
    "    \n",
    "Progress Made & Next Steps\n",
    "You've made excellent progress today! You've:\n",
    "\n",
    "- Identified key issues in the survival analysis codebase\n",
    "- Designed a much cleaner architecture with proper separation of concerns\n",
    "- Improved the dropout definition for more accurate analysis\n",
    "- Fixed the precision issues with weight loss calculations\n",
    "\n",
    "Tomorrow, you'll be ready to implement these improvements and have a much more reliable and maintainable codebase. You're very close to being able to run robust survival analyses that will reveal meaningful patterns in your weight loss data.\n",
    "\n",
    "The restructured code will make it easier to extend your analysis (e.g., adding new predictors or trying different time windows) and will produce more trustworthy results. You're setting yourself up for success with the actual statistical modeling in your next steps!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[15.30] continuing\n",
    "\n",
    "[11.10 **problems for future self:**]\n",
    "- revise and comment the modules that complete medical records, and create the survival analysis input\n",
    "- hunt down the issues noted at 9.05 \n",
    "\n",
    "[9.05] issues:  \n",
    "- *medical records complete is smaller than original medical records, look into it - SOLVED - data is lost because any medical record with no measurement associated to it is dropped*\n",
    "- *measurements_with_metadata probably has irrelevant columns at the end - SOLVED - actually, that data can be relevant in better identifying the relationship of a given measurement to the prescription it was linked to, so it is kept with naming updates*\n",
    "- dropout handling: only those with 0 days of followup are considered dropouts, while dropout should be someone who does not achieve nor success nor target measurement time SA INPUT DF\n",
    "- 14% WL as True for 15% SA INPUT DF\n",
    "- column order, bmi localization, table order in SQL in the survival analysis\n",
    "\n",
    "\n",
    "\n",
    "[8.05] commit message: \n",
    "\n",
    "Updated the way that the Paper 1-specific SQL is created. After subsetting the medical_records table, all other tables were subsetted by patient_id only. Prescriptions however contained medical records from some patients that were lacking emotional data. \n",
    "\n",
    "The updated code is supposed to filter prescriptions by both patient and medical record IDs, really only keeping records with complete emotional data availability. \n",
    "\n",
    "\n",
    "number of prescriptions when using the old code, that filters prescriptions by patient ID only: 16937\n",
    "number when using the new code that filters both by patient and medical record IDs: 10190\n",
    "\n",
    "started at 7.40 today\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15/4/25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**problems for future self:**] 18% of medical record-derived data in the lates SA input tables is missing. That means that the medical records for those patients are not available, and that there is no consistency regarding the records contained in the study-specific SQL. That probably makes me have to look several steps back - in the future! [*problem solved on 16/4/2025 - the SQL subsetting was done incorrectly, prescriptions were filtered by patient ID and not medical record ID, and so in the step of linking prescriptions to measurements, many measurements that had no corresponding emotional values were retained*]\n",
    "\n",
    "[19.46] yay, survival analysis input done! But there are issues - it all needs to be revised, all the pipeline for this paper-specific analysis\n",
    "\n",
    "[16.30] prompt to get survival analysis input\n",
    "\n",
    "Write a module that pulls data from measurements_with_metadata, and obtains input data organized for survival analysis. The outcome of interest is 10% weight loss. \n",
    "\n",
    "The code should group the measurements (rows) by patient id and medical record ID (measurements from the same treatment of the same patient), and take the first measurement of each group as the reference baseline measurement. \n",
    "Counting from the first measurement, it should look at the following measurements in a range of 60, +/- 10 days, \n",
    "and check the porcentual amount of weight lost in each timepoint. \n",
    "If a weight loss over 10% is achieved, the days it took to achieve it should be noted. \n",
    "If the patient did not follow the treatment for at least 60 +/- 10 days, the last measurement should be noted.\n",
    "\n",
    "The output should be a data frame called sa_60d_10%. It should have the following columns: \n",
    "- patient_id - text, patient ID\n",
    "- medical_record_id - text, medical record ID\n",
    "- first_measurement_date - datetime, the first measurement in the group\n",
    "- end_measurement_date - datetime, either the last measurement in the group (in case of dropout before 60 days, OR not achieving 10% weight loss in 60 days) OR the first measurement where 10% weight loss is achieved\n",
    "- days_to_end_measurement - numeric, days passed between the first and last measurement date\n",
    "- nr_measurements - numeric, the number of measurements between the first and the end measurement\n",
    "- baseline_weight - numeric, the weight at time of the first measurement\n",
    "- final_weight - numeric, the weight at the time of the last measurement\n",
    "- wl% - numeric, the porcentual difference between the baseline and final weight values\n",
    "- 10%_achieved - bool, yes/no, yes if final weight is over 10% less than baseline, no if not\n",
    "- dropout - bool, yes/no, yes if didn't reach 60+/- 10 days of followup AND didn't reach over 10% weight loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[13.15] **reu Alfredo** \n",
    "- 6 regresiones para 6 variables, y luego uno para todo\n",
    "- pÃ©rdida de peso a los 50, 60, 70 dÃ­as, y 7.5, 10, 12.5% por ejemplo\n",
    "- hipÃ³tesis para JPM: personalizar pronÃ³stico de adelgazamiento segÃºn estado emocional basal\n",
    "- apoyo estadÃ­stico: 50â¬ por hora y media, 1-2 veces al mes. VendrÃ­a a travÃ©s de ISCYL, en bonos, probable que para mÃ­, y lo a Ãrpi\n",
    "- no abusar de chatgpt! comprobar siempre las propuestas. \n",
    "\n",
    "\n",
    "[12.25] haladunk, egyre tÃ¶bb metadata egyre koncentrÃ¡ltabban van jelen a bÃ¡zisban. kÃ¶vi nagy lÃ©pÃ©s a 10% eventek idÅpontjÃ¡nak megÃ¡llapÃ­tÃ¡sa. utÃ¡na illeszthetÅek a regressziÃ³s modellek. nem hiszem, h ez ma meglesz, ezzel egyÃ¼tt haladunk ahogy kell. \n",
    "\n",
    "[7.40] MÃ¡ra miket kÃ©ne csinÃ¡lni? \n",
    "- exploratory association plot with 60-day deltas and emotional scores; picking first records from multirec patients\n",
    "- possibly: making a **larger wide df**, that contains the most data possible from patients\n",
    "- regarding survival analysis: \n",
    "    - define input data and its structure\n",
    "    - exclude inmediate dropouts\n",
    "    - EDA: descriptive stats and recommended KM curves\n",
    "    - advanced models: \n",
    "        - define train/test sets\n",
    "        - fit simple Cox model, 1 or a few variables\n",
    "        - fit competing risk models with time strata (because of proportional hazards assumption, although maybe that is for Cox actually?)\n",
    "        - maybe fit parametric models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14/4/25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[AI GENERATED ANALYSIS PROTOCOL BASED ON DISCUSSIONS THROUGHOUT THE DAY]\n",
    "\n",
    "**Analysis Protocol: Emotional Behaviors and Weight Loss Success**  \n",
    "**Objective**: Understand how baseline emotional/eating behaviors predict the time it takes to achieve 10% weight loss, while addressing dropouts and statistical pitfalls.\n",
    "\n",
    "**1. Data Preparation**  \n",
    "**Goal**: Clean and structure your data for survival analysis.  \n",
    "- **Define Events**:  \n",
    "  - **Success**: The day a participant first reaches â¥10% weight loss (e.g., Day 45).  \n",
    "  - **Censoring**: The last day a participant was measured if they dropped out or never reached 10% loss.  \n",
    "- **Exclude Immediate Dropouts**:  \n",
    "  - **Why?** Participants with only 1 measurement (e.g., Day 0) provide no information about *time-to-event*. Including them would artificially inflate the number of \"censored\" cases without meaningful follow-up.  \n",
    "  - **How?** Remove anyone with only baseline data (no follow-up).  \n",
    "\n",
    " **2. Exploratory Analysis**  \n",
    "**Goal**: Understand patterns in your data before modeling.  \n",
    "- **Descriptive Stats**:  \n",
    "  - Report averages for continuous variables (e.g., \"Average emotional eating score: 6.2/10\").  \n",
    "  - Report percentages for categorical variables (e.g., \"60% female\").  \n",
    "- **Kaplan-Meier Curves**:  \n",
    "  - **What?** A graph showing the probability of *not* achieving 10% weight loss over time.  \n",
    "  - **How?** Compare groups (e.g., high vs. low emotional eating). If the curves separate, the variable might matter.  \n",
    "\n",
    "**3. Primary Analysis: Cox Regression**  \n",
    "**Goal**: Test if emotional behaviors predict *how quickly* people lose weight.  \n",
    "- **Model 1 (Simple)**:  \n",
    "  - Predictor: One emotional variable (e.g., emotional eating score).  \n",
    "  - Output: **Hazard ratio (HR)**.  \n",
    "    - HR > 1: Faster weight loss.  \n",
    "    - HR  0.5**: Model is better than random guessing.  \n",
    "\n",
    "**5. Sensitivity Analyses**  \n",
    "**Goal**: Check if results hold under different assumptions.  \n",
    "- **Competing Risks**:  \n",
    "  - **What?** Treat dropout as a competing event (e.g., someone quits and canât lose weight).  \n",
    "  - **How?** Use Fine-Gray models instead of Cox.  \n",
    "- **Parametric Models**:  \n",
    "  - **What?** Assume the risk of weight loss decreases over time (e.g., Weibull model).  \n",
    "  - **Why?** Compare results to Cox for consistency.  \n",
    "\n",
    "**6. Reporting Results**  \n",
    "**Tables**:  \n",
    "- Show hazard ratios (HRs), 95% confidence intervals (CIs), and *p*-values for all models.  \n",
    "**Figures**:  \n",
    "- Kaplan-Meier curves for key predictors.  \n",
    "- Forest plots comparing HRs across subgroups.  \n",
    "\n",
    "**Key Questions to Answer**  \n",
    "1. Does emotional eating slow down or speed up weight loss?  \n",
    "2. Do effects change over time (e.g., matter more in the first month)?  \n",
    "3. Are results robust after adjusting for age, sex, and BMI?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[jotting down paper notes taken over the day] EgyÃ©rtelmÅ±, h cikkre kell fÃ³kuszÃ¡lni, az is, h ehhez a Cox reg fontos concept. Ott kell felvenni a fonalat, ahol letettem - mÃ©rÃ©sek Ã©s metaadatok linkelÃ©se. Ha azt jÃ³l csinÃ¡ltam, ott meg is vannak a 60 napos deltÃ¡k - igaz mÃ©g az input adatokat tisztÃ­tani kell, pl duplikÃ¡ltak kidobÃ¡sa stb, visszamenÅleg tisztÃ­tani kell a pipelinet. Illetve, ezek tentatÃ­v elemzÃ©sek Ã©s nem vÃ©gleges adatok lesznek, egÃ©szen addig amÃ­g a Measurements Ã¡tnÃ©zÃ©se meg nem tÃ¶rtÃ©nik. \n",
    "\n",
    "[JOTS ON COX/SURVIVAL ANALYSIS RESEARCH] \n",
    "- Cox model: effect of a unit increase in a covariate: multiplicative\n",
    "- low risk vs high risk / low/high emotionality\n",
    "- hazard ratio: the key concept, that's estimated in a Cox model. Associated with a risk factor or predictor for a given endpoint - meaning? Its prerequisite is assesment of proportional hazards. Meaning? \n",
    "- Kaplan-Meier: simplest SA, after comes Cox, penalized Cox models like ridge, lasso, elastic net, and more robust MLs for SA\n",
    "- SA: effects of several factors on an event happening - rate of this event happening is the HAZARDS RATE; risk of event X at time t\n",
    "- want to find out the influence of factors A, B, C, D, X, Y, Z on the RATE OF THE EVENT HAPPENING? \n",
    "- covariates: predictor variables, factors\n",
    "- linking COVARIATES data to TIME OF EVENT data - establish LINKS with SA\n",
    "- selecting best predictive variable is important    \n",
    "- concordance index: correlation between predicted risk scores and time point\n",
    "- Cox model needs train/test data too!\n",
    "- Penalized models, Random survival forest, survival SMVs, survival GB can have interesting advantages, \n",
    "- scikit-survival: https://scikit-survival.readthedocs.io/en/stable/user_guide/index.html\n",
    "\n",
    "[BRAINSTORM: HOW TO USE IT]\n",
    "- this is like a regression - but parts of the training data is CENSORED due to dropout, death, and in some cases, the outcome is just not achieved - > we can use data from ALL patients and fitting and adequate survival model we can account for dropouts\n",
    "- existing conditions, comorbidities as confounders should also be accounted for; and a subset with genetics, also maybe look for links in weight gain reasons\n",
    "- observe 60 days - some drop out, some lose 10%, some don't\n",
    "- adjust for age, sex, baseline BMI, comorbidities etc\n",
    "- Still, SA focuses on **time to event happening**, which may not be the most revealing test to look for associations between emotional eating and WL outcomes. \n",
    "- clause 0, if your supervisor asks you to look into something, you do\n",
    "- clause 1, survival analysis looks good for the case where we have dropouts, successes, different durations till success, no success and all\n",
    "- what can be complicated is the relatively large nr of covariates in our analyses. we need to adjust for demographics, anthropometrics, emotional and clinical variables. maybe more complex models are the answer here. scikit-surv guide mentions versatility of SSVMs for example. Perplexity recommends those for complex, highD, nonlinear datasets. \n",
    "- what we *really care about* is not the time to success, but the *influence of certain covariates on success*. \n",
    "- **competing risk models**, like Fine-Gray, accounts for meaningful dropouts - like, if emotional values influence on dropout, then dropout should be accounted for as a competing risk that prevents the outcome of interest. If a covariate predicts higher risk for a competing risk, that is insightful. Otherwise, non-informative dropouts should be censored, and the researcher should stick to a normal Cox model. \n",
    "- regarding the analysis: time-to-event analysis should be the best, like identifying the time needed to achieve a 10% WL. OK, that implies doing the absolute to relative calculation maybe more times, but I think that can be a dynamic part of the code, no need to calculate for every measurement, only pulling a function on measurements of interest. \n",
    "- IMPORTANT: Proportional hazards is some kind of assumption in Cox, that the effect size of one covariate is constant over time, does not change. Like if hunger influences weight this much at baseline, the size of the influence will stay constant. **This is risky** and needs to be adjusted for - emotional values can change, and their effects can change. Some kind of test needs to be ran to check whether this assumption is violated (Schoenfeld residuals, 'check_assumptions' in scikit-survival), and if it is, stratifying the data for periodic blocks and doing different analyses for each block can be a way to go. Like, fit a new model for every 20 days, and engineer it well enough to link the different ones. In every model, the covariates are baseline (cross-sectional basically), the outcome is derived from the measurements that are continuously recorded. The goal is to see how the weight decreases, in function of some variables, and how long it takes to reach 5, 10, 15, 20% WL. \n",
    "\n",
    "[QUESTIONS]\n",
    "- what should the input data structure look like for the survival models? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5/4/25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**problems for future self**] mÃ©g mindig messy a notebook, ezt a pipelinet nem is lÃ¡tom Ã¡t egÃ©szen, Ã©s duplikÃ¡lt mÃ©rÃ©sek vannak az inputban, mert colclean, nem rowclean inputtal fut le a kÃ³d\n",
    "\n",
    "[09.11] CsÃ¶kkenni fog a hosszÃºtÃ¡vÃº adatokkal rendelkezÅ pÃ¡ciensek szÃ¡ma az Ã©rzelmi datasetben, ha 60 nap followupra szÅ±rÃ¼nk, tÃ¶bben dropoutoltak. Viszont a baseline adatokban az Ã©rzelem - baseline obesity Ã©s a droput rate kÃ¶zÃ¶tti Ã¶sszefÃ¼ggÃ©seket is meg lehet nÃ©zni\n",
    "\n",
    "makin progress! kÃ©t jÃ³ Ã¶tlet, Ã©s megvan a 2 df a 60day delta weight Ã©rtÃ©kekkel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4/4/25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[16.25]\n",
    "SzeretnÃ©k mock analÃ­ziseket csinÃ¡lni, amelyekben Ã¶sszehasonlÃ­tom az eating behavior Ã©s a weight loss outcome adatokat. Mivel egy ember tÃ¶bb kezelÃ©sen megy Ã¡t, Ã©s a kezelÃ©sek hossza vÃ¡ltozÃ³, a tudomÃ¡nyos pontossÃ¡g miatt fontos, hogy fix vagy nagyjÃ¡bÃ³l fix idÅtartamÃº kezelÃ©seket nÃ©zzÃ¼nk (persze kÃ©sÅbb teljes kezelÃ©sre is rÃ¡ lehet vetÃ­teni, 60-day weight loss vs overall treatment outcome). \n",
    "\n",
    "Ahhoz, hogy a fix idÅtartamÃº kÃ¶vetÃ©si idÅszakok elsÅ Ã©s utolsÃ³ mÃ©rÃ©si adatÃ¡t megszerezzÃ¼k, ahhoz minden mÃ©rÃ©shez hozzÃ¡ kell rendelni a hozzÃ¡ tartozÃ³ kezelÃ©st, illetve receptet, amibÅl megtudjuk, hogy a kezelÃ©s melyik szakaszÃ¡ban van a pÃ¡ciens, Ã©s hogy az adott szakasz meddig tartott, illetve hogy maga az egÃ©sz kezelÃ©s milyen hosszÃº volt. Most ami megvan, azt hiszem, hogy a kezelÃ©sek Ã©s receptek hozzÃ¡ vannak rendelve az egyes mÃ©rÃ©sekhez. \n",
    "\n",
    "Ezek a hozzÃ¡rendelÃ©sek long formÃ¡jÃºak, Ã©s tovÃ¡bb vannak fejlesztve egy olyan tÃ¡blÃ¡zatban, ahol minden prescription, step Ã©s medical record elsÅ Ã©s utolsÃ³ mÃ©rÃ©se van, illetve a kÃ¶ztÃ¼k eltelt idÅ Ã©s sÃºlyvÃ¡ltozÃ¡s. Ez egyfajta megkÃ¶zelÃ­tÃ©s, olvashatÃ³bb, viszont elvesznek a kÃ¶ztes adatok. IdÃ¡ig jutottam, Ã©s itt megakadtam egy kicsit, hogy akkor most hogy is tovÃ¡bb? Nem is vagyok biztos benne, hogy egyÃ¡ltalÃ¡n lÃ¡tom a problÃ©mÃ¡t, ami megakasztott. \n",
    "\n",
    "Amit szeretnÃ©k, az 1 treatment cikluson belÃ¼li, 1 medical recordhoz tartozÃ³ 2 hÃ³nap kÃ¶rÃ¼li mÃ©rÃ©sek sÃºlyvÃ¡ltozÃ¡sa. Az kulcsfontossÃ¡gÃº, hogy egy medical recordhoz tartozzanak, az viszont, hogy melyik lÃ©pÃ©shez tartoznak, az inkÃ¡bb kiegÃ©szÃ­tÅ informÃ¡ciÃ³. Sok kontextust ad, mert tudjuk, hogy keto vagy nem keto fÃ¡zisban van-e a pÃ¡ciens, de nem vÃ©gzetes hiba, ha ez az infÃ³ elvÃ©sz. Ami vÃ©gzetes hiba lenne, az pl. kÃ©t kezelÃ©si ciklus adatainak az Ã¶sszevonÃ¡sa. \n",
    "\n",
    "TehÃ¡t a most meglÃ©vÅ megkÃ¶zelÃ­tÃ©ssel, a prescription/step/record szinten is elsÅ Ã©s utolsÃ³ adatot tartalmazÃ³ tÃ¡blÃ¡val az a baj, hogy tÃºl bonyolult, Ã©s talÃ¡n adatot is veszÃ­tek. ElÃ©g lenne egy olyan tÃ¡bla, ahol 1 medical recordhoz tartozÃ³ adatok vannak, baseline Ã©s 3, +/- 1 hÃ³nap cutoffnÃ¡l. TehÃ¡t a most (piszkozat Ã¡llapotban) 'linked' nevÅ± tÃ¡blÃ¡zat sorait kÃ©ne patient>medical record szerint csoportosÃ­tani, measurement_date szerint rendezni, Ã©s az elsÅ mÃ©rÃ©stÅl szÃ¡mÃ­tott 2-4 hÃ³nap kÃ¶zÃ¶tti, 3-hoz legkÃ¶zelebbi Ã©rtÃ©ket kinyerni, ezzel szÃ¡molni egy pivoted delta weight tÃ¡blÃ¡zatot, amiben benne van a step is, mint metadata. Ezeket a sÃºlyadatokat lehet mÃ¡r korrelÃ¡ltatni az Ã©rzelmi adatokkal, Ã©s a megfelelÅ metadata ismeretÃ©ben kÃ¶nnyebb a cleaning Ã©s az Ã©rtelmezÃ©s is. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
