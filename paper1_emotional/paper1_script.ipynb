{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper 1 - eating behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the relevant directories used in this paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is pulled from the standardized data folder; subsequently, it is stored and managed in the paper 1 folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the source and output directories\n",
    "source_directory = r\"C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\DB2_standard\"\n",
    "paper1_directory = r\"C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(paper1_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a research question-specific SQL database subset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check those medical records where any/3+/all emotional values are available, and filter the database to contain only the specified patients and medical records. Save the data to 3 new SQL files - one with any, one with some, one with all values available. For research purposes, the last one is most likely to be used. The first two may be relevant if trying to increase the sample size for one or a few specific emotional values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any emotional data points are available in 2482 records from 2437 patients\n",
      "At least 3 emotional data points are available in 2169 records from 2132 patients\n",
      "All emotional data points are available in 1853 records from 1826 patients\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Use the above defined directories\n",
    "db_path = os.path.join(source_directory, \"pnk_db2_colclean.sqlite\")\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# List all tables in the database\n",
    "query_tables = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "tables = pd.read_sql_query(query_tables, conn)\n",
    "table_names = tables['name'].tolist()\n",
    "\n",
    "# Define criteria for filtering for any/3+/all emotional values available\n",
    "def create_filtered_database(criteria, output_filename):\n",
    "    # Set up the appropriate query based on the criteria for the three scenarios\n",
    "    if criteria == \"any\":\n",
    "        # to select records where at least one emotional variable is not null\n",
    "        query = \"\"\"\n",
    "        SELECT medical_record_id, patient_id\n",
    "        FROM medical_records_colclean\n",
    "        WHERE hunger IS NOT NULL\n",
    "           OR satiety IS NOT NULL\n",
    "           OR emotional_eating IS NOT NULL\n",
    "           OR emotional_eating_value IS NOT NULL\n",
    "           OR quantity_control IS NOT NULL\n",
    "           OR impulse_control IS NOT NULL;\n",
    "        \"\"\"\n",
    "    elif criteria == \"3plus\":\n",
    "        # to select records where at least three emotional variables are not null\n",
    "        query = \"\"\"\n",
    "        SELECT medical_record_id, patient_id\n",
    "        FROM medical_records_colclean\n",
    "        WHERE (CASE WHEN hunger IS NOT NULL THEN 1 ELSE 0 END +\n",
    "               CASE WHEN satiety IS NOT NULL THEN 1 ELSE 0 END +\n",
    "               CASE WHEN emotional_eating IS NOT NULL THEN 1 ELSE 0 END +\n",
    "               CASE WHEN emotional_eating_value IS NOT NULL THEN 1 ELSE 0 END +\n",
    "               CASE WHEN quantity_control IS NOT NULL THEN 1 ELSE 0 END +\n",
    "               CASE WHEN impulse_control IS NOT NULL THEN 1 ELSE 0 END) >= 3;\n",
    "        \"\"\"\n",
    "    elif criteria == \"all\":\n",
    "        # to select records where all emotional variables are not null \n",
    "        query = \"\"\"\n",
    "        SELECT medical_record_id, patient_id\n",
    "        FROM medical_records_colclean\n",
    "        WHERE hunger IS NOT NULL\n",
    "          AND satiety IS NOT NULL\n",
    "          AND emotional_eating IS NOT NULL\n",
    "          AND emotional_eating_value IS NOT NULL\n",
    "          AND quantity_control IS NOT NULL\n",
    "          AND impulse_control IS NOT NULL;\n",
    "        \"\"\"\n",
    "    \n",
    "    # Get the relevant records, and extract their medical record and patient IDs\n",
    "    relevant_records = pd.read_sql_query(query, conn)\n",
    "    relevant_medical_record_ids = tuple(relevant_records['medical_record_id'])\n",
    "    relevant_patient_ids = tuple(relevant_records['patient_id'])\n",
    "    \n",
    "    # Create a new database in the output directory\n",
    "    output_db_path = os.path.join(paper1_directory, output_filename)\n",
    "    filtered_conn = sqlite3.connect(output_db_path)\n",
    "    \n",
    "    # Filter each table in the source SQl to only contain the records that comply the criteria of the given scenario; \n",
    "    # ie. they have records where any/3+/all emotional variables are available\n",
    "    for table_name in table_names:\n",
    "        if table_name.startswith(\"sqlite_\"):\n",
    "            # Skip any SQLite system tables\n",
    "            continue \n",
    "        # In case of tables that may contain several medical records from the same patient, \n",
    "        # filter by medical_record_id\n",
    "        if table_name == \"medical_records_colclean\" or table_name == \"prescriptions_colclean\":\n",
    "            query = f\"\"\"\n",
    "            SELECT * \n",
    "            FROM {table_name}\n",
    "            WHERE medical_record_id IN {relevant_medical_record_ids}\n",
    "            \"\"\"\n",
    "        else:\n",
    "            # For all other tables, filter by patient_id only, as medical record ID is not available in those\n",
    "            query_check_column = f\"PRAGMA table_info({table_name});\"\n",
    "            columns = pd.read_sql_query(query_check_column, conn)\n",
    "            if 'patient_id' not in columns['name'].values:\n",
    "                continue  # Skip tables without patient_id\n",
    "            query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM {table_name}\n",
    "            WHERE patient_id IN {relevant_patient_ids}\n",
    "            \"\"\"\n",
    "        \n",
    "        # Execute the given query and save the result in a new SQLite database\n",
    "        filtered_data = pd.read_sql_query(query, conn)\n",
    "        filtered_data.to_sql(table_name, filtered_conn, index=False, if_exists=\"replace\")\n",
    "    \n",
    "    filtered_conn.close()\n",
    "    return len(relevant_records), len(set(relevant_records['patient_id']))\n",
    "\n",
    "# Create and save the three databases for the three scenarios - any/3+/all emotional variables available\n",
    "any_count, any_patients = create_filtered_database(\"any\", \"emotional_any_notna.sqlite\")\n",
    "three_plus_count, three_plus_patients = create_filtered_database(\"3plus\", \"emotional_3plus_notna.sqlite\")\n",
    "all_count, all_patients = create_filtered_database(\"all\", \"emotional_all_notna.sqlite\")\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "# Print summary\n",
    "print(f\"Any emotional data points are available in {any_count} records from {any_patients} patients\")\n",
    "print(f\"At least 3 emotional data points are available in {three_plus_count} records from {three_plus_patients} patients\")\n",
    "print(f\"All emotional data points are available in {all_count} records from {all_patients} patients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean and link measurements to prescriptions and medical records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the patient ID and the date of a given measurements, look for prescriptions with the same patient ID that cover the range of time in which the measurement was taken. This way, measurements can be linked to important metadata, such as the prescription and medical record they belong to, the step of the programme they were taken in, etc. \n",
    "\n",
    "In summary, this is a key step in the research, without which data on any measurement's identity would be insufficient, and measurements from different prescriptions of the same individual could be mixed, for example. In a previous attempt, I tried identifying blocks of measurements as those that are taken within two months of each other, but I consider this a much more solid approach. \n",
    "\n",
    "It is important to note that some patients may take repeated measurements on the same occasion. These duplicates need to be removed, as they inflate the dataset. \n",
    "\n",
    "After removing the duplicates, measurements and prescriptions are linked in a two-step process. \n",
    "\n",
    "First, measurements are linked to all possible prescriptions that can belong to them based on the shared patient ID (this scenario where every option is linked to every option is called a Cartesian product). \n",
    "\n",
    "After, these possible links are filtered by date: a measurement belongs to a prescription if it is within its validity period, or is 5 days within its start or end dates. In the latter case, a measurement may be assigned to multiple prescriptions; if this happens, it is assigned to the one it is closer to in time. \n",
    "\n",
    "If a measurement is not succesfully linked to any prescription, it is lost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate measurements removed. There are 35709 measurements from 1826 patients.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Remove duplicate measurements before doing any data frame merging. \n",
    "Any measurement from the same patient on the same day (ignoring time) with the same weight should be considered a duplicate.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# Connect to the database, load the measurements table\n",
    "conn = sqlite3.connect(os.path.join(paper1_directory, \"emotional_all_notna.sqlite\"))\n",
    "measurements = pd.read_sql_query(\"SELECT * FROM measurements_colclean\", conn)\n",
    "\n",
    "# Convert measurement_date to datetime, if not already in that format. \n",
    "# Add a temporary column with the measurement date only; time is ignored, \n",
    "# as repeated measurements are at least a few seconds or minutes apart. \n",
    "measurements['measurement_date'] = pd.to_datetime(measurements['measurement_date'])\n",
    "measurements['measurement_date_date'] = measurements['measurement_date'].dt.date\n",
    "# After, remove duplicates based on patient id, date, and weight. \n",
    "# Drop the temporary column. \n",
    "measurements_rowclean = measurements.drop_duplicates(subset=['patient_id', 'measurement_date_date', 'weight_kg'])\n",
    "measurements_rowclean = measurements_rowclean.drop(columns=['measurement_date_date'])\n",
    "# Save the cleaned measurements back to the database with the _rowclean name code\n",
    "measurements_rowclean.to_sql(\"measurements_rowclean\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "print(f\"Duplicate measurements removed. There are {len(measurements_rowclean)} measurements from {measurements_rowclean['patient_id'].nunique()} patients.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measurements are linked to their corresponding prescriptions and medical records. \n",
      "There are a total of 20976 measurements from 1678 medical records of 1664 patients.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Link metadata from the prescriptions table to measurements. \n",
    "\n",
    "The two dataframes are merged based on patient_id, creating a Cartesian product of the two tables, \n",
    "where every measurement from one patient is linked to every possible prescription from that patient.\n",
    "\n",
    "This Cartesian product is then filtered based on the dates of both the measurement and the prescription, \n",
    "in order to, preferably, only consider a prescription being linked to a given measurement\n",
    "if the measurement date is between the prescription's start and end dates. \n",
    "\n",
    "If a measurement is not within any prescription's validity period, \n",
    "there is a permissivity of 5 days, meaning that a measurement can be linked to a prescription if\n",
    "it is within 5 days from the start or end date of the prescription.\n",
    "If this allows a measurement to be linked to multiple prescriptions,\n",
    "it is linked to the one it is closest to in date. \n",
    "\n",
    "If a measurement is not linked to any valid prescription, \n",
    "it is excluded from the outuput. \n",
    "\"\"\"\n",
    "\n",
    "# Connect to the paper-specific database, load the prescriptions table, and make sure its date values are in datetime format\n",
    "conn = sqlite3.connect(os.path.join(paper1_directory, \"emotional_all_notna.sqlite\"))\n",
    "prescriptions = pd.read_sql_query(\"SELECT * FROM prescriptions_colclean\", conn)\n",
    "prescriptions['prescription_creation_date'] = pd.to_datetime(prescriptions['prescription_creation_date'])\n",
    "prescriptions['prescription_validity_end_date'] = pd.to_datetime(prescriptions['prescription_validity_end_date'])\n",
    "\n",
    "# Merge the measurements and prescriptions data frames on patient ID,\n",
    "# creating the Cartesian product that needs further date-based filtering\n",
    "merged = pd.merge(measurements_rowclean, prescriptions, on=\"patient_id\", how=\"left\", suffixes=('_meas', '_presc'))\n",
    "\n",
    "# To execute date-based filtering: \n",
    "# First, define those measurements that are within the range of a prescription. \n",
    "# If any measurement can be assigned to a prescription based on this criteria, it will be. \n",
    "merged['measurement_in_prescription_range'] = (\n",
    "    (merged['measurement_date'] >= merged['prescription_creation_date']) &\n",
    "    (merged['measurement_date'] <= merged['prescription_validity_end_date'])\n",
    ")\n",
    "# If after this, a measurement is still not linked to any prescription due to not being in the range of any, \n",
    "# it will be linked to the prescription it is closest to, within a 5-day permissivity range. \n",
    "# For these out-of-range measurements, first, the distance from the start/end dates of any prescription is calculated. \n",
    "merged['days_before_prescription_start'] = (merged['prescription_creation_date'] - merged['measurement_date']).dt.days\n",
    "merged['days_after_prescription_end'] = (merged['measurement_date'] - merged['prescription_validity_end_date']).dt.days\n",
    "# After, near-range measurements are defined, \n",
    "# as measurements that are NOT within the range of any prescription, \n",
    "# AND they are at within 5 days before the start/after the end of any prescription. \n",
    "merged['measurement_near_prescription_range'] = (\n",
    "    (~merged['measurement_in_prescription_range']) &\n",
    "    (\n",
    "        ((merged['days_before_prescription_start'] <= 5) & (merged['days_before_prescription_start'] > 0)) |\n",
    "        ((merged['days_after_prescription_end'] <= 5) & (merged['days_after_prescription_end'] > 0))\n",
    "    )\n",
    ")\n",
    "# After, a distance metric calculation determines how far a given measurement is from a prescription. \n",
    "# In-range measurements get a distance metric of 0,\n",
    "# while out-of-range measurements get the minimum distance to any boundary they are close to. \n",
    "merged['measurement_distance_from_prescription_range'] = merged.apply(\n",
    "    lambda row: 0 if row['measurement_in_prescription_range'] else min(max(row['days_before_prescription_start'], 0), max(row['days_after_prescription_end'], 0)),\n",
    "    axis=1\n",
    ")\n",
    "# After defining the in-range and near-range logics, the database (currently containing Cartesian products) \n",
    "# is filtered to keep only in-or near-range measurements. \n",
    "# Any measurements not assigned to a prescription is lost. \n",
    "measurements_with_metadata = merged[merged['measurement_in_prescription_range'] | merged['measurement_near_prescription_range']].copy()\n",
    "# In edge cases where multiple prescriptions are linked to a single measurement, only the closest match is kept. \n",
    "# This is done by sorting the data frame by patient id, measurement date and distance from range, \n",
    "# and if multiple measurement-prescription pairs from the same patient on the same date are found, \n",
    "# duplicates are removed and only the row with the smallest distance from range is kept. \n",
    "measurements_with_metadata = measurements_with_metadata.sort_values(['patient_id', 'measurement_date', 'measurement_distance_from_prescription_range'])\n",
    "measurements_with_metadata = measurements_with_metadata.drop_duplicates(['patient_id', 'measurement_date'])\n",
    "\n",
    "# After filtering the data frame, columns are reordered, and any irrelevant ones, like prescribed supplements, are dropped. \n",
    "column_order = [\n",
    "    'patient_id',\n",
    "    'medical_record_id',\n",
    "    'prescription_id',\n",
    "    'measurement_date',\n",
    "    'prescription_creation_date',\n",
    "    'prescription_validity_end_date',\n",
    "    'prescription_validity_days',\n",
    "    'method',\n",
    "    'step',\n",
    "    'weight_kg',\n",
    "    'bmi',\n",
    "    'bmr_kcal',\n",
    "    'fat_%',\n",
    "    'vat_%',\n",
    "    'muscle_%',\n",
    "    'water_%',\n",
    "    'measurement_in_prescription_range',\n",
    "    'days_before_prescription_start',\n",
    "    'days_after_prescription_end',\n",
    "    'measurement_near_prescription_range',\n",
    "    'measurement_distance_from_prescription_range'\n",
    "]\n",
    "measurements_with_metadata = measurements_with_metadata[column_order]\n",
    "\n",
    "# The measurements_with_metadata data frame is saved within the SQL database, and some summary info is printed. \n",
    "measurements_with_metadata.to_sql(\"measurements_with_metadata\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "print(f\"Measurements are linked to their corresponding prescriptions and medical records. \\n\"\n",
    "    f\"There are a total of {measurements_with_metadata.shape[0]} measurements \"\n",
    "    f\"from {measurements_with_metadata['medical_record_id'].nunique()} medical records \"\n",
    "    f\"of {measurements_with_metadata['patient_id'].nunique()} patients.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add sex, genomics ID and baseline/final weight data to medical records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an effort to create data frames containing the most possible information in one place, the medical records data frame is completed with the sex (originally stored in Patients) as well as the baseline and final weight data (measurements linked to medical records stored in measurements_with_metadata) of patients. Genomics sample IDs are also fetched for patients that have it available. \n",
    "\n",
    "Besides executing these merge operations, the code checks the time passed between baseline and final measurements in each medical record, along with whether the measurements are close to the beginning/end date of the medical record they belong to or not. This helps checking whether the length of the actual followup is similar to that of the medical record or not. \n",
    "\n",
    "Any medical record that has no associated measurements is lost here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medical records table completed with sex and baseline/final weight data. \n",
      "There are 1678 records available from 1664 patients.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Complete Medical records by adding sex and baseline/final weight data to it. \n",
    "\n",
    "Sex and genomics sample IDs are fetched from the Patients table, based on the patient_id.\n",
    "\n",
    "Baseline and final weight measurements are obtained from the measurements_with_metadata table created in the previous step. \n",
    "The logic is the following: \n",
    "Measurements are grouped by patient and medical record ID, and the first and last measurements of each group are assigned\n",
    "to the medical records table as baseline and final measurements, respectively.\n",
    "Delta weight is calculated as the difference between final and baseline weights, to obtain negative results. \n",
    "\n",
    "Measurement dates are added and it is checked if they are within the medical record creation and closing dates.\n",
    "\n",
    "If a medical record has no measurements linked to it, it is dropped. \n",
    "\n",
    "Additionally, the 'days_between_measurements' column is added to calculate the number of days between the baseline and final measurements.\n",
    "\n",
    "Finally, the columns are reordered to match the desired order.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Connect to the database, load relevant tables\n",
    "conn = sqlite3.connect(os.path.join(paper1_directory, \"emotional_all_notna.sqlite\"))\n",
    "medical_records = pd.read_sql_query(\"SELECT * FROM medical_records_colclean\", conn)\n",
    "patients = pd.read_sql_query(\"SELECT * FROM patients_colclean\", conn)\n",
    "measurements_with_metadata = pd.read_sql_query(\"SELECT * FROM measurements_with_metadata\", conn)\n",
    "\n",
    "# The following functions complete the original medical records data frame with research-relevant variables.\n",
    "# First, add the sex variable to medical_records_complete by merging patients' sex into medical_records_complete based on patient_id\n",
    "\"\"\"\n",
    "Adding sex data and genomics sample IDs to medical records\n",
    "\"\"\"\n",
    "medical_records_complete = pd.merge(\n",
    "    medical_records,\n",
    "    patients[['patient_id', 'sex', 'genomics_sample_id']],\n",
    "    on='patient_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# After, add baseline and final measurements to medical_records_complete\n",
    "# Treat measurements coming from a given medical record as units\n",
    "# by grouping measurements_with_metadata by patient_id and medical_record_id \n",
    "\"\"\"\n",
    "Adding weight data to medical records\n",
    "\"\"\"\n",
    "grouped_measurements = measurements_with_metadata.groupby(['patient_id', 'medical_record_id'])\n",
    "# Extract the first (baseline) and last (final) measurement for each group\n",
    "baseline = grouped_measurements.first().reset_index()\n",
    "final = grouped_measurements.last().reset_index()\n",
    "# Insert baseline and final measurements into medical_records_complete\n",
    "medical_records_complete = pd.merge(\n",
    "    medical_records_complete,\n",
    "    baseline[['patient_id', 'medical_record_id', 'measurement_date', 'weight_kg', 'bmi']],\n",
    "    on=['patient_id', 'medical_record_id'],\n",
    "    how='left'\n",
    ")\n",
    "medical_records_complete = pd.merge(\n",
    "    medical_records_complete,\n",
    "    final[['patient_id', 'medical_record_id', 'measurement_date', 'weight_kg', 'bmi']],\n",
    "    on=['patient_id', 'medical_record_id'],\n",
    "    how='left',\n",
    "    suffixes=('_baseline', '_final')\n",
    ")\n",
    "# Make sure all dates are in datetime format for further operations, \n",
    "# and calculate delta weight and delta BMI values (final - baseline, so the resulting weight loss value is negative)\n",
    "medical_records_complete['medical_record_creation_date'] = pd.to_datetime(medical_records_complete['medical_record_creation_date'])\n",
    "medical_records_complete['medical_record_closing_date'] = pd.to_datetime(medical_records_complete['medical_record_closing_date'])\n",
    "medical_records_complete['measurement_date_baseline'] = pd.to_datetime(medical_records_complete['measurement_date_baseline'])\n",
    "medical_records_complete['measurement_date_final'] = pd.to_datetime(medical_records_complete['measurement_date_final'])\n",
    "medical_records_complete['delta_weight_kg'] = medical_records_complete['weight_kg_final'] - medical_records_complete['weight_kg_baseline']\n",
    "medical_records_complete['delta_bmi'] = medical_records_complete['bmi_final'] - medical_records_complete['bmi_baseline']\n",
    "# Check if the baseline and final measurements are close to the starting/closing date of the medical record they belong to or not (within a 10-day window). \n",
    "# In some cases, the first measurement is recorded weeks after opening the medical record, or the last one is taken long before closing it. \n",
    "# In other cases, the medical record's closing date is absent, if this happens, the last measurement will be considered as out of range. \n",
    "# This is supposed to help identify cases where the followup has some imperfections. \n",
    "window_days = 10\n",
    "medical_records_complete['baseline_measurement_inrange'] = (\n",
    "    (medical_records_complete['measurement_date_baseline'] >= \n",
    "     medical_records_complete['medical_record_creation_date'] - pd.Timedelta(days=window_days)) &\n",
    "    (medical_records_complete['measurement_date_baseline'] <=\n",
    "     medical_records_complete['measurement_date_baseline'] + pd.Timedelta(days=window_days))\n",
    ")\n",
    "medical_records_complete['final_measurement_inrange'] = (\n",
    "    (medical_records_complete['measurement_date_final'] >= \n",
    "     medical_records_complete['medical_record_closing_date'] - pd.Timedelta(days=window_days)) &\n",
    "    (medical_records_complete['measurement_date_final'] <= \n",
    "     medical_records_complete['medical_record_closing_date'] + pd.Timedelta(days=window_days))\n",
    ")\n",
    "# Add a column that calculates the days passed between baseline and final measurements\n",
    "# This also helps identify cases where the medical record's duration and the actual followup time are very different\n",
    "medical_records_complete['days_between_measurements'] = (\n",
    "    (medical_records_complete['measurement_date_final'] - medical_records_complete['measurement_date_baseline']).dt.days\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Removing medical records with no associated measurements\n",
    "\"\"\"\n",
    "# As for some reason (unidentified as of 16Apr25) many medical records have no available measurements associated to them, \n",
    "# any such instances are dropped from the data frame. \n",
    "medical_records_complete = medical_records_complete.dropna(subset=['weight_kg_baseline', 'weight_kg_final'])\n",
    "\n",
    "\"\"\"\n",
    "Presenting and saving the output\n",
    "\"\"\"\n",
    "# Rename and reorder columns for better clarity and interpretability\n",
    "medical_records_complete = medical_records_complete.rename(columns={\n",
    "    'measurement_date_baseline': 'baseline_measurement_date',\n",
    "    'measurement_date_final': 'final_measurement_date',\n",
    "    'weight_kg_baseline': 'baseline_weight_kg',\n",
    "    'weight_kg_final': 'final_weight_kg', \n",
    "    'bmi_baseline': 'baseline_bmi',\n",
    "    'bmi_final': 'final_bmi'\n",
    "})\n",
    "desired_column_order = [\n",
    "    'patient_id',\n",
    "    'medical_record_id',\n",
    "    'genomics_sample_id',\n",
    "    'medical_record_creation_date',\n",
    "    'medical_record_closing_date',\n",
    "    'intervention_duration_days',\n",
    "    'baseline_measurement_date',\n",
    "    'final_measurement_date',\n",
    "    'days_between_measurements',\n",
    "    'baseline_measurement_inrange',\n",
    "    'final_measurement_inrange',\n",
    "    'birth_date',\n",
    "    'age',\n",
    "    'age_when_creating_record',\n",
    "    'sex',\n",
    "    'height_m',\n",
    "    'baseline_weight_kg',\n",
    "    'final_weight_kg',\n",
    "    'delta_weight_kg',\n",
    "    'baseline_bmi',\n",
    "    'final_bmi',\n",
    "    'delta_bmi',\n",
    "    'wc_cm_confirm_time',\n",
    "    'pnk_method',\n",
    "    'orders_in_medical_record',\n",
    "    'dietitian_visits',\n",
    "    'physical_activity',\n",
    "    'physical_activity_frequency',\n",
    "    'physical_inactivity_cause',\n",
    "    'weight_gain_cause',\n",
    "    'smoking',\n",
    "    'medications',\n",
    "    'hunger',\n",
    "    'satiety',\n",
    "    'emotional_eating',\n",
    "    'emotional_eating_value',\n",
    "    'quantity_control',\n",
    "    'impulse_control'\n",
    "]\n",
    "medical_records_complete = medical_records_complete[desired_column_order]\n",
    "# Save the complete medical records to the SQL database, and print a summary statement\n",
    "medical_records_complete.to_sql(\"medical_records_complete\", conn, if_exists=\"replace\", index=False)\n",
    "print(f\"Medical records table completed with sex and baseline/final weight data. \\n\" \n",
    "      f\"There are {len(medical_records_complete)} records available from {medical_records_complete['patient_id'].nunique()} patients.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the base input for survival analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, data frames specifically prepared for survival analysis are created. Time-to-event (days) of achieving 3 different weight loss targets (5-10-15%) in 3 different time frames (40-60-80 days) is analyzed. Relevant demographic, anthropometric and eating behavior variables are added to each analyzed medical record. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Starting Survival Analysis Script ==========\n",
      "Connecting to input database: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\emotional_all_notna.sqlite\n",
      "Connecting to output database: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\survival_analysis.sqlite\n",
      "--- Processing: sa_40d_5p ---\n",
      "--- Processing: sa_40d_10p ---\n",
      "--- Processing: sa_40d_15p ---\n",
      "--- Processing: sa_60d_5p ---\n",
      "--- Processing: sa_60d_10p ---\n",
      "--- Processing: sa_60d_15p ---\n",
      "--- Processing: sa_80d_5p ---\n",
      "--- Processing: sa_80d_10p ---\n",
      "--- Processing: sa_80d_15p ---\n",
      "--- Saving results to output database: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\survival_analysis.sqlite ---\n",
      "Saving table: sa_40d_5p (1664 rows)\n",
      "Saving table: sa_40d_10p (1664 rows)\n",
      "Saving table: sa_40d_15p (1664 rows)\n",
      "Saving table: sa_60d_5p (1664 rows)\n",
      "Saving table: sa_60d_10p (1664 rows)\n",
      "Saving table: sa_60d_15p (1664 rows)\n",
      "Saving table: sa_80d_5p (1664 rows)\n",
      "Saving table: sa_80d_10p (1664 rows)\n",
      "Saving table: sa_80d_15p (1664 rows)\n",
      "Saving summary table: survival_analysis_summary (9 rows)\n",
      "--- All results saved successfully ---\n",
      "\n",
      "--- Survival Analysis Summary ---\n",
      "  analysis_name  weight_loss_target  time_window  total_patients  achieved_target  dropout_count  avg_weight_loss_pct\n",
      "0     sa_40d_5p                   5           40            1664              987            584             4.211154\n",
      "1    sa_40d_10p                  10           40            1664              328            784             5.682572\n",
      "2    sa_40d_15p                  15           40            1664               24            789             5.747578\n",
      "3     sa_60d_5p                   5           60            1664             1019            611             4.272037\n",
      "4    sa_60d_10p                  10           60            1664              452            989             6.097951\n",
      "5    sa_60d_15p                  15           60            1664               83           1069             6.539477\n",
      "6     sa_80d_5p                   5           80            1664             1027            625             4.289597\n",
      "7    sa_80d_10p                  10           80            1664              506           1061             6.214567\n",
      "8    sa_80d_15p                  15           80            1664              138           1226             6.895883\n",
      "--- End Summary ---\n",
      "Analysis data successfully generated and saved to C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\survival_analysis.sqlite\n",
      "Closing database connections...\n",
      "========== Survival Analysis Script Finished ==========\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sqlite3\n",
    "from datetime import timedelta\n",
    "# Removed: import logging\n",
    "\n",
    "\"\"\"\n",
    "CONFIGURATION\n",
    "\"\"\"\n",
    "# Define directories and database paths - paper1_directory should be defined \n",
    "# in the first cell of this notebook chapter\n",
    "input_db_path = os.path.join(paper1_directory, 'emotional_all_notna.sqlite')\n",
    "input_measurements = \"measurements_with_metadata\"\n",
    "input_medical_records = \"medical_records_complete\"\n",
    "output_db_path = os.path.join(paper1_directory, 'survival_analysis.sqlite')\n",
    "\n",
    "# Define analysis parameters\n",
    "weight_loss_targets = [5, 10, 15]     # Weight loss target percentages\n",
    "time_windows = [40, 60, 80]       # Time windows (centers) in days\n",
    "window_span = 10                   # Permissible span around windows (+/- days)\n",
    "\n",
    "# Define the variables stored in medical_records_complete that are relevant for the analysis. \n",
    "# These include basic metadata like patient and record ID,\n",
    "# basic factors such as age and sex, \n",
    "# as well as the emotional and eating behavior variables pivotal to the research question. \n",
    "# The list can be amended on demand - \n",
    "# for example, right now it does not include medical record creating and closing dates. \n",
    "relevant_medical_values = ['patient_id', 'medical_record_id', 'sex', 'age',\n",
    "                             'height_m', 'baseline_bmi', 'hunger', 'satiety', 'emotional_eating',\n",
    "                             'emotional_eating_value', 'quantity_control', 'impulse_control']\n",
    "\n",
    "\"\"\"\n",
    "DATA LOADING & PREPARATION\n",
    "\"\"\"\n",
    "\n",
    "def load_measurements(connection):\n",
    "    \"\"\"\n",
    "    Load measurements from the measurement_with_metadata table; \n",
    "    make sure key values are in the correct format. \n",
    "    \"\"\"\n",
    "    query = f\"SELECT * FROM {input_measurements}\"\n",
    "    measurements = pd.read_sql_query(query, connection)\n",
    "    measurements['measurement_date'] = pd.to_datetime(measurements['measurement_date'], errors='coerce')\n",
    "    measurements['weight_kg'] = pd.to_numeric(measurements['weight_kg'], errors='coerce')\n",
    "    return measurements\n",
    "\n",
    "def load_medical_records(connection):\n",
    "    \"\"\"\n",
    "    Load medical records from the medical_records_complete table;\n",
    "    make sure date values are in datetime format. \n",
    "    The exact columns to be used are defined in the prepare_patient_data function.\n",
    "    \"\"\"\n",
    "    query = f\"SELECT * FROM {input_medical_records}\"\n",
    "    medical_records = pd.read_sql_query(query, connection)\n",
    "    medical_records['medical_record_creation_date'] = pd.to_datetime(medical_records['medical_record_creation_date'], errors='coerce')\n",
    "    return medical_records\n",
    "\n",
    "def prepare_patient_data(measurements, medical_records):\n",
    "    \"\"\"\n",
    "    Filter measurements to only include those from the earliest medical record for each patient.\n",
    "    Merge measurements with relevant medical record data, including the pivotal eating behavior scores. \n",
    "    \"\"\"\n",
    "    # Filter measurements to only include those from the first treatment record of each patient\n",
    "    earliest_records_with_data = measurements.sort_values('measurement_date')\\\n",
    "        .groupby('patient_id')['medical_record_id']\\\n",
    "        .first()\\\n",
    "        .reset_index()\n",
    "    filtered_measurements = pd.merge(\n",
    "        measurements,\n",
    "        earliest_records_with_data,\n",
    "        on=['patient_id', 'medical_record_id'],\n",
    "        how='inner'\n",
    "    )\n",
    "    # Identify the baseline measurement in each record\n",
    "    baseline_data = filtered_measurements.sort_values('measurement_date')\\\n",
    "                                       .groupby(['patient_id', 'medical_record_id'])\\\n",
    "                                       .first()\\\n",
    "                                       .reset_index()\n",
    "\n",
    "    cols_to_select = [col for col in relevant_medical_values if col in medical_records.columns]\n",
    "    medical_record_data = medical_records[cols_to_select]\n",
    "    # Merge baseline measurements with relevant medical record data\n",
    "    prepared_data = pd.merge(\n",
    "        baseline_data,\n",
    "        medical_record_data,\n",
    "        on=['patient_id', 'medical_record_id'],\n",
    "        how='left' # Keep all baseline data\n",
    "    )\n",
    "    return prepared_data, filtered_measurements\n",
    "\n",
    "\"\"\"\n",
    "CALCULATE WEIGHT LOSS OUTCOMES\n",
    "\"\"\"\n",
    "\n",
    "def _get_patient_baseline(patient_data, patient_id, medical_record_id):\n",
    "    \"\"\"\n",
    "    Get the baseline data for each patient's corresponding medical record.\n",
    "    \"\"\"\n",
    "    patient_baseline = patient_data[\n",
    "        (patient_data['patient_id'] == patient_id) &\n",
    "        (patient_data['medical_record_id'] == medical_record_id)\n",
    "    ]\n",
    "    if len(patient_baseline) == 0:\n",
    "        print(f\"WARN: No baseline data found for patient {patient_id}, record {medical_record_id}. Skipping.\")\n",
    "        return None\n",
    "    return patient_baseline.iloc[0]\n",
    "\n",
    "def _check_target_achievement(measurements_within_window, baseline_weight, weight_loss_target):\n",
    "    \"\"\"\n",
    "    Check if the weight loss target was achieved in some of the given measurements.\n",
    "    \"\"\"\n",
    "    # Set default to False/None\n",
    "    target_achieved = False\n",
    "    first_success_measurement = None\n",
    "    # Calculate weight loss percentage for each measurement in the window, \n",
    "    # and check if it meets the target\n",
    "    for _, row in measurements_within_window.iterrows():\n",
    "        current_weight = row['weight_kg']\n",
    "        if baseline_weight is not None and baseline_weight > 0:\n",
    "            current_weight_loss = ((baseline_weight - current_weight) / baseline_weight) * 100\n",
    "            if round(current_weight_loss, 2) >= weight_loss_target:\n",
    "                target_achieved = True\n",
    "                first_success_measurement = row\n",
    "                break # Stop at the first success; if that is not identified, target_achieved remains False as by default\n",
    "    return target_achieved, first_success_measurement\n",
    "\n",
    "def _determine_final_measurement(target_achieved, first_success_row, measurements_around_cutoff,\n",
    "                                measurements_within_window, baseline_date, window_center):\n",
    "    \"\"\"\n",
    "    Determine the final measurement based on success or censoring (ie. completion without success) rules.\n",
    "    \"\"\"\n",
    "    # Set final measurement to None by default\n",
    "    final_measurement = None\n",
    "    # Set target final date based on the given time window\n",
    "    target_date = baseline_date + timedelta(days=window_center)\n",
    "    # If weight loss target was achieved at any point of the followup time window,\n",
    "    # use the first success measurement as the final measurement.  \n",
    "    if target_achieved:\n",
    "        final_measurement = first_success_row\n",
    "    # In case of no success, the date closest to the target date is used as the final measurement. \n",
    "    elif not measurements_around_cutoff.empty:\n",
    "        measurements_around_cutoff = measurements_around_cutoff.copy()\n",
    "        measurements_around_cutoff['distance_to_center'] = abs(\n",
    "            (measurements_around_cutoff['measurement_date'] - target_date).dt.days\n",
    "        )\n",
    "        closest_measurement_idx = measurements_around_cutoff['distance_to_center'].idxmin()\n",
    "        final_measurement = measurements_around_cutoff.loc[closest_measurement_idx]\n",
    "    # In case of no success nor completion (delayed dropout), use the last available measurement as the final measurement\n",
    "    elif not measurements_within_window.empty:\n",
    "        final_measurement = measurements_within_window.sort_values('measurement_date').iloc[-1]\n",
    "    # Else: Instant dropout, final_measurement remains None, \n",
    "    # and is set to the baseline measurementin the calculate_outcome_metrics function.\n",
    "    return final_measurement\n",
    "\n",
    "def _calculate_outcome_metrics(baseline_row, final_measurement_row):\n",
    "    \"\"\"\n",
    "    Calculate follow-up lenght and weight loss percentage based on baseline and final measurement.\n",
    "    \"\"\"\n",
    "    # Identify the baseline measurement\n",
    "    baseline_date = baseline_row['measurement_date']\n",
    "    baseline_weight = baseline_row['weight_kg']\n",
    "    # In patients that have at least one followup measurement, identify the end date and final weight, \n",
    "    # to calculate followup length and weight loss in kg and %\n",
    "    if final_measurement_row is not None:\n",
    "        end_date = final_measurement_row['measurement_date']\n",
    "        final_weight = final_measurement_row['weight_kg']\n",
    "        followup_period = (end_date - baseline_date).days\n",
    "        weight_loss_kg = baseline_weight - final_weight\n",
    "        weight_loss_pct = ((baseline_weight - final_weight) / baseline_weight) * 100\n",
    "    # In patients that have no followup measurement (instant dropouts), \n",
    "    # the end date and final weight are set to the baseline values, \n",
    "    # and followup length and weight loss are set to 0. \n",
    "    else: \n",
    "        end_date = baseline_date\n",
    "        final_weight = baseline_weight\n",
    "        followup_period = 0\n",
    "        weight_loss_kg = 0\n",
    "        weight_loss_pct = 0\n",
    "    return {\n",
    "        'end_date': end_date,\n",
    "        'final_weight': final_weight,\n",
    "        'followup_period': followup_period,\n",
    "        'weight_loss_kg': weight_loss_kg,\n",
    "        'weight_loss_pct': round(weight_loss_pct, 2)\n",
    "    }\n",
    "\n",
    "\"\"\"\n",
    "CORE ANALYSIS FUNCTION\n",
    "\"\"\"\n",
    "\n",
    "def calculate_weight_loss_outcome(patient_data, filtered_measurements, weight_loss_target, window_center, window_span):\n",
    "    \"\"\"\n",
    "    Calculate weight loss outcomes for each patient in a survival analysis-ready format. \n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store results, and group measurements by patient and medical record ID\n",
    "    results = []\n",
    "    grouped_measurements = filtered_measurements.groupby(['patient_id', 'medical_record_id'])\n",
    "    # Iterate through each group within measurements. \n",
    "    for (patient_id, medical_record_id), group in grouped_measurements:\n",
    "        # 1. Identify baseline measurement date and weight\n",
    "        baseline_row = _get_patient_baseline(patient_data, patient_id, medical_record_id)\n",
    "        if baseline_row is None: continue\n",
    "        baseline_date = baseline_row['measurement_date']\n",
    "        baseline_weight = baseline_row['weight_kg']\n",
    "        # 2. Define observation time windows and group measurements within the defined window\n",
    "        # Calculations are done for both the complete observation period, \n",
    "        # as well as the period strictry around the cutoff date, within the defined permissivity window. \n",
    "        min_window_date = baseline_date + timedelta(days=(window_center - window_span))\n",
    "        max_window_date = baseline_date + timedelta(days=(window_center + window_span))\n",
    "        measurements_within_window = group[\n",
    "            (group['measurement_date'] > baseline_date) &\n",
    "            (group['measurement_date'] <= max_window_date)\n",
    "        ].sort_values('measurement_date')\n",
    "        measurements_around_cutoff = group[\n",
    "            (group['measurement_date'] >= min_window_date) &\n",
    "            (group['measurement_date'] <= max_window_date)\n",
    "        ]\n",
    "        # 3. Check whether target weight loss was achieved in the defined time window\n",
    "        target_achieved, first_success_row = _check_target_achievement(\n",
    "            measurements_within_window, baseline_weight, weight_loss_target\n",
    "        )\n",
    "        # 4. Identify the last measurement date within the time window,\n",
    "        # whether based on target achievment or followup completion\n",
    "        final_measurement_row = _determine_final_measurement(\n",
    "            target_achieved, first_success_row, measurements_around_cutoff,\n",
    "            measurements_within_window, baseline_date, window_center\n",
    "        )\n",
    "        # 5. Check for dropout status - instant dropouts are those who have no second measurement, \n",
    "        # while delayed dropouts are those who have not reached target, \n",
    "        # and their final measurement is before the cutoff window. \n",
    "        is_instant_dropout = final_measurement_row is None\n",
    "        is_delayed_dropout = (not target_achieved and\n",
    "                              final_measurement_row is not None and\n",
    "                              final_measurement_row['measurement_date'] < min_window_date)\n",
    "        dropout = is_instant_dropout or is_delayed_dropout\n",
    "        success = target_achieved\n",
    "        # 6. Calculate metrics like final date and weight, followup length and weight lost. \n",
    "        outcome_metrics = _calculate_outcome_metrics(baseline_row, final_measurement_row)\n",
    "\n",
    "        \"\"\"ARE WE GOING TO MODIFY AVG CALCS?\"\"\"\n",
    "\n",
    "        # # 7. \n",
    "        # # --- NEW: Calculate metrics based *always* on the last measurement within the window ---\n",
    "        # actual_last_measurement_row = None\n",
    "        # if not measurements_within_window.empty:\n",
    "        #     actual_last_measurement_row = measurements_within_window.iloc[-1]\n",
    "\n",
    "        # # Use the same helper, but pass the actual last measurement row\n",
    "        # actual_end_metrics = _calculate_outcome_metrics(baseline_row, actual_last_measurement_row)\n",
    "        # actual_wl_pct_at_window_end = actual_end_metrics['weight_loss_pct']\n",
    "        # # --- End NEW ---\n",
    "\n",
    "\n",
    "\n",
    "        # 8. Assemble the result - this is where the output tables' columns are defined. \n",
    "        # If additional variables are inserted at an earlier part of the code, \n",
    "        # they need to be mentioned here as well. \n",
    "        result = {\n",
    "            'patient_id': patient_id,\n",
    "            'medical_record_id': medical_record_id,\n",
    "            'baseline_date': baseline_date,\n",
    "            'end_date': outcome_metrics['end_date'],\n",
    "            'followup_period': outcome_metrics['followup_period'],\n",
    "            'baseline_weight': baseline_weight,\n",
    "            'final_weight': outcome_metrics['final_weight'],\n",
    "            'weight_loss_kg': outcome_metrics['weight_loss_kg'],\n",
    "            'weight_loss_pct': outcome_metrics['weight_loss_pct'],\n",
    "            # \n",
    "            f'{weight_loss_target}pct_achieved': success,\n",
    "            'dropout': dropout,\n",
    "            # Add baseline characteristics safely using .get()\n",
    "            'sex': baseline_row.get('sex'),\n",
    "            'age': baseline_row.get('age'),\n",
    "            'height_m': baseline_row.get('height_m'),\n",
    "            'baseline_bmi': baseline_row.get('bmi'),\n",
    "            'hunger': baseline_row.get('hunger'),\n",
    "            'satiety': baseline_row.get('satiety'),\n",
    "            'emotional_eating': baseline_row.get('emotional_eating'),\n",
    "            'emotional_eating_value': baseline_row.get('emotional_eating_value'),\n",
    "            'quantity_control': baseline_row.get('quantity_control'),\n",
    "            'impulse_control': baseline_row.get('impulse_control')\n",
    "        }\n",
    "        results.append(result)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "MAIN ORCHESTRATION FUNCTION\n",
    "\"\"\"\n",
    "\n",
    "def generate_survival_analysis_datasets(input_connection, output_connection, weight_loss_targets, time_windows, window_span=10):\n",
    "    \"\"\"\n",
    "    The main function to orchestrate the survival analysis process, calling all previously defined functions in an organized manner. \n",
    "    Generate survival analysis datasets for multiple weight loss targets and observation time windows.\n",
    "    Targets and timeframes are defined in the configuration section at the beginning of the code module.\n",
    "    Save data to a separate SQLite database. \n",
    "    \"\"\"\n",
    "    # 1. Load and prepare input data\n",
    "    measurements = load_measurements(input_connection)\n",
    "    medical_records = load_medical_records(input_connection)\n",
    "    patient_data, filtered_measurements = prepare_patient_data(measurements, medical_records)\n",
    "    if patient_data.empty:\n",
    "        print(\"ERROR: Prepared patient data is empty. Cannot proceed.\")\n",
    "        return {}, pd.DataFrame()\n",
    "    # 2. Calculate weight loss outcomes for each target-timeframe combination. \n",
    "    # Targets and timeframes are defined in the config section of the script. \n",
    "    # Initialize a results dictionary and a list for summary statistics. \n",
    "    results = {}\n",
    "    summary_list = []\n",
    "    for window in sorted(time_windows):\n",
    "        for target in sorted(weight_loss_targets):\n",
    "            # Name each instance accordingly, where sa stands for survival analysis, \n",
    "            # and the numbers indicate the time window and target percentage.\n",
    "            name = f\"sa_{window}d_{target}p\"\n",
    "            print(f\"--- Processing: {name} ---\") # Minimal progress indication\n",
    "            result_df = calculate_weight_loss_outcome(\n",
    "                patient_data,\n",
    "                filtered_measurements,\n",
    "                target,\n",
    "                window,\n",
    "                window_span # Defined in config - the permissivity window around the followup cutoff time\n",
    "            )\n",
    "            results[name] = result_df\n",
    "            # Add the calculated instances to the summary statistics list. \n",
    "            if not result_df.empty:\n",
    "                summary_row = {\n",
    "                    'analysis_name': name,\n",
    "                    'weight_loss_target': target,\n",
    "                    'time_window': window,\n",
    "                    'total_patients': len(result_df),\n",
    "                    'achieved_target': int(result_df[f'{target}pct_achieved'].sum()),\n",
    "                    'dropout_count': int(result_df['dropout'].sum()),\n",
    "                    'avg_weight_loss_pct': result_df['weight_loss_pct'].mean() if not result_df['weight_loss_pct'].isnull().all() else 0\n",
    "                }\n",
    "                summary_list.append(summary_row)\n",
    "            else:\n",
    "                 print(f\"WARN: No results generated for {name}. Skipping summary entry.\")\n",
    "    # Turn the summary statistics list into a data frame\n",
    "    summary = pd.DataFrame(summary_list)\n",
    "\n",
    "    # 3. Save the analysis results (9 tables by default) to the SQLite database defined in the config section\n",
    "    print(f\"--- Saving results to output database: {output_db_path} ---\")\n",
    "    # Save individual tables\n",
    "    for name, df in results.items():\n",
    "        print(f\"Saving table: {name} ({len(df)} rows)\")\n",
    "        df.to_sql(name, output_connection, if_exists='replace', index=False)\n",
    "    # Save the summary stats table in the database as well\n",
    "    print(f\"Saving summary table: survival_analysis_summary ({len(summary)} rows)\")\n",
    "    summary.to_sql('survival_analysis_summary', output_connection, if_exists='replace', index=False)\n",
    "    output_connection.commit() # Ensure changes are saved\n",
    "    print(\"--- All results saved successfully ---\")\n",
    "    return results, summary\n",
    "\n",
    "\"\"\"\n",
    "EXECUTION BLOCK\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "This part of the code calls all the functions and executes the code. \n",
    "Currently it has a lot of debug messages and error handling, which might be an overkill, \n",
    "but overall, it should not affect transparency of the code.\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"========== Starting Survival Analysis Script ==========\")\n",
    "    # By default, connections are set to None, and will be established in the try block.\n",
    "    input_conn = None\n",
    "    output_conn = None\n",
    "    try:\n",
    "        # Connect to in-and output databases\n",
    "        print(f\"Connecting to input database: {input_db_path}\")\n",
    "        if not os.path.exists(input_db_path):\n",
    "             raise FileNotFoundError(f\"Input database not found at {input_db_path}\")\n",
    "        input_conn = sqlite3.connect(input_db_path)\n",
    "        print(f\"Connecting to output database: {output_db_path}\")\n",
    "        output_conn = sqlite3.connect(output_db_path)\n",
    "        # Run the main analysis function\n",
    "        results, summary = generate_survival_analysis_datasets(\n",
    "            input_conn,\n",
    "            output_conn,\n",
    "            weight_loss_targets,\n",
    "            time_windows,\n",
    "            window_span\n",
    "        )\n",
    "        # Display summary if successful\n",
    "        if not summary.empty:\n",
    "            print(\"\\n--- Survival Analysis Summary ---\")\n",
    "            print(summary.to_string()) # Use print for console display\n",
    "            print(\"--- End Summary ---\")\n",
    "        else:\n",
    "            print(\"WARN: Analysis completed, but the summary table is empty.\")\n",
    "        print(f\"Analysis data successfully generated and saved to {output_db_path}\")\n",
    "\n",
    "    # Minimal error handling for critical failures\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"ERROR: Database file not found - {e}\")\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"ERROR: SQLite database error - {e}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"ERROR: Data processing error - {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: An unexpected error occurred - {e}\")\n",
    "        # Consider adding traceback for debugging complex errors:\n",
    "        # import traceback\n",
    "        # print(traceback.format_exc())\n",
    "    finally:\n",
    "        # Ensure connections are closed\n",
    "        print(\"Closing database connections...\")\n",
    "        if input_conn:\n",
    "            input_conn.close()\n",
    "        if output_conn:\n",
    "            output_conn.close()\n",
    "        print(\"========== Survival Analysis Script Finished ==========\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconsidered input structure for SA 18-19 Apr v1 - this is still not perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Starting Wide Survival Analysis Script ==========\n",
      "Connecting to input database: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\emotional_all_notna.sqlite\n",
      "Connecting to output database: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\shukishukishuuu.sqlite\n",
      "Loading measurements...\n",
      "Loading medical records...\n",
      "Preparing patient data...\n",
      "Calculating wide outcomes for all patients...\n",
      "--- Saving results to output database: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\shukishukishuuu.sqlite ---\n",
      "Saving table: shukishukishuuu (1664 rows)\n",
      "--- Wide table saved successfully ---\n",
      "\n",
      "--- Wide Analysis Table Generation Summary ---\n",
      "Generated table 'shukishukishuuu' with 1664 rows and 49 columns.\n",
      "--- End Summary ---\n",
      "Analysis data successfully generated and saved to C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\shukishukishuuu.sqlite\n",
      "Closing database connections...\n",
      "========== Wide Survival Analysis Script Finished ==========\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sqlite3\n",
    "from datetime import timedelta\n",
    "import numpy as np # Import numpy for NaN\n",
    "\n",
    "\"\"\"\n",
    "CONFIGURATION\n",
    "\"\"\"\n",
    "# Define directories and database paths - paper1_directory should be defined\n",
    "# in the first cell of this notebook chapter\n",
    "input_db_path = os.path.join(paper1_directory, 'emotional_all_notna.sqlite')\n",
    "input_measurements = \"measurements_with_metadata\"\n",
    "input_medical_records = \"medical_records_complete\"\n",
    "# Modified: Output database path remains the same, but will contain a different table structure\n",
    "output_db_path = os.path.join(paper1_directory, 'shukishukishuuu.sqlite')\n",
    "output_table_name = \"shukishukishuuu\" # Define the name for the single wide table\n",
    "\n",
    "# Define analysis parameters\n",
    "weight_loss_targets = [5, 10, 15]     # Weight loss target percentages\n",
    "time_windows = [40, 60, 80]       # Time windows (centers) in days\n",
    "window_span = 10                   # Permissible span around windows (+/- days)\n",
    "\n",
    "# Define the variables stored in medical_records_complete that are relevant for the analysis.\n",
    "# These include basic metadata like patient and record ID,\n",
    "# basic factors such as age and sex,\n",
    "# as well as the emotional and eating behavior variables pivotal to the research question.\n",
    "# The list can be amended on demand -\n",
    "# for example, right now it does not include medical record creating and closing dates.\n",
    "relevant_medical_values = ['patient_id', 'medical_record_id', 'sex', 'age',\n",
    "                             'height_m', 'baseline_bmi', 'hunger', 'satiety', 'emotional_eating',\n",
    "                             'emotional_eating_value', 'quantity_control', 'impulse_control', 'genomics_sample_id']\n",
    "\n",
    "\"\"\"\n",
    "DATA LOADING & PREPARATION\n",
    "\"\"\"\n",
    "\n",
    "def load_measurements(connection):\n",
    "    \"\"\"\n",
    "    Load measurements from the measurement_with_metadata table;\n",
    "    make sure key values are in the correct format.\n",
    "    \"\"\"\n",
    "    query = f\"SELECT * FROM {input_measurements}\"\n",
    "    measurements = pd.read_sql_query(query, connection)\n",
    "    measurements['measurement_date'] = pd.to_datetime(measurements['measurement_date'], errors='coerce')\n",
    "    measurements['weight_kg'] = pd.to_numeric(measurements['weight_kg'], errors='coerce')\n",
    "    # Ensure sorting for consistent 'first'/'last' operations later\n",
    "    measurements = measurements.sort_values(['patient_id', 'medical_record_id', 'measurement_date'])\n",
    "    return measurements\n",
    "\n",
    "def load_medical_records(connection):\n",
    "    \"\"\"\n",
    "    Load medical records from the medical_records_complete table;\n",
    "    make sure date values are in datetime format.\n",
    "    The exact columns to be used are defined in the prepare_patient_data function.\n",
    "    \"\"\"\n",
    "    query = f\"SELECT * FROM {input_medical_records}\"\n",
    "    medical_records = pd.read_sql_query(query, connection)\n",
    "    # Ensure relevant date columns are datetime\n",
    "    for col in ['medical_record_creation_date', 'baseline_measurement_date', 'final_measurement_date']:\n",
    "         if col in medical_records.columns:\n",
    "             medical_records[col] = pd.to_datetime(medical_records[col], errors='coerce')\n",
    "    return medical_records\n",
    "\n",
    "def prepare_patient_data(measurements, medical_records):\n",
    "    \"\"\"\n",
    "    Filter measurements to only include those from the earliest medical record for each patient.\n",
    "    Merge measurements with relevant medical record data, including the pivotal eating behavior scores.\n",
    "    \"\"\"\n",
    "    # Filter measurements to only include those from the first treatment record of each patient\n",
    "    # Ensure measurements are sorted before grouping\n",
    "    measurements = measurements.sort_values(['patient_id', 'measurement_date'])\n",
    "    earliest_records_with_data = measurements.groupby('patient_id')['medical_record_id'].first().reset_index()\n",
    "\n",
    "    filtered_measurements = pd.merge(\n",
    "        measurements,\n",
    "        earliest_records_with_data,\n",
    "        on=['patient_id', 'medical_record_id'],\n",
    "        how='inner'\n",
    "    )\n",
    "    # Ensure filtered_measurements are sorted for baseline identification\n",
    "    filtered_measurements = filtered_measurements.sort_values(['patient_id', 'medical_record_id', 'measurement_date'])\n",
    "\n",
    "    # Identify the baseline measurement row for each record (the first measurement in the filtered set)\n",
    "    baseline_data_rows = filtered_measurements.groupby(['patient_id', 'medical_record_id']).first().reset_index()\n",
    "\n",
    "    # Select only relevant columns from medical_records to merge\n",
    "    cols_to_select = [col for col in relevant_medical_values if col in medical_records.columns]\n",
    "    medical_record_subset = medical_records[cols_to_select]\n",
    "\n",
    "    # Merge baseline measurement info with the selected medical record data\n",
    "    # Use baseline_data_rows which contains the actual first measurement details\n",
    "    prepared_data = pd.merge(\n",
    "        baseline_data_rows[['patient_id', 'medical_record_id', 'measurement_date', 'weight_kg']], # Get baseline date/weight from actual first measurement\n",
    "        medical_record_subset,\n",
    "        on=['patient_id', 'medical_record_id'],\n",
    "        how='left' # Keep all baseline measurements\n",
    "    )\n",
    "    # Rename columns for clarity before returning\n",
    "    prepared_data = prepared_data.rename(columns={'measurement_date': 'baseline_date', 'weight_kg': 'baseline_weight_kg'})\n",
    "\n",
    "    # Add baseline_bmi from medical_records if available and not already present from measurement merge\n",
    "    if 'baseline_bmi' in medical_record_subset.columns and 'baseline_bmi' not in prepared_data.columns:\n",
    "         prepared_data = pd.merge(\n",
    "              prepared_data,\n",
    "              medical_record_subset[['patient_id', 'medical_record_id', 'baseline_bmi']],\n",
    "              on=['patient_id', 'medical_record_id'],\n",
    "              how='left'\n",
    "         )\n",
    "\n",
    "    return prepared_data, filtered_measurements\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "MODIFIED: CALCULATE WEIGHT LOSS OUTCOMES FOR WIDE TABLE\n",
    "\"\"\"\n",
    "\n",
    "def _get_patient_baseline(patient_data, patient_id, medical_record_id):\n",
    "    \"\"\"\n",
    "    Get the prepared baseline data for a patient's specific medical record.\n",
    "    (Function remains largely the same, but operates on the prepared_data structure)\n",
    "    \"\"\"\n",
    "    patient_baseline = patient_data[\n",
    "        (patient_data['patient_id'] == patient_id) &\n",
    "        (patient_data['medical_record_id'] == medical_record_id)\n",
    "    ]\n",
    "    if patient_baseline.empty:\n",
    "        print(f\"WARN: No baseline data found for patient {patient_id}, record {medical_record_id}. Skipping.\")\n",
    "        return None\n",
    "    # Ensure we return a Series for consistent access\n",
    "    return patient_baseline.iloc[0]\n",
    "\n",
    "def _calculate_wl_metrics(baseline_weight, current_weight):\n",
    "    \"\"\" Helper to calculate weight loss kg and % \"\"\"\n",
    "    if pd.isna(baseline_weight) or pd.isna(current_weight) or baseline_weight == 0:\n",
    "        return np.nan, np.nan\n",
    "    wl_kg = baseline_weight - current_weight\n",
    "    wl_pct = (wl_kg / baseline_weight) * 100\n",
    "    return wl_kg, round(wl_pct, 2)\n",
    "\n",
    "# Removed _check_target_achievement, _determine_final_measurement, _calculate_outcome_metrics\n",
    "# Their logic will be integrated into the main calculation function.\n",
    "\n",
    "def calculate_wide_patient_outcomes(prepared_patient_data, filtered_measurements, weight_loss_targets, time_windows, window_span):\n",
    "    \"\"\"\n",
    "    Calculate all required outcomes (fixed-time and time-to-event) for each patient\n",
    "    and return a list of dictionaries, each representing a row in the wide table.\n",
    "    \"\"\"\n",
    "    results_list = []\n",
    "    # Group all measurements to only get the relevant (earliest) medical record per patient\n",
    "    grouped_measurements = filtered_measurements.groupby(['patient_id', 'medical_record_id'])\n",
    "\n",
    "    for (patient_id, medical_record_id), group in grouped_measurements:\n",
    "        # 1. Get Baseline Info\n",
    "        baseline_info = _get_patient_baseline(prepared_patient_data, patient_id, medical_record_id)\n",
    "        if baseline_info is None:\n",
    "            continue\n",
    "\n",
    "        baseline_date = baseline_info['baseline_date']\n",
    "        baseline_weight = baseline_info['baseline_weight_kg']\n",
    "\n",
    "        # Initialize result dictionary with baseline info\n",
    "        result = {\n",
    "            'patient_ID': patient_id, \n",
    "            'medical_record_ID': medical_record_id,\n",
    "            'baseline_date': baseline_date,\n",
    "            'baseline_weight_kg': baseline_weight,\n",
    "            # Add other baseline characteristics safely using .get() or direct access\n",
    "            'sex': baseline_info.get('sex'),\n",
    "            'age': baseline_info.get('age'),\n",
    "            'height_m': baseline_info.get('height_m'),\n",
    "            'baseline_bmi': baseline_info.get('baseline_bmi'), # Get BMI from prepared data\n",
    "            'hunger': baseline_info.get('hunger'),\n",
    "            'satiety': baseline_info.get('satiety'),\n",
    "            'emotional_eating': baseline_info.get('emotional_eating'),\n",
    "            'emotional_eating_value': baseline_info.get('emotional_eating_value'),\n",
    "            'quantity_control': baseline_info.get('quantity_control'),\n",
    "            'impulse_control': baseline_info.get('impulse_control')\n",
    "        }\n",
    "\n",
    "        # Get all measurements *after* baseline for this group\n",
    "        followup_measurements = group[group['measurement_date'] > baseline_date].sort_values('measurement_date')\n",
    "\n",
    "        # 2. Calculate Overall Follow-up Metrics\n",
    "        if not followup_measurements.empty:\n",
    "            last_measurement = followup_measurements.iloc[-1]\n",
    "            result['last_aval_date'] = last_measurement['measurement_date']\n",
    "            result['total_followup_days'] = (last_measurement['measurement_date'] - baseline_date).days\n",
    "            result['last_aval_weight_kg'] = last_measurement['weight_kg']\n",
    "            wl_kg, wl_pct = _calculate_wl_metrics(baseline_weight, last_measurement['weight_kg'])\n",
    "            result['total_wl_kg'] = wl_kg\n",
    "            result['total_wl_%'] = wl_pct\n",
    "        else:\n",
    "            # Handle instant dropouts (only baseline measurement exists)\n",
    "            result['last_aval_date'] = baseline_date\n",
    "            result['total_followup_days'] = 0\n",
    "            result['last_aval_weight_kg'] = baseline_weight\n",
    "            result['total_wl_kg'] = 0.0\n",
    "            result['total_wl_%'] = 0.0\n",
    "\n",
    "        # 3. Calculate Fixed-Timepoint Metrics (for each time window)\n",
    "        for window_center in time_windows:\n",
    "            min_window_date = baseline_date + timedelta(days=(window_center - window_span))\n",
    "            max_window_date = baseline_date + timedelta(days=(window_center + window_span))\n",
    "            target_date = baseline_date + timedelta(days=window_center)\n",
    "\n",
    "            # Find measurements around the cutoff window\n",
    "            measurements_around_cutoff = followup_measurements[\n",
    "                (followup_measurements['measurement_date'] >= min_window_date) &\n",
    "                (followup_measurements['measurement_date'] <= max_window_date)\n",
    "            ]\n",
    "\n",
    "            measurement_at_window = None\n",
    "            is_dropout_at_window = True # Assume dropout unless proven otherwise\n",
    "\n",
    "            if not measurements_around_cutoff.empty:\n",
    "                # Find measurement closest to the window center\n",
    "                measurements_around_cutoff = measurements_around_cutoff.copy()\n",
    "                measurements_around_cutoff['distance_to_center'] = abs(\n",
    "                    (measurements_around_cutoff['measurement_date'] - target_date).dt.days\n",
    "                )\n",
    "                closest_measurement_idx = measurements_around_cutoff['distance_to_center'].idxmin()\n",
    "                measurement_at_window = measurements_around_cutoff.loc[closest_measurement_idx]\n",
    "                is_dropout_at_window = False # Measurement found within/around window\n",
    "            elif not followup_measurements.empty:\n",
    "                 # No measurement in cutoff window, check if *any* followup exists before the window\n",
    "                 last_followup_before_window = followup_measurements[followup_measurements['measurement_date'] < min_window_date]\n",
    "                 if not last_followup_before_window.empty:\n",
    "                      # Use the latest measurement before the window started\n",
    "                      measurement_at_window = last_followup_before_window.iloc[-1]\n",
    "                      # Still considered dropout *for this window* as they didn't reach it\n",
    "                      is_dropout_at_window = True\n",
    "                 else:\n",
    "                      \"\"\"!!!CHECK this logic, it might get tricky!!!\"\"\"\n",
    "                      # Followup exists, but only *after* the window (unlikely but possible)\n",
    "                      # Treat as dropout for this window, no relevant measurement\n",
    "                      measurement_at_window = None\n",
    "                      is_dropout_at_window = True\n",
    "            else:\n",
    "                 # Instant dropout (no followup measurements at all)\n",
    "                 measurement_at_window = None\n",
    "                 is_dropout_at_window = True\n",
    "\n",
    "\n",
    "            # Populate results for this time window\n",
    "            prefix = f\"{window_center}d\"\n",
    "            if measurement_at_window is not None:\n",
    "                result[f'{prefix}_weight_kg'] = measurement_at_window['weight_kg']\n",
    "                wl_kg, wl_pct = _calculate_wl_metrics(baseline_weight, measurement_at_window['weight_kg'])\n",
    "                result[f'wl_{prefix}_kg'] = wl_kg\n",
    "                result[f'wl_{prefix}_%'] = wl_pct\n",
    "                result[f'{prefix}_date'] = measurement_at_window['measurement_date']\n",
    "                result[f'days_to_{prefix}_measurement'] = (measurement_at_window['measurement_date'] - baseline_date).days\n",
    "            else:\n",
    "                # No relevant measurement found for this window\n",
    "                result[f'{prefix}_weight_kg'] = np.nan\n",
    "                result[f'wl_{prefix}_kg'] = np.nan\n",
    "                result[f'wl_{prefix}_%'] = np.nan\n",
    "                result[f'{prefix}_date'] = pd.NaT\n",
    "                result[f'days_to_{prefix}_measurement'] = np.nan\n",
    "\n",
    "            result[f'{prefix}_dropout'] = is_dropout_at_window\n",
    "\n",
    "\n",
    "        # 4. Calculate Time-to-Event Metrics (for each weight loss target)\n",
    "        for target in weight_loss_targets:\n",
    "            target_achieved = False\n",
    "            first_success_measurement = None\n",
    "            actual_wl_at_success = np.nan\n",
    "\n",
    "            # Check all followup measurements for the first success\n",
    "            for _, row in followup_measurements.iterrows():\n",
    "                current_weight = row['weight_kg']\n",
    "                if baseline_weight is not None and baseline_weight > 0:\n",
    "                    current_weight_loss_pct = ((baseline_weight - current_weight) / baseline_weight) * 100\n",
    "                    if round(current_weight_loss_pct, 2) >= target:\n",
    "                        target_achieved = True\n",
    "                        first_success_measurement = row\n",
    "                        actual_wl_at_success = round(current_weight_loss_pct, 2)\n",
    "                        break # Stop at the first success\n",
    "\n",
    "            # Populate results for this target\n",
    "            prefix = f\"{target}%_wl\"\n",
    "            result[f'{prefix}_achieved'] = target_achieved\n",
    "            if target_achieved and first_success_measurement is not None:\n",
    "                result[f'{prefix}_%'] = actual_wl_at_success\n",
    "                result[f'{prefix}_date'] = first_success_measurement['measurement_date']\n",
    "                result[f'days_to_{prefix}'] = (first_success_measurement['measurement_date'] - baseline_date).days\n",
    "            else:\n",
    "                result[f'{prefix}_%'] = np.nan\n",
    "                result[f'{prefix}_date'] = pd.NaT\n",
    "                result[f'days_to_{prefix}'] = np.nan # Or perhaps total_followup_days if censored? Check analysis plan needs. NaN is safer.\n",
    "\n",
    "        results_list.append(result)\n",
    "\n",
    "    return pd.DataFrame(results_list)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "MODIFIED: MAIN ORCHESTRATION FUNCTION FOR WIDE TABLE\n",
    "\"\"\"\n",
    "\n",
    "def generate_wide_analysis_dataset(input_connection, output_connection, weight_loss_targets, time_windows, window_span=10):\n",
    "    \"\"\"\n",
    "    Orchestrates the process to generate the single wide survival analysis dataset.\n",
    "    Loads data, prepares patient baseline info, calculates all outcomes per patient,\n",
    "    and saves the resulting wide DataFrame to the output database.\n",
    "    \"\"\"\n",
    "    # 1. Load and prepare input data\n",
    "    print(\"Loading measurements...\")\n",
    "    measurements = load_measurements(input_connection)\n",
    "    print(\"Loading medical records...\")\n",
    "    medical_records = load_medical_records(input_connection)\n",
    "    print(\"Preparing patient data...\")\n",
    "    prepared_data, filtered_measurements = prepare_patient_data(measurements, medical_records)\n",
    "\n",
    "    if prepared_data.empty:\n",
    "        print(\"ERROR: Prepared patient data is empty. Cannot proceed.\")\n",
    "        return pd.DataFrame() # Return empty DataFrame\n",
    "\n",
    "    # 2. Calculate wide outcomes for all patients\n",
    "    print(\"Calculating wide outcomes for all patients...\")\n",
    "    wide_results_df = calculate_wide_patient_outcomes(\n",
    "        prepared_data,\n",
    "        filtered_measurements,\n",
    "        weight_loss_targets,\n",
    "        time_windows,\n",
    "        window_span\n",
    "    )\n",
    "\n",
    "    # 3. Save the single wide table\n",
    "    if not wide_results_df.empty:\n",
    "        print(f\"--- Saving results to output database: {output_db_path} ---\")\n",
    "        print(f\"Saving table: {output_table_name} ({len(wide_results_df)} rows)\")\n",
    "        wide_results_df.to_sql(output_table_name, output_connection, if_exists='replace', index=False)\n",
    "        output_connection.commit() # Ensure changes are saved\n",
    "        print(\"--- Wide table saved successfully ---\")\n",
    "    else:\n",
    "        print(\"WARN: No results generated. Output table will be empty or not created.\")\n",
    "\n",
    "    # Removed the old summary logic based on multiple tables\n",
    "    # A new summary could be generated from wide_results_df if needed\n",
    "\n",
    "    return wide_results_df # Return the generated DataFrame\n",
    "\n",
    "\"\"\"\n",
    "EXECUTION BLOCK (Modified to call the new main function)\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"========== Generating Survival Analysis Input Dataset ==========\")\n",
    "    input_conn = None\n",
    "    output_conn = None\n",
    "    try:\n",
    "        # Connect to in-and output databases\n",
    "        print(f\"Connecting to input database: {input_db_path}\")\n",
    "        if not os.path.exists(input_db_path):\n",
    "             raise FileNotFoundError(f\"Input database not found at {input_db_path}\")\n",
    "        input_conn = sqlite3.connect(input_db_path)\n",
    "\n",
    "        print(f\"Connecting to output database: {output_db_path}\")\n",
    "        output_conn = sqlite3.connect(output_db_path)\n",
    "\n",
    "        # Run the new main analysis function\n",
    "        wide_df = generate_wide_analysis_dataset(\n",
    "            input_conn,\n",
    "            output_conn,\n",
    "            weight_loss_targets,\n",
    "            time_windows,\n",
    "            window_span\n",
    "        )\n",
    "\n",
    "        # Display basic info if successful\n",
    "        if not wide_df.empty:\n",
    "            print(\"\\n--- Survival Analysis Input Table Generation Summary ---\")\n",
    "            print(f\"Generated table '{output_table_name}' with {len(wide_df)} rows and {len(wide_df.columns)} columns.\")\n",
    "            # print(wide_df.head().to_string()) # Optionally print head\n",
    "            print(\"--- End Summary ---\")\n",
    "        else:\n",
    "            print(\"WARN: Analysis completed, but the resulting DataFrame is empty.\")\n",
    "\n",
    "        print(f\"Analysis data saved to {output_db_path}\")\n",
    "\n",
    "    # Error handling remains the same\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"ERROR: Database file not found - {e}\")\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"ERROR: SQLite database error - {e}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"ERROR: Data processing error - {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: An unexpected error occurred - {e}\")\n",
    "        # Consider adding traceback for debugging complex errors:\n",
    "        # import traceback\n",
    "        # print(traceback.format_exc())\n",
    "    finally:\n",
    "        # Ensure connections are closed\n",
    "        print(\"Closing database connections...\")\n",
    "        if input_conn:\n",
    "            input_conn.close()\n",
    "        if output_conn:\n",
    "            output_conn.close()\n",
    "        print(\"========== Survival Analysis Input Data GenerationFinished ==========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconsidered input structure for SA 18-19 Apr v2 - revise this well, I think this is the one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Generating Survival Analysis Input Dataset (Wide Format v3) ==========\n",
      "Connecting to input database: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\emotional_all_notna.sqlite\n",
      "Connecting to output database: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\survival_analysis_wide_v3.sqlite\n",
      "Loading measurements...\n",
      "Loading medical records...\n",
      "Preparing patient data...\n",
      "Calculating wide outcomes for all patients...\n",
      "Reordering columns...\n",
      "--- Saving results to output database: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\survival_analysis_wide_v3.sqlite ---\n",
      "Saving table: survival_analysis_wide_v3 (1664 rows)\n",
      "--- Wide table saved successfully ---\n",
      "\n",
      "--- Survival Analysis Input Table Generation Summary ---\n",
      "Generated table 'survival_analysis_wide_v3' with 1664 rows and 56 columns.\n",
      "--- End Summary ---\n",
      "Analysis data saved to C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\survival_analysis_wide_v3.sqlite\n",
      "Closing database connections...\n",
      "========== Survival Analysis Input Data Generation Finished (Wide Format v3) ==========\n"
     ]
    }
   ],
   "source": [
    "# This code should be placed in a new cell in your Jupyter Notebook.\n",
    "# It incorporates the requested changes into the previous wide-format script.\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import sqlite3\n",
    "from datetime import timedelta\n",
    "import numpy as np # Import numpy for NaN\n",
    "\n",
    "\"\"\"\n",
    "CONFIGURATION\n",
    "\"\"\"\n",
    "# Define directories and database paths - paper1_directory should be defined\n",
    "# in the first cell of this notebook chapter\n",
    "input_db_path = os.path.join(paper1_directory, 'emotional_all_notna.sqlite')\n",
    "input_measurements = \"measurements_with_metadata\"\n",
    "input_medical_records = \"medical_records_complete\"\n",
    "# Modified: Output database path remains the same, but will contain a different table structure\n",
    "output_db_path = os.path.join(paper1_directory, 'survival_analysis_wide_v2.sqlite') # Changed filename for new version\n",
    "output_table_name = \"survival_analysis_wide_v2\" # Changed table name for new version\n",
    "\n",
    "# Define analysis parameters\n",
    "weight_loss_targets = [5, 10, 15]     # Weight loss target percentages\n",
    "time_windows = [40, 60, 80]       # Time windows (centers) in days\n",
    "window_span = 10                   # Permissible span around windows (+/- days)\n",
    "\n",
    "# Define the variables stored in medical_records_complete that are relevant for the analysis.\n",
    "# These include basic metadata like patient and record ID,\n",
    "# basic factors such as age and sex,\n",
    "# as well as the emotional and eating behavior variables pivotal to the research question.\n",
    "# The list can be amended on demand -\n",
    "# for example, right now it does not include medical record creating and closing dates.\n",
    "# --- MODIFIED: Added 'dietitian_visits' ---\n",
    "relevant_medical_values = ['patient_id', 'medical_record_id', 'dietitian_visits', 'sex', 'age',\n",
    "                             'height_m', 'baseline_bmi', 'hunger', 'satiety', 'emotional_eating',\n",
    "                             'emotional_eating_value', 'quantity_control', 'impulse_control', 'weight_gain_cause', 'genomics_sample_id']\n",
    "\n",
    "# --- NEW: Define the desired final column order ---\n",
    "# This list determines the order of columns in the final output table.\n",
    "# It includes baseline info, overall followup, adherence proxies,\n",
    "# fixed-time metrics, time-to-event metrics, and finally confounders/predictors.\n",
    "FINAL_COLUMN_ORDER = [\n",
    "    # IDs\n",
    "    'patient_ID', 'medical_record_ID',\n",
    "    # Followup period info and adherence proxies\n",
    "    'baseline_date', 'last_aval_date', 'total_followup_days', 'nr_visits', 'nr_total_measurements', 'avg_days_between_measurements',\n",
    "    # Total weight change\n",
    "    'baseline_weight_kg', 'last_aval_weight_kg', 'total_wl_kg', 'total_wl_%', 'baseline_bmi', 'final_bmi', 'bmi_reduction',\n",
    "    # Fixed-Timepoint Analysis (Dynamically generated columns will be inserted here by logic below)\n",
    "    # Time-to-Event Analysis (Dynamically generated columns will be inserted here by logic below)\n",
    "    # Confounders / Predictors (from medical records)\n",
    "    'sex', 'age', 'height_m', 'hunger', 'satiety',\n",
    "    'emotional_eating', 'emotional_eating_value', 'quantity_control', 'impulse_control', 'weight_gain_cause', 'genomics_sample_id'\n",
    "]\n",
    "\n",
    "# --- NEW: Dynamically insert fixed-time and time-to-event columns into FINAL_COLUMN_ORDER ---\n",
    "fixed_time_cols = []\n",
    "for window in time_windows:\n",
    "    prefix = f\"{window}d\"\n",
    "    fixed_time_cols.extend([\n",
    "        f'{prefix}_weight_kg', f'wl_{prefix}_kg', f'wl_{prefix}_%',\n",
    "        f'{prefix}_date', f'days_to_{prefix}_measurement', f'{prefix}_dropout'\n",
    "    ])\n",
    "\n",
    "time_to_event_cols = []\n",
    "for target in weight_loss_targets:\n",
    "    prefix = f\"{target}%_wl\"\n",
    "    time_to_event_cols.extend([\n",
    "        f'{prefix}_achieved', f'{prefix}_%', f'{prefix}_date', f'days_to_{prefix}'\n",
    "    ])\n",
    "\n",
    "# Find the insertion point (after adherence proxies)\n",
    "insert_point = FINAL_COLUMN_ORDER.index('total_wl_%') + 1\n",
    "# Insert the dynamic columns\n",
    "FINAL_COLUMN_ORDER[insert_point:insert_point] = fixed_time_cols + time_to_event_cols\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DATA LOADING & PREPARATION (No changes needed here, kept for context)\n",
    "\"\"\"\n",
    "\n",
    "def load_measurements(connection):\n",
    "    \"\"\"\n",
    "    Load measurements from the measurement_with_metadata table;\n",
    "    make sure key values are in the correct format.\n",
    "    \"\"\"\n",
    "    query = f\"SELECT * FROM {input_measurements}\"\n",
    "    measurements = pd.read_sql_query(query, connection)\n",
    "    measurements['measurement_date'] = pd.to_datetime(measurements['measurement_date'], errors='coerce')\n",
    "    measurements['weight_kg'] = pd.to_numeric(measurements['weight_kg'], errors='coerce')\n",
    "    # Ensure sorting for consistent 'first'/'last' operations later\n",
    "    measurements = measurements.sort_values(['patient_id', 'medical_record_id', 'measurement_date'])\n",
    "    return measurements\n",
    "\n",
    "def load_medical_records(connection):\n",
    "    \"\"\"\n",
    "    Load medical records from the medical_records_complete table;\n",
    "    make sure date values are in datetime format.\n",
    "    The exact columns to be used are defined in the prepare_patient_data function.\n",
    "    \"\"\"\n",
    "    query = f\"SELECT * FROM {input_medical_records}\"\n",
    "    medical_records = pd.read_sql_query(query, connection)\n",
    "    # Ensure relevant date columns are datetime\n",
    "    for col in ['medical_record_creation_date', 'baseline_measurement_date', 'final_measurement_date']:\n",
    "         if col in medical_records.columns:\n",
    "             medical_records[col] = pd.to_datetime(medical_records[col], errors='coerce')\n",
    "    # --- NEW: Ensure dietitian_visits is numeric ---\n",
    "    if 'dietitian_visits' in medical_records.columns:\n",
    "        medical_records['dietitian_visits'] = pd.to_numeric(medical_records['dietitian_visits'], errors='coerce')\n",
    "    return medical_records\n",
    "\n",
    "def prepare_patient_data(measurements, medical_records):\n",
    "    \"\"\"\n",
    "    Filter measurements to only include those from the earliest medical record for each patient.\n",
    "    Merge measurements with relevant medical record data, including the pivotal eating behavior scores.\n",
    "    \"\"\"\n",
    "    # Filter measurements to only include those from the first treatment record of each patient\n",
    "    # Ensure measurements are sorted before grouping\n",
    "    measurements = measurements.sort_values(['patient_id', 'measurement_date'])\n",
    "    earliest_records_with_data = measurements.groupby('patient_id')['medical_record_id'].first().reset_index()\n",
    "\n",
    "    filtered_measurements = pd.merge(\n",
    "        measurements,\n",
    "        earliest_records_with_data,\n",
    "        on=['patient_id', 'medical_record_id'],\n",
    "        how='inner'\n",
    "    )\n",
    "    # Ensure filtered_measurements are sorted for baseline identification\n",
    "    filtered_measurements = filtered_measurements.sort_values(['patient_id', 'medical_record_id', 'measurement_date'])\n",
    "\n",
    "    # Identify the baseline measurement row for each record (the first measurement in the filtered set)\n",
    "    baseline_data_rows = filtered_measurements.groupby(['patient_id', 'medical_record_id']).first().reset_index()\n",
    "\n",
    "    # Select only relevant columns from medical_records to merge\n",
    "    # --- MODIFIED: Now includes 'dietitian_visits' if added to relevant_medical_values ---\n",
    "    cols_to_select = [col for col in relevant_medical_values if col in medical_records.columns]\n",
    "    medical_record_subset = medical_records[cols_to_select]\n",
    "\n",
    "    # Merge baseline measurement info with the selected medical record data\n",
    "    # Use baseline_data_rows which contains the actual first measurement details\n",
    "    prepared_data = pd.merge(\n",
    "        baseline_data_rows[['patient_id', 'medical_record_id', 'measurement_date', 'weight_kg']], # Get baseline date/weight from actual first measurement\n",
    "        medical_record_subset,\n",
    "        on=['patient_id', 'medical_record_id'],\n",
    "        how='left' # Keep all baseline measurements\n",
    "    )\n",
    "    # Rename columns for clarity before returning\n",
    "    prepared_data = prepared_data.rename(columns={'measurement_date': 'baseline_date', 'weight_kg': 'baseline_weight_kg'})\n",
    "\n",
    "    # Add baseline_bmi from medical_records if available and not already present from measurement merge\n",
    "    if 'baseline_bmi' in medical_record_subset.columns and 'baseline_bmi' not in prepared_data.columns:\n",
    "         prepared_data = pd.merge(\n",
    "              prepared_data,\n",
    "              medical_record_subset[['patient_id', 'medical_record_id', 'baseline_bmi']],\n",
    "              on=['patient_id', 'medical_record_id'],\n",
    "              how='left'\n",
    "         )\n",
    "\n",
    "    return prepared_data, filtered_measurements\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "MODIFIED: CALCULATE WEIGHT LOSS OUTCOMES FOR WIDE TABLE\n",
    "\"\"\"\n",
    "\n",
    "def _get_patient_baseline(patient_data, patient_id, medical_record_id):\n",
    "    \"\"\"\n",
    "    Get the prepared baseline data for a patient's specific medical record.\n",
    "    (Function remains largely the same, but operates on the prepared_data structure)\n",
    "    \"\"\"\n",
    "    patient_baseline = patient_data[\n",
    "        (patient_data['patient_id'] == patient_id) &\n",
    "        (patient_data['medical_record_id'] == medical_record_id)\n",
    "    ]\n",
    "    if patient_baseline.empty:\n",
    "        print(f\"WARN: No baseline data found for patient {patient_id}, record {medical_record_id}. Skipping.\")\n",
    "        return None\n",
    "    # Ensure we return a Series for consistent access\n",
    "    return patient_baseline.iloc[0]\n",
    "\n",
    "def _calculate_wl_metrics(baseline_weight, current_weight):\n",
    "    \"\"\" Helper to calculate weight loss kg and % \"\"\"\n",
    "    if pd.isna(baseline_weight) or pd.isna(current_weight) or baseline_weight == 0:\n",
    "        return np.nan, np.nan\n",
    "    wl_kg = baseline_weight - current_weight\n",
    "    wl_pct = (wl_kg / baseline_weight) * 100\n",
    "    return wl_kg, round(wl_pct, 2)\n",
    "\n",
    "# Removed _check_target_achievement, _determine_final_measurement, _calculate_outcome_metrics\n",
    "# Their logic will be integrated into the main calculation function.\n",
    "\n",
    "def calculate_wide_patient_outcomes(prepared_patient_data, filtered_measurements, weight_loss_targets, time_windows, window_span):\n",
    "    \"\"\"\n",
    "    Calculate all required outcomes (fixed-time and time-to-event) for each patient\n",
    "    and return a list of dictionaries, each representing a row in the wide table.\n",
    "    \"\"\"\n",
    "    results_list = []\n",
    "    # Group all measurements for the relevant (earliest) medical record per patient\n",
    "    grouped_measurements = filtered_measurements.groupby(['patient_id', 'medical_record_id'])\n",
    "\n",
    "    for (patient_id, medical_record_id), group in grouped_measurements:\n",
    "        # --- NEW: Calculate total number of measurements for this group (record) ---\n",
    "        num_measurements = len(group)\n",
    "\n",
    "        # 1. Get Baseline Info\n",
    "        baseline_info = _get_patient_baseline(prepared_patient_data, patient_id, medical_record_id)\n",
    "        if baseline_info is None:\n",
    "            continue\n",
    "\n",
    "        baseline_date = baseline_info['baseline_date']\n",
    "        baseline_weight = baseline_info['baseline_weight_kg']\n",
    "\n",
    "        # Initialize result dictionary with baseline info AND adherence proxies\n",
    "        result = {\n",
    "            # IDs\n",
    "            'patient_ID': patient_id, # Match Excel header\n",
    "            'medical_record_ID': medical_record_id, # Match Excel header\n",
    "            # Baseline\n",
    "            'baseline_date': baseline_date,\n",
    "            'baseline_weight_kg': baseline_weight,\n",
    "            # Adherence Proxies (Initialize early)\n",
    "            'nr_visits': baseline_info.get('dietitian_visits'), # Get from merged baseline data\n",
    "            'nr_total_measurements': num_measurements, # Use calculated value\n",
    "            'avg_days_between_measurements': np.nan, # Initialize, calculated later\n",
    "            # Confounders / Predictors (Initialize early)\n",
    "            'sex': baseline_info.get('sex'),\n",
    "            'age': baseline_info.get('age'),\n",
    "            'height_m': baseline_info.get('height_m'),\n",
    "            'baseline_bmi': baseline_info.get('baseline_bmi'), # Get BMI from prepared data\n",
    "            'hunger': baseline_info.get('hunger'),\n",
    "            'satiety': baseline_info.get('satiety'),\n",
    "            'emotional_eating': baseline_info.get('emotional_eating'),\n",
    "            'emotional_eating_value': baseline_info.get('emotional_eating_value'),\n",
    "            'quantity_control': baseline_info.get('quantity_control'),\n",
    "            'impulse_control': baseline_info.get('impulse_control'), \n",
    "            'weight_gain_cause': baseline_info.get('weight_gain_cause'),\n",
    "            'genomics_sample_id': baseline_info.get('genomics_sample_id')\n",
    "        }\n",
    "\n",
    "        # Get all measurements *after* baseline for this group\n",
    "        followup_measurements = group[group['measurement_date'] > baseline_date].sort_values('measurement_date')\n",
    "\n",
    "        # 2. Calculate Overall Follow-up Metrics\n",
    "        if not followup_measurements.empty:\n",
    "            last_measurement = followup_measurements.iloc[-1]\n",
    "            result['last_aval_date'] = last_measurement['measurement_date']\n",
    "            # --- MODIFIED: Calculate inclusive total_followup_days (+1) ---\n",
    "            result['total_followup_days'] = (last_measurement['measurement_date'] - baseline_date).days + 1\n",
    "            result['last_aval_weight_kg'] = last_measurement['weight_kg']\n",
    "            wl_kg, wl_pct = _calculate_wl_metrics(baseline_weight, last_measurement['weight_kg'])\n",
    "            result['total_wl_kg'] = wl_kg\n",
    "            result['total_wl_%'] = wl_pct\n",
    "        else:\n",
    "            # Handle instant dropouts (only baseline measurement exists)\n",
    "            result['last_aval_date'] = baseline_date\n",
    "            # --- MODIFIED: Set total_followup_days to 1 for instant dropouts ---\n",
    "            result['total_followup_days'] = 1\n",
    "            result['last_aval_weight_kg'] = baseline_weight\n",
    "            result['total_wl_kg'] = 0.0\n",
    "            result['total_wl_%'] = 0.0\n",
    "\n",
    "        # --- NEW: Calculate avg_days_between_measurements ---\n",
    "        # Requires total_followup_days and nr_total_measurements\n",
    "        if result['nr_total_measurements'] is not None and result['nr_total_measurements'] > 1:\n",
    "             # Use the calculated inclusive total_followup_days\n",
    "             total_days = result['total_followup_days']\n",
    "             # Calculate average based on number of intervals (N measurements = N-1 intervals)\n",
    "             # Ensure total_days is treated as the span covering N points (so N-1 intervals)\n",
    "             # If total_days is 1 (instant dropout), num_measurements is 1, this condition isn't met.\n",
    "             # If total_days > 1, num_measurements must be >= 2.\n",
    "             result['avg_days_between_measurements'] = round( (total_days -1) / (result['nr_total_measurements'] - 1) , 2) if (result['nr_total_measurements'] - 1) > 0 else np.nan\n",
    "        else:\n",
    "             # If 0 or 1 measurements, average days between is undefined\n",
    "             result['avg_days_between_measurements'] = np.nan\n",
    "\n",
    "\n",
    "        # 3. Calculate Fixed-Timepoint Metrics (for each time window)\n",
    "        for window_center in time_windows:\n",
    "            min_window_date = baseline_date + timedelta(days=(window_center - window_span))\n",
    "            max_window_date = baseline_date + timedelta(days=(window_center + window_span))\n",
    "            target_date = baseline_date + timedelta(days=window_center)\n",
    "\n",
    "            # Find measurements strictly *within* the cutoff window span\n",
    "            measurements_around_cutoff = followup_measurements[\n",
    "                (followup_measurements['measurement_date'] >= min_window_date) &\n",
    "                (followup_measurements['measurement_date'] <= max_window_date)\n",
    "            ]\n",
    "\n",
    "            measurement_for_window = None # The measurement to use for this window's stats\n",
    "            is_dropout_at_window = True # Assume dropout unless a measurement is found *in* the window\n",
    "\n",
    "            if not measurements_around_cutoff.empty:\n",
    "                # Measurement exists within the window span. Find the closest one.\n",
    "                is_dropout_at_window = False # Found measurement in window\n",
    "                measurements_around_cutoff = measurements_around_cutoff.copy()\n",
    "                measurements_around_cutoff['distance_to_center'] = abs(\n",
    "                    (measurements_around_cutoff['measurement_date'] - target_date).dt.days\n",
    "                )\n",
    "                closest_measurement_idx = measurements_around_cutoff['distance_to_center'].idxmin()\n",
    "                measurement_for_window = measurements_around_cutoff.loc[closest_measurement_idx]\n",
    "            # else: If measurements_around_cutoff is empty, is_dropout_at_window remains True.\n",
    "            # No need to check for measurements *before* the window for populating data,\n",
    "            # as per the requirement to leave fields blank for dropouts.\n",
    "\n",
    "            # --- MODIFIED: Populate results based *strictly* on dropout status for the window ---\n",
    "            prefix = f\"{window_center}d\"\n",
    "            result[f'{prefix}_dropout'] = is_dropout_at_window # Set dropout status first\n",
    "\n",
    "            if is_dropout_at_window:\n",
    "                # If dropout for this window, set all related metrics to NaN/NaT\n",
    "                result[f'{prefix}_weight_kg'] = np.nan\n",
    "                result[f'wl_{prefix}_kg'] = np.nan\n",
    "                result[f'wl_{prefix}_%'] = np.nan\n",
    "                result[f'{prefix}_date'] = pd.NaT\n",
    "                result[f'days_to_{prefix}_measurement'] = np.nan\n",
    "            else:\n",
    "                # If NOT dropout, populate metrics using the found measurement_for_window\n",
    "                # (This block only runs if measurement_for_window is not None)\n",
    "                result[f'{prefix}_weight_kg'] = measurement_for_window['weight_kg']\n",
    "                wl_kg, wl_pct = _calculate_wl_metrics(baseline_weight, measurement_for_window['weight_kg'])\n",
    "                result[f'wl_{prefix}_kg'] = wl_kg\n",
    "                result[f'wl_{prefix}_%'] = wl_pct\n",
    "                result[f'{prefix}_date'] = measurement_for_window['measurement_date']\n",
    "                # Calculate days from baseline to this specific measurement\n",
    "                result[f'days_to_{prefix}_measurement'] = (measurement_for_window['measurement_date'] - baseline_date).days + 1 # Inclusive days\n",
    "\n",
    "\n",
    "        # 4. Calculate Time-to-Event Metrics (for each weight loss target)\n",
    "        # (No changes needed in this section based on discussion)\n",
    "        for target in weight_loss_targets:\n",
    "            target_achieved = False\n",
    "            first_success_measurement = None\n",
    "            actual_wl_at_success = np.nan\n",
    "\n",
    "            # Check all followup measurements for the first success\n",
    "            for _, row in followup_measurements.iterrows():\n",
    "                current_weight = row['weight_kg']\n",
    "                if baseline_weight is not None and baseline_weight > 0:\n",
    "                    current_weight_loss_pct = ((baseline_weight - current_weight) / baseline_weight) * 100\n",
    "                    if round(current_weight_loss_pct, 2) >= target:\n",
    "                        target_achieved = True\n",
    "                        first_success_measurement = row\n",
    "                        actual_wl_at_success = round(current_weight_loss_pct, 2)\n",
    "                        break # Stop at the first success\n",
    "\n",
    "            # Populate results for this target\n",
    "            prefix = f\"{target}%_wl\"\n",
    "            result[f'{prefix}_achieved'] = target_achieved\n",
    "            if target_achieved and first_success_measurement is not None:\n",
    "                result[f'{prefix}_%'] = actual_wl_at_success\n",
    "                result[f'{prefix}_date'] = first_success_measurement['measurement_date']\n",
    "                # Calculate inclusive days to achieve target\n",
    "                result[f'days_to_{prefix}'] = (first_success_measurement['measurement_date'] - baseline_date).days + 1\n",
    "            else:\n",
    "                result[f'{prefix}_%'] = np.nan\n",
    "                result[f'{prefix}_date'] = pd.NaT\n",
    "                result[f'days_to_{prefix}'] = np.nan\n",
    "\n",
    "        results_list.append(result)\n",
    "\n",
    "    return pd.DataFrame(results_list)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "MODIFIED: MAIN ORCHESTRATION FUNCTION FOR WIDE TABLE\n",
    "\"\"\"\n",
    "\n",
    "def generate_wide_analysis_dataset(input_connection, output_connection, weight_loss_targets, time_windows, window_span=10):\n",
    "    \"\"\"\n",
    "    Orchestrates the process to generate the single wide survival analysis dataset.\n",
    "    Loads data, prepares patient baseline info, calculates all outcomes per patient,\n",
    "    reorders columns, and saves the resulting wide DataFrame to the output database.\n",
    "    \"\"\"\n",
    "    # 1. Load and prepare input data\n",
    "    print(\"Loading measurements...\")\n",
    "    measurements = load_measurements(input_connection)\n",
    "    print(\"Loading medical records...\")\n",
    "    medical_records = load_medical_records(input_connection)\n",
    "    print(\"Preparing patient data...\")\n",
    "    prepared_data, filtered_measurements = prepare_patient_data(measurements, medical_records)\n",
    "\n",
    "    if prepared_data.empty:\n",
    "        print(\"ERROR: Prepared patient data is empty. Cannot proceed.\")\n",
    "        return pd.DataFrame() # Return empty DataFrame\n",
    "\n",
    "    # 2. Calculate wide outcomes for all patients\n",
    "    print(\"Calculating wide outcomes for all patients...\")\n",
    "    wide_results_df = calculate_wide_patient_outcomes(\n",
    "        prepared_data,\n",
    "        filtered_measurements,\n",
    "        weight_loss_targets,\n",
    "        time_windows,\n",
    "        window_span\n",
    "    )\n",
    "\n",
    "    # --- NEW: Reorder columns according to FINAL_COLUMN_ORDER defined in config ---\n",
    "    if not wide_results_df.empty:\n",
    "        print(\"Reordering columns...\")\n",
    "        # Ensure all columns in FINAL_COLUMN_ORDER exist in the DataFrame, handle potential missing ones\n",
    "        final_columns_present = [col for col in FINAL_COLUMN_ORDER if col in wide_results_df.columns]\n",
    "        missing_cols = [col for col in FINAL_COLUMN_ORDER if col not in wide_results_df.columns]\n",
    "        if missing_cols:\n",
    "             print(f\"WARN: The following columns defined in FINAL_COLUMN_ORDER were not found in the generated data and will be skipped: {missing_cols}\")\n",
    "        # Add any columns present in DataFrame but not in FINAL_COLUMN_ORDER to the end, just in case\n",
    "        extra_cols = [col for col in wide_results_df.columns if col not in final_columns_present]\n",
    "        if extra_cols:\n",
    "             print(f\"WARN: The following columns were generated but not included in FINAL_COLUMN_ORDER; they will be added to the end: {extra_cols}\")\n",
    "\n",
    "        wide_results_df = wide_results_df[final_columns_present + extra_cols]\n",
    "\n",
    "\n",
    "    # 3. Save the single wide table\n",
    "    if not wide_results_df.empty:\n",
    "        print(f\"--- Saving results to output database: {output_db_path} ---\")\n",
    "        print(f\"Saving table: {output_table_name} ({len(wide_results_df)} rows)\")\n",
    "        wide_results_df.to_sql(output_table_name, output_connection, if_exists='replace', index=False)\n",
    "        output_connection.commit() # Ensure changes are saved\n",
    "        print(\"--- Wide table saved successfully ---\")\n",
    "    else:\n",
    "        print(\"WARN: No results generated. Output table will be empty or not created.\")\n",
    "\n",
    "    # Removed the old summary logic based on multiple tables\n",
    "    # A new summary could be generated from wide_results_df if needed\n",
    "\n",
    "    return wide_results_df # Return the generated DataFrame\n",
    "\n",
    "\"\"\"\n",
    "EXECUTION BLOCK (Modified to call the new main function and use new output names)\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"========== Generating Survival Analysis Input Dataset (Wide Format v2) ==========\") # Updated title\n",
    "    input_conn = None\n",
    "    output_conn = None\n",
    "    try:\n",
    "        # Connect to in-and output databases\n",
    "        print(f\"Connecting to input database: {input_db_path}\")\n",
    "        if not os.path.exists(input_db_path):\n",
    "             raise FileNotFoundError(f\"Input database not found at {input_db_path}\")\n",
    "        input_conn = sqlite3.connect(input_db_path)\n",
    "\n",
    "        print(f\"Connecting to output database: {output_db_path}\")\n",
    "        output_conn = sqlite3.connect(output_db_path)\n",
    "\n",
    "        # Run the new main analysis function\n",
    "        wide_df = generate_wide_analysis_dataset(\n",
    "            input_conn,\n",
    "            output_conn,\n",
    "            weight_loss_targets,\n",
    "            time_windows,\n",
    "            window_span\n",
    "        )\n",
    "\n",
    "        # Display basic info if successful\n",
    "        if not wide_df.empty:\n",
    "            print(\"\\n--- Survival Analysis Input Table Generation Summary ---\")\n",
    "            print(f\"Generated table '{output_table_name}' with {len(wide_df)} rows and {len(wide_df.columns)} columns.\")\n",
    "            # print(wide_df.head().to_string()) # Optionally print head\n",
    "            print(\"--- End Summary ---\")\n",
    "        else:\n",
    "            print(\"WARN: Analysis completed, but the resulting DataFrame is empty.\")\n",
    "\n",
    "        print(f\"Analysis data saved to {output_db_path}\")\n",
    "\n",
    "    # Error handling remains the same\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"ERROR: Database file not found - {e}\")\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"ERROR: SQLite database error - {e}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"ERROR: Data processing error - {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: An unexpected error occurred - {e}\")\n",
    "        # Consider adding traceback for debugging complex errors:\n",
    "        # import traceback\n",
    "        # print(traceback.format_exc())\n",
    "    finally:\n",
    "        # Ensure connections are closed\n",
    "        print(\"Closing database connections...\")\n",
    "        if input_conn:\n",
    "            input_conn.close()\n",
    "        if output_conn:\n",
    "            output_conn.close()\n",
    "        print(\"========== Survival Analysis Input Data Generation Finished (Wide Format v2) ==========\") # Updated title\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actually, this may be the one, Apr 20 - added final BMI too, and fetching BMI from measurements instead of medical records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Generating Survival Analysis Input Dataset (Wide Format v3) ==========\n",
      "Connecting to input database: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\emotional_all_notna.sqlite\n",
      "Connecting to output database: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\survival_analysis.sqlite\n",
      "Loading measurements...\n",
      "Loading medical records...\n",
      "Preparing patient data...\n",
      "Calculating wide outcomes for all patients...\n",
      "Reordering columns...\n",
      "--- Saving results to output database: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\survival_analysis.sqlite ---\n",
      "Saving table: sa_input_table (1664 rows)\n",
      "--- Wide table saved successfully ---\n",
      "\n",
      "--- Survival Analysis Input Table Generation Summary ---\n",
      "Generated table 'sa_input_table' with 1664 rows and 56 columns.\n",
      "--- End Summary ---\n",
      "Analysis data saved to C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\survival_analysis.sqlite\n",
      "Closing database connections...\n",
      "========== Survival Analysis Input Data Generation Finished (Wide Format v3) ==========\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# filepath: c:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\paper1_script.ipynb\n",
    "# This code should be placed in a new cell in your Jupyter Notebook.\n",
    "# It incorporates the requested changes for BMI handling into the wide-format script.\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import sqlite3\n",
    "from datetime import timedelta\n",
    "import numpy as np # Import numpy for NaN\n",
    "\n",
    "\"\"\"\n",
    "CONFIGURATION\n",
    "\"\"\"\n",
    "# Define directories and database paths - paper1_directory should be defined\n",
    "# in the first cell of this notebook chapter\n",
    "input_db_path = os.path.join(paper1_directory, 'emotional_all_notna.sqlite')\n",
    "input_measurements = \"measurements_with_metadata\"\n",
    "input_medical_records = \"medical_records_complete\"\n",
    "# Output database path and table name\n",
    "output_db_path = os.path.join(paper1_directory, 'survival_analysis.sqlite') # Changed filename for new version\n",
    "output_table_name = \"sa_input_table\" # Changed table name for new version\n",
    "\n",
    "# Define analysis parameters\n",
    "weight_loss_targets = [5, 10, 15]     # Weight loss target percentages\n",
    "time_windows = [40, 60, 80]       # Time windows (centers) in days\n",
    "window_span = 10                   # Permissible span around windows (+/- days)\n",
    "\n",
    "# Define the variables stored in medical_records_complete that are relevant for the analysis.\n",
    "# --- MODIFIED: Removed 'baseline_bmi' as it will be sourced from measurements ---\n",
    "relevant_medical_values = ['patient_id', 'medical_record_id', 'dietitian_visits', 'sex', 'age',\n",
    "                             'height_m', 'hunger', 'satiety', 'emotional_eating', # Removed 'baseline_bmi'\n",
    "                             'emotional_eating_value', 'quantity_control', 'impulse_control', 'weight_gain_cause', 'genomics_sample_id']\n",
    "\n",
    "# --- MODIFIED: Renamed to output_column_order and updated for BMI ---\n",
    "# This list determines the order of columns in the final output table.\n",
    "output_column_order = [\n",
    "    # IDs\n",
    "    'patient_ID', 'medical_record_ID',\n",
    "    # Followup period info and adherence proxies\n",
    "    'baseline_date', 'last_aval_date', 'total_followup_days', 'nr_visits', 'nr_total_measurements', 'avg_days_between_measurements',\n",
    "    # Total weight change & BMI change\n",
    "    'baseline_weight_kg', 'last_aval_weight_kg', 'total_wl_kg', 'total_wl_%',\n",
    "    'baseline_bmi', 'final_bmi', 'bmi_reduction', # Added BMI columns here\n",
    "    # Fixed-Timepoint Analysis (Dynamically generated columns will be inserted here by logic below)\n",
    "    # Time-to-Event Analysis (Dynamically generated columns will be inserted here by logic below)\n",
    "    # Confounders / Predictors (from medical records)\n",
    "    'sex', 'age', 'height_m', 'hunger', 'satiety', # Removed 'baseline_bmi' from here\n",
    "    'emotional_eating', 'emotional_eating_value', 'quantity_control', 'impulse_control', 'weight_gain_cause', 'genomics_sample_id'\n",
    "]\n",
    "\n",
    "# --- MODIFIED: Dynamically insert fixed-time and time-to-event columns into output_column_order ---\n",
    "fixed_time_cols = []\n",
    "for window in time_windows:\n",
    "    prefix = f\"{window}d\"\n",
    "    # --- MODIFIED: Excluded BMI calculation for fixed timepoints ---\n",
    "    fixed_time_cols.extend([\n",
    "        f'{prefix}_weight_kg', f'wl_{prefix}_kg', f'wl_{prefix}_%',\n",
    "        f'{prefix}_date', f'days_to_{prefix}_measurement', f'{prefix}_dropout'\n",
    "        # Removed: f'{prefix}_bmi'\n",
    "    ])\n",
    "\n",
    "time_to_event_cols = []\n",
    "for target in weight_loss_targets:\n",
    "    prefix = f\"{target}%_wl\"\n",
    "    time_to_event_cols.extend([\n",
    "        f'{prefix}_achieved', f'{prefix}_%', f'{prefix}_date', f'days_to_{prefix}'\n",
    "    ])\n",
    "\n",
    "# Find the insertion point (after BMI reduction)\n",
    "# --- MODIFIED: Insertion point updated ---\n",
    "insert_point = output_column_order.index('bmi_reduction') + 1\n",
    "# Insert the dynamic columns\n",
    "output_column_order[insert_point:insert_point] = fixed_time_cols + time_to_event_cols\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DATA LOADING & PREPARATION\n",
    "\"\"\"\n",
    "\n",
    "def load_measurements(connection):\n",
    "    \"\"\"\n",
    "    Load measurements from the measurement_with_metadata table;\n",
    "    make sure key values are in the correct format.\n",
    "    --- MODIFIED: Ensure BMI is numeric ---\n",
    "    \"\"\"\n",
    "    query = f\"SELECT * FROM {input_measurements}\"\n",
    "    measurements = pd.read_sql_query(query, connection)\n",
    "    measurements['measurement_date'] = pd.to_datetime(measurements['measurement_date'], errors='coerce')\n",
    "    measurements['weight_kg'] = pd.to_numeric(measurements['weight_kg'], errors='coerce')\n",
    "    # --- NEW: Ensure BMI is numeric ---\n",
    "    measurements['bmi'] = pd.to_numeric(measurements['bmi'], errors='coerce')\n",
    "    # Ensure sorting for consistent 'first'/'last' operations later\n",
    "    measurements = measurements.sort_values(['patient_id', 'medical_record_id', 'measurement_date'])\n",
    "    return measurements\n",
    "\n",
    "def load_medical_records(connection):\n",
    "    \"\"\"\n",
    "    Load medical records from the medical_records_complete table;\n",
    "    make sure date values are in datetime format.\n",
    "    The exact columns to be used are defined in the prepare_patient_data function.\n",
    "    \"\"\"\n",
    "    query = f\"SELECT * FROM {input_medical_records}\"\n",
    "    medical_records = pd.read_sql_query(query, connection)\n",
    "    # Ensure relevant date columns are datetime\n",
    "    for col in ['medical_record_creation_date', 'baseline_measurement_date', 'final_measurement_date']:\n",
    "         if col in medical_records.columns:\n",
    "             medical_records[col] = pd.to_datetime(medical_records[col], errors='coerce')\n",
    "    # Ensure dietitian_visits is numeric\n",
    "    if 'dietitian_visits' in medical_records.columns:\n",
    "        medical_records['dietitian_visits'] = pd.to_numeric(medical_records['dietitian_visits'], errors='coerce')\n",
    "    return medical_records\n",
    "\n",
    "def prepare_patient_data(measurements, medical_records):\n",
    "    \"\"\"\n",
    "    Filter measurements to only include those from the earliest medical record for each patient.\n",
    "    Merge measurements with relevant medical record data, including the pivotal eating behavior scores.\n",
    "    --- MODIFIED: Fetch baseline BMI from measurements ---\n",
    "    \"\"\"\n",
    "    # Filter measurements to only include those from the first treatment record of each patient\n",
    "    measurements = measurements.sort_values(['patient_id', 'measurement_date'])\n",
    "    earliest_records_with_data = measurements.groupby('patient_id')['medical_record_id'].first().reset_index()\n",
    "\n",
    "    filtered_measurements = pd.merge(\n",
    "        measurements,\n",
    "        earliest_records_with_data,\n",
    "        on=['patient_id', 'medical_record_id'],\n",
    "        how='inner'\n",
    "    )\n",
    "    filtered_measurements = filtered_measurements.sort_values(['patient_id', 'medical_record_id', 'measurement_date'])\n",
    "\n",
    "    # Identify the baseline measurement row for each record (the first measurement in the filtered set)\n",
    "    # --- MODIFIED: Include 'bmi' ---\n",
    "    baseline_data_rows = filtered_measurements.groupby(['patient_id', 'medical_record_id']).first().reset_index()\n",
    "\n",
    "    # Select only relevant columns from medical_records to merge\n",
    "    cols_to_select = [col for col in relevant_medical_values if col in medical_records.columns]\n",
    "    medical_record_subset = medical_records[cols_to_select]\n",
    "\n",
    "    # Merge baseline measurement info with the selected medical record data\n",
    "    # --- MODIFIED: Include 'bmi' from baseline_data_rows ---\n",
    "    prepared_data = pd.merge(\n",
    "        baseline_data_rows[['patient_id', 'medical_record_id', 'measurement_date', 'weight_kg', 'bmi']], # Added 'bmi'\n",
    "        medical_record_subset,\n",
    "        on=['patient_id', 'medical_record_id'],\n",
    "        how='left' # Keep all baseline measurements\n",
    "    )\n",
    "    # Rename columns for clarity before returning\n",
    "    # --- MODIFIED: Rename 'bmi' to 'baseline_bmi' ---\n",
    "    prepared_data = prepared_data.rename(columns={\n",
    "        'measurement_date': 'baseline_date',\n",
    "        'weight_kg': 'baseline_weight_kg',\n",
    "        'bmi': 'baseline_bmi' # Rename BMI from measurement\n",
    "    })\n",
    "\n",
    "    # --- REMOVED: Separate merge for baseline_bmi from medical_records is no longer needed ---\n",
    "    # if 'baseline_bmi' in medical_record_subset.columns and 'baseline_bmi' not in prepared_data.columns:\n",
    "    #      ... (old merge logic removed) ...\n",
    "\n",
    "    return prepared_data, filtered_measurements\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "MODIFIED: CALCULATE WEIGHT LOSS OUTCOMES FOR WIDE TABLE\n",
    "\"\"\"\n",
    "\n",
    "def _get_patient_baseline(patient_data, patient_id, medical_record_id):\n",
    "    \"\"\"\n",
    "    Get the prepared baseline data for a patient's specific medical record.\n",
    "    \"\"\"\n",
    "    patient_baseline = patient_data[\n",
    "        (patient_data['patient_id'] == patient_id) &\n",
    "        (patient_data['medical_record_id'] == medical_record_id)\n",
    "    ]\n",
    "    if patient_baseline.empty:\n",
    "        print(f\"WARN: No baseline data found for patient {patient_id}, record {medical_record_id}. Skipping.\")\n",
    "        return None\n",
    "    return patient_baseline.iloc[0]\n",
    "\n",
    "def _calculate_wl_metrics(baseline_weight, current_weight):\n",
    "    \"\"\" Helper to calculate weight loss kg and % \"\"\"\n",
    "    if pd.isna(baseline_weight) or pd.isna(current_weight) or baseline_weight == 0:\n",
    "        return np.nan, np.nan\n",
    "    wl_kg = baseline_weight - current_weight\n",
    "    wl_pct = (wl_kg / baseline_weight) * 100\n",
    "    return wl_kg, round(wl_pct, 2)\n",
    "\n",
    "# --- NEW: Helper to calculate BMI reduction ---\n",
    "def _calculate_bmi_reduction(baseline_bmi, final_bmi):\n",
    "    \"\"\" Helper to calculate BMI reduction (final - baseline) \"\"\"\n",
    "    if pd.isna(baseline_bmi) or pd.isna(final_bmi):\n",
    "        return np.nan\n",
    "    return round(final_bmi - baseline_bmi, 2)\n",
    "\n",
    "\n",
    "def calculate_wide_patient_outcomes(prepared_patient_data, filtered_measurements, weight_loss_targets, time_windows, window_span):\n",
    "    \"\"\"\n",
    "    Calculate all required outcomes (fixed-time and time-to-event) for each patient\n",
    "    and return a list of dictionaries, each representing a row in the wide table.\n",
    "    --- MODIFIED: Incorporates BMI from measurements and BMI reduction ---\n",
    "    \"\"\"\n",
    "    results_list = []\n",
    "    grouped_measurements = filtered_measurements.groupby(['patient_id', 'medical_record_id'])\n",
    "\n",
    "    for (patient_id, medical_record_id), group in grouped_measurements:\n",
    "        num_measurements = len(group)\n",
    "\n",
    "        # 1. Get Baseline Info\n",
    "        baseline_info = _get_patient_baseline(prepared_patient_data, patient_id, medical_record_id)\n",
    "        if baseline_info is None:\n",
    "            continue\n",
    "\n",
    "        baseline_date = baseline_info['baseline_date']\n",
    "        baseline_weight = baseline_info['baseline_weight_kg']\n",
    "        # --- MODIFIED: Get baseline_bmi from prepared_data (sourced from measurement) ---\n",
    "        baseline_bmi = baseline_info.get('baseline_bmi') # Use .get() for safety\n",
    "\n",
    "        # Initialize result dictionary\n",
    "        result = {\n",
    "            # IDs\n",
    "            'patient_ID': patient_id,\n",
    "            'medical_record_ID': medical_record_id,\n",
    "            # Baseline\n",
    "            'baseline_date': baseline_date,\n",
    "            'baseline_weight_kg': baseline_weight,\n",
    "            'baseline_bmi': baseline_bmi, # Add baseline BMI here\n",
    "            # Adherence Proxies\n",
    "            'nr_visits': baseline_info.get('dietitian_visits'),\n",
    "            'nr_total_measurements': num_measurements,\n",
    "            'avg_days_between_measurements': np.nan,\n",
    "            # Confounders / Predictors\n",
    "            'sex': baseline_info.get('sex'),\n",
    "            'age': baseline_info.get('age'),\n",
    "            'height_m': baseline_info.get('height_m'),\n",
    "            'hunger': baseline_info.get('hunger'),\n",
    "            'satiety': baseline_info.get('satiety'),\n",
    "            'emotional_eating': baseline_info.get('emotional_eating'),\n",
    "            'emotional_eating_value': baseline_info.get('emotional_eating_value'),\n",
    "            'quantity_control': baseline_info.get('quantity_control'),\n",
    "            'impulse_control': baseline_info.get('impulse_control'),\n",
    "            'weight_gain_cause': baseline_info.get('weight_gain_cause'),\n",
    "            'genomics_sample_id': baseline_info.get('genomics_sample_id')\n",
    "        }\n",
    "\n",
    "        followup_measurements = group[group['measurement_date'] > baseline_date].sort_values('measurement_date')\n",
    "\n",
    "        # 2. Calculate Overall Follow-up Metrics\n",
    "        if not followup_measurements.empty:\n",
    "            last_measurement = followup_measurements.iloc[-1]\n",
    "            result['last_aval_date'] = last_measurement['measurement_date']\n",
    "            result['total_followup_days'] = (last_measurement['measurement_date'] - baseline_date).days + 1\n",
    "            result['last_aval_weight_kg'] = last_measurement['weight_kg']\n",
    "            wl_kg, wl_pct = _calculate_wl_metrics(baseline_weight, last_measurement['weight_kg'])\n",
    "            result['total_wl_kg'] = wl_kg\n",
    "            result['total_wl_%'] = wl_pct\n",
    "            # --- NEW: Get final BMI and calculate reduction ---\n",
    "            result['final_bmi'] = last_measurement.get('bmi') # Get BMI from last measurement\n",
    "            result['bmi_reduction'] = _calculate_bmi_reduction(result['baseline_bmi'], result['final_bmi'])\n",
    "        else:\n",
    "            # Handle instant dropouts\n",
    "            result['last_aval_date'] = baseline_date\n",
    "            result['total_followup_days'] = 1\n",
    "            result['last_aval_weight_kg'] = baseline_weight\n",
    "            result['total_wl_kg'] = 0.0\n",
    "            result['total_wl_%'] = 0.0\n",
    "            # --- NEW: Set final BMI and reduction for instant dropouts ---\n",
    "            result['final_bmi'] = result['baseline_bmi'] # Final BMI is baseline BMI\n",
    "            result['bmi_reduction'] = 0.0 # No change\n",
    "\n",
    "        # Calculate avg_days_between_measurements\n",
    "        if result['nr_total_measurements'] is not None and result['nr_total_measurements'] > 1:\n",
    "             total_days = result['total_followup_days']\n",
    "             result['avg_days_between_measurements'] = round( (total_days -1) / (result['nr_total_measurements'] - 1) , 2) if (result['nr_total_measurements'] - 1) > 0 else np.nan\n",
    "        else:\n",
    "             result['avg_days_between_measurements'] = np.nan\n",
    "\n",
    "\n",
    "        # 3. Calculate Fixed-Timepoint Metrics (for each time window)\n",
    "        # --- MODIFIED: Excluded BMI calculation ---\n",
    "        for window_center in time_windows:\n",
    "            min_window_date = baseline_date + timedelta(days=(window_center - window_span))\n",
    "            max_window_date = baseline_date + timedelta(days=(window_center + window_span))\n",
    "            target_date = baseline_date + timedelta(days=window_center)\n",
    "\n",
    "            measurements_around_cutoff = followup_measurements[\n",
    "                (followup_measurements['measurement_date'] >= min_window_date) &\n",
    "                (followup_measurements['measurement_date'] <= max_window_date)\n",
    "            ]\n",
    "\n",
    "            measurement_for_window = None\n",
    "            is_dropout_at_window = True\n",
    "\n",
    "            if not measurements_around_cutoff.empty:\n",
    "                is_dropout_at_window = False\n",
    "                measurements_around_cutoff = measurements_around_cutoff.copy()\n",
    "                measurements_around_cutoff['distance_to_center'] = abs(\n",
    "                    (measurements_around_cutoff['measurement_date'] - target_date).dt.days\n",
    "                )\n",
    "                closest_measurement_idx = measurements_around_cutoff['distance_to_center'].idxmin()\n",
    "                measurement_for_window = measurements_around_cutoff.loc[closest_measurement_idx]\n",
    "\n",
    "            prefix = f\"{window_center}d\"\n",
    "            result[f'{prefix}_dropout'] = is_dropout_at_window\n",
    "\n",
    "            if is_dropout_at_window:\n",
    "                result[f'{prefix}_weight_kg'] = np.nan\n",
    "                result[f'wl_{prefix}_kg'] = np.nan\n",
    "                result[f'wl_{prefix}_%'] = np.nan\n",
    "                result[f'{prefix}_date'] = pd.NaT\n",
    "                result[f'days_to_{prefix}_measurement'] = np.nan\n",
    "                # Removed: result[f'{prefix}_bmi'] = np.nan\n",
    "            else:\n",
    "                result[f'{prefix}_weight_kg'] = measurement_for_window['weight_kg']\n",
    "                wl_kg, wl_pct = _calculate_wl_metrics(baseline_weight, measurement_for_window['weight_kg'])\n",
    "                result[f'wl_{prefix}_kg'] = wl_kg\n",
    "                result[f'wl_{prefix}_%'] = wl_pct\n",
    "                result[f'{prefix}_date'] = measurement_for_window['measurement_date']\n",
    "                result[f'days_to_{prefix}_measurement'] = (measurement_for_window['measurement_date'] - baseline_date).days + 1\n",
    "                # Removed: result[f'{prefix}_bmi'] = measurement_for_window.get('bmi')\n",
    "\n",
    "\n",
    "        # 4. Calculate Time-to-Event Metrics (for each weight loss target)\n",
    "        # (No changes needed in this section)\n",
    "        for target in weight_loss_targets:\n",
    "            target_achieved = False\n",
    "            first_success_measurement = None\n",
    "            actual_wl_at_success = np.nan\n",
    "\n",
    "            for _, row in followup_measurements.iterrows():\n",
    "                current_weight = row['weight_kg']\n",
    "                if baseline_weight is not None and baseline_weight > 0:\n",
    "                    current_weight_loss_pct = ((baseline_weight - current_weight) / baseline_weight) * 100\n",
    "                    if round(current_weight_loss_pct, 2) >= target:\n",
    "                        target_achieved = True\n",
    "                        first_success_measurement = row\n",
    "                        actual_wl_at_success = round(current_weight_loss_pct, 2)\n",
    "                        break\n",
    "\n",
    "            prefix = f\"{target}%_wl\"\n",
    "            result[f'{prefix}_achieved'] = target_achieved\n",
    "            if target_achieved and first_success_measurement is not None:\n",
    "                result[f'{prefix}_%'] = actual_wl_at_success\n",
    "                result[f'{prefix}_date'] = first_success_measurement['measurement_date']\n",
    "                result[f'days_to_{prefix}'] = (first_success_measurement['measurement_date'] - baseline_date).days + 1\n",
    "            else:\n",
    "                result[f'{prefix}_%'] = np.nan\n",
    "                result[f'{prefix}_date'] = pd.NaT\n",
    "                result[f'days_to_{prefix}'] = np.nan\n",
    "\n",
    "        results_list.append(result)\n",
    "\n",
    "    return pd.DataFrame(results_list)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "MODIFIED: MAIN ORCHESTRATION FUNCTION FOR WIDE TABLE\n",
    "\"\"\"\n",
    "\n",
    "def generate_wide_analysis_dataset(input_connection, output_connection, weight_loss_targets, time_windows, window_span=10):\n",
    "    \"\"\"\n",
    "    Orchestrates the process to generate the single wide survival analysis dataset.\n",
    "    Loads data, prepares patient baseline info, calculates all outcomes per patient,\n",
    "    reorders columns, and saves the resulting wide DataFrame to the output database.\n",
    "    --- MODIFIED: Uses output_column_order ---\n",
    "    \"\"\"\n",
    "    # 1. Load and prepare input data\n",
    "    print(\"Loading measurements...\")\n",
    "    measurements = load_measurements(input_connection)\n",
    "    print(\"Loading medical records...\")\n",
    "    medical_records = load_medical_records(input_connection)\n",
    "    print(\"Preparing patient data...\")\n",
    "    prepared_data, filtered_measurements = prepare_patient_data(measurements, medical_records)\n",
    "\n",
    "    if prepared_data.empty:\n",
    "        print(\"ERROR: Prepared patient data is empty. Cannot proceed.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # 2. Calculate wide outcomes for all patients\n",
    "    print(\"Calculating wide outcomes for all patients...\")\n",
    "    wide_results_df = calculate_wide_patient_outcomes(\n",
    "        prepared_data,\n",
    "        filtered_measurements,\n",
    "        weight_loss_targets,\n",
    "        time_windows,\n",
    "        window_span\n",
    "    )\n",
    "\n",
    "    # --- MODIFIED: Reorder columns according to output_column_order ---\n",
    "    if not wide_results_df.empty:\n",
    "        print(\"Reordering columns...\")\n",
    "        # Ensure all columns in output_column_order exist in the DataFrame\n",
    "        final_columns_present = [col for col in output_column_order if col in wide_results_df.columns]\n",
    "        missing_cols = [col for col in output_column_order if col not in wide_results_df.columns]\n",
    "        if missing_cols:\n",
    "             print(f\"WARN: The following columns defined in output_column_order were not found and will be skipped: {missing_cols}\")\n",
    "        # Add any extra columns not in the defined order to the end\n",
    "        extra_cols = [col for col in wide_results_df.columns if col not in final_columns_present]\n",
    "        if extra_cols:\n",
    "             print(f\"WARN: The following columns were generated but not in output_column_order; adding to the end: {extra_cols}\")\n",
    "\n",
    "        wide_results_df = wide_results_df[final_columns_present + extra_cols]\n",
    "\n",
    "\n",
    "    # 3. Save the single wide table\n",
    "    if not wide_results_df.empty:\n",
    "        print(f\"--- Saving results to output database: {output_db_path} ---\")\n",
    "        print(f\"Saving table: {output_table_name} ({len(wide_results_df)} rows)\")\n",
    "        wide_results_df.to_sql(output_table_name, output_connection, if_exists='replace', index=False)\n",
    "        output_connection.commit()\n",
    "        print(\"--- Wide table saved successfully ---\")\n",
    "    else:\n",
    "        print(\"WARN: No results generated. Output table will be empty or not created.\")\n",
    "\n",
    "    return wide_results_df\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "EXECUTION BLOCK (Modified to use new output names)\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"========== Generating Survival Analysis Input Dataset (Wide Format v3) ==========\") # Updated title\n",
    "    input_conn = None\n",
    "    output_conn = None\n",
    "    try:\n",
    "        print(f\"Connecting to input database: {input_db_path}\")\n",
    "        if not os.path.exists(input_db_path):\n",
    "             raise FileNotFoundError(f\"Input database not found at {input_db_path}\")\n",
    "        input_conn = sqlite3.connect(input_db_path)\n",
    "\n",
    "        print(f\"Connecting to output database: {output_db_path}\")\n",
    "        output_conn = sqlite3.connect(output_db_path)\n",
    "\n",
    "        wide_df = generate_wide_analysis_dataset(\n",
    "            input_conn,\n",
    "            output_conn,\n",
    "            weight_loss_targets,\n",
    "            time_windows,\n",
    "            window_span\n",
    "        )\n",
    "\n",
    "        if not wide_df.empty:\n",
    "            print(\"\\n--- Survival Analysis Input Table Generation Summary ---\")\n",
    "            print(f\"Generated table '{output_table_name}' with {len(wide_df)} rows and {len(wide_df.columns)} columns.\")\n",
    "            # print(wide_df.head().to_string())\n",
    "            print(\"--- End Summary ---\")\n",
    "        else:\n",
    "            print(\"WARN: Analysis completed, but the resulting DataFrame is empty.\")\n",
    "\n",
    "        print(f\"Analysis data saved to {output_db_path}\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"ERROR: Database file not found - {e}\")\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"ERROR: SQLite database error - {e}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"ERROR: Data processing error - {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: An unexpected error occurred - {e}\")\n",
    "        # import traceback\n",
    "        # print(traceback.format_exc())\n",
    "    finally:\n",
    "        print(\"Closing database connections...\")\n",
    "        if input_conn:\n",
    "            input_conn.close()\n",
    "        if output_conn:\n",
    "            output_conn.close()\n",
    "        print(\"========== Survival Analysis Input Data Generation Finished (Wide Format v3) ==========\") # Updated title\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start studying the survival analysis dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation of summary stats tables in the database - the code is not revised, the output is draft-level, but it is already insightful and seems correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Connecting to database: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\survival_analysis.sqlite\n",
      "Loading input table: sa_input_table\n",
      "Generating population summary data (N=1664)...\n",
      "\n",
      "--- Population Summary (population_summary) ---\n",
      "                                   N         N (%)    Mean (SD)      Median (IQR)     Min - Max\n",
      "Statistic                                                                                      \n",
      "Total Population              1664.0           NaN          NaN               NaN           NaN\n",
      "age                           1620.0           NaN  47.5 ± 10.2  48.0 (41.0-54.0)   18.0 - 84.0\n",
      "height_m                      1664.0           NaN    1.7 ± 0.1     1.6 (1.6-1.7)     1.4 - 2.1\n",
      "baseline_weight_kg            1664.0           NaN  82.8 ± 12.1  82.0 (74.0-89.1)  57.5 - 131.2\n",
      "last_aval_weight_kg           1664.0           NaN  76.3 ± 10.8  74.7 (68.4-82.4)  53.7 - 130.5\n",
      "baseline_bmi                  1664.0           NaN   30.2 ± 3.0  30.1 (27.7-32.5)   25.0 - 41.0\n",
      "final_bmi                     1664.0           NaN   27.8 ± 2.7  27.0 (25.4-29.7)   25.0 - 40.5\n",
      "emotional_eating_value        1664.0           NaN    6.8 ± 2.4     7.0 (5.0-8.0)    1.0 - 10.0\n",
      "quantity_control              1664.0           NaN    5.8 ± 2.4     6.0 (5.0-8.0)    1.0 - 10.0\n",
      "impulse_control               1664.0           NaN    6.0 ± 2.3     6.0 (5.0-8.0)    1.0 - 10.0\n",
      "Sex: Female                      NaN  1368 (82.2%)          NaN               NaN           NaN\n",
      "Hunger: Yes                      NaN  1098 (66.0%)          NaN               NaN           NaN\n",
      "Satiety: Yes                     NaN  1009 (60.6%)          NaN               NaN           NaN\n",
      "Emotional Eating: Yes            NaN  1301 (78.2%)          NaN               NaN           NaN\n",
      "Weight Gain Cause Available      NaN   504 (30.3%)          NaN               NaN           NaN\n",
      "Genomics Sample ID Available     NaN   332 (20.0%)          NaN               NaN           NaN\n",
      "\n",
      "Saving population summary to table: population_summary\n",
      "Generating outcome summary data (N=1664)...\n",
      "\n",
      "--- Outcome Summary (outcome_summary) ---\n",
      "                                    N         N (%)    Mean (SD)      Median (IQR)    Min - Max\n",
      "Statistic                                                                                      \n",
      "Total Population               1664.0           NaN          NaN               NaN          NaN\n",
      "total_followup_days            1664.0           NaN  59.7 ± 72.3  36.0 (13.0-79.0)  1.0 - 632.0\n",
      "nr_visits                      1664.0           NaN    5.1 ± 4.4     4.0 (2.0-7.0)   1.0 - 33.0\n",
      "nr_total_measurements          1664.0           NaN  12.5 ± 15.7    8.0 (3.0-16.0)  1.0 - 188.0\n",
      "avg_days_between_measurements  1499.0           NaN    6.6 ± 5.7     5.5 (3.0-8.0)   0.0 - 60.0\n",
      "total_wl_kg                    1664.0           NaN    6.5 ± 5.6     5.2 (2.2-9.7)  -7.7 - 39.6\n",
      "total_wl_%                     1664.0           NaN    7.6 ± 6.1    6.6 (2.8-11.3)  -9.6 - 36.1\n",
      "bmi_reduction                  1664.0           NaN   -2.4 ± 2.0  -1.9 (-3.5--0.8)  -14.7 - 2.7\n",
      "WL (kg) at 40d [Completers]     874.0           NaN    7.0 ± 3.0               NaN          NaN\n",
      "WL (%) at 40d [Completers]      874.0           NaN    8.1 ± 2.9               NaN          NaN\n",
      "WL (kg) at 60d [Completers]     592.0           NaN    9.2 ± 3.9               NaN          NaN\n",
      "WL (%) at 60d [Completers]      592.0           NaN   10.4 ± 3.8               NaN          NaN\n",
      "WL (kg) at 80d [Completers]     417.0           NaN   10.8 ± 4.3               NaN          NaN\n",
      "WL (%) at 80d [Completers]      417.0           NaN   12.2 ± 4.1               NaN          NaN\n",
      "Days to 5% WL [Achievers]      1034.0           NaN  22.6 ± 14.3               NaN          NaN\n",
      "Days to 10% WL [Achievers]      546.0           NaN  53.2 ± 25.5               NaN          NaN\n",
      "Days to 15% WL [Achievers]      227.0           NaN  88.6 ± 36.0               NaN          NaN\n",
      "Instant Dropouts                  NaN   187 (11.2%)          NaN               NaN          NaN\n",
      "Completers at 40d                 NaN   874 (52.5%)          NaN               NaN          NaN\n",
      "Completers at 60d                 NaN   592 (35.6%)          NaN               NaN          NaN\n",
      "Completers at 80d                 NaN   417 (25.1%)          NaN               NaN          NaN\n",
      "Achieved 5% WL                    NaN  1034 (62.1%)          NaN               NaN          NaN\n",
      "Achieved 10% WL                   NaN   546 (32.8%)          NaN               NaN          NaN\n",
      "Achieved 15% WL                   NaN   227 (13.6%)          NaN               NaN          NaN\n",
      "\n",
      "Saving outcome summary to table: outcome_summary\n",
      "\n",
      "Summary tables saved successfully.\n",
      "Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "# This code calculates detailed population and outcome summary statistics,\n",
    "# ensures variables are columns and stats are rows in the final output,\n",
    "# fixes categorical counts (both population and outcome),\n",
    "# and saves them to the specified table names in the SQLite database.\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "CONFIGURATION\n",
    "\"\"\"\n",
    "# Define database path, input table name, and output table names\n",
    "# Ensure 'paper1_directory' is defined in your notebook environment before running this cell\n",
    "if 'paper1_directory' not in locals() and 'paper1_directory' not in globals():\n",
    "     raise NameError(\"'paper1_directory' is not defined. Please define it in a previous cell.\")\n",
    "\n",
    "db_path = os.path.join(paper1_directory, 'survival_analysis.sqlite') # DB with the final wide table\n",
    "input_table_name = \"sa_input_table\" # The wide table created previously\n",
    "output_pop_summary_table = \"population_summary\"\n",
    "output_outcome_summary_table = \"outcome_summary\"\n",
    "\n",
    "# Define dynamic parameters (should match those used to create sa_input_table)\n",
    "weight_loss_targets = [5, 10, 15]\n",
    "time_windows = [40, 60, 80]\n",
    "\n",
    "\"\"\"\n",
    "HELPER FUNCTIONS\n",
    "\"\"\"\n",
    "\n",
    "def format_mean_sd(series, decimals=1):\n",
    "    \"\"\"Calculates mean and SD, returns formatted string 'mean ± SD'.\"\"\"\n",
    "    numeric_series = pd.to_numeric(series, errors='coerce')\n",
    "    if numeric_series.isnull().all() or numeric_series.empty:\n",
    "        return np.nan\n",
    "    mean = numeric_series.mean()\n",
    "    std = numeric_series.std()\n",
    "    if pd.isna(mean) or pd.isna(std):\n",
    "         return np.nan\n",
    "    return f\"{mean:.{decimals}f} ± {std:.{decimals}f}\"\n",
    "\n",
    "def format_n_percent(series, condition_value, total_n, decimals=1):\n",
    "    \"\"\"Calculates N and % matching a condition (primarily for strings),\n",
    "       returns formatted string 'N (X.X%)'. Case-insensitive for strings.\n",
    "    \"\"\"\n",
    "    if total_n == 0:\n",
    "        return \"0 (NaN%)\"\n",
    "\n",
    "    # Primarily designed for string comparison now\n",
    "    condition_str = str(condition_value).lower()\n",
    "    try:\n",
    "        # Convert series to string, strip whitespace, convert to lower case, and compare\n",
    "        condition_mask = series.astype(str).str.strip().str.lower().eq(condition_str)\n",
    "        n = condition_mask.sum()\n",
    "    except Exception as e: # Broad exception catch if string methods fail\n",
    "        print(f\"  Warning: Could not perform string comparison for series '{series.name}' with value '{condition_value}'. Error: {e}. Falling back to direct comparison.\")\n",
    "        # Fallback for non-string types or errors during string conversion\n",
    "        try:\n",
    "            condition_mask = (series == condition_value)\n",
    "            n = condition_mask.sum()\n",
    "        except TypeError:\n",
    "             print(f\"  Error: Type error during fallback comparison for series '{series.name}' with value '{condition_value}'. Setting N to 0.\")\n",
    "             n = 0\n",
    "        except Exception as e_fallback:\n",
    "             print(f\"  Error: Unexpected error during fallback comparison for series '{series.name}' with value '{condition_value}'. Error: {e_fallback}. Setting N to 0.\")\n",
    "             n = 0\n",
    "\n",
    "    percent = (n / total_n) * 100 if total_n > 0 else 0\n",
    "    return f\"{int(n)} ({percent:.{decimals}f}%)\" # Ensure n is int for formatting\n",
    "\n",
    "\n",
    "def get_describe_stats(df, columns):\n",
    "    \"\"\"Runs describe() and extracts key stats for specified columns.\"\"\"\n",
    "    actual_cols = [col for col in columns if col in df.columns]\n",
    "    if not actual_cols:\n",
    "        return pd.DataFrame()\n",
    "    numeric_subset = df[actual_cols].select_dtypes(include=np.number)\n",
    "    if numeric_subset.empty:\n",
    "        # print(f\"  Warning: No numeric columns found among {actual_cols} for describe.\") # Less verbose\n",
    "        return pd.DataFrame()\n",
    "    described = numeric_subset.describe(percentiles=[.25, .5, .75]).transpose()\n",
    "    stats_to_keep = ['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']\n",
    "    described = described[[col for col in stats_to_keep if col in described.columns]]\n",
    "    return described\n",
    "\n",
    "\"\"\"\n",
    "SUMMARY TABLE GENERATION FUNCTIONS\n",
    "\"\"\"\n",
    "\n",
    "def generate_population_summary(df):\n",
    "    \"\"\"Generates the population summary data (Variables as index, Stats as columns).\"\"\"\n",
    "    # NOTE: This function returns the data BEFORE transposition.\n",
    "    if df.empty:\n",
    "        print(\"Input DataFrame is empty. Cannot generate population summary.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    total_n = len(df)\n",
    "    summary_data_by_var = {}\n",
    "    print(f\"Generating population summary data (N={total_n})...\")\n",
    "\n",
    "    def add_stats(var_name, stats_dict):\n",
    "        if var_name not in summary_data_by_var:\n",
    "            summary_data_by_var[var_name] = {}\n",
    "        summary_data_by_var[var_name].update(stats_dict)\n",
    "\n",
    "    add_stats('Total Population', {'N': total_n})\n",
    "\n",
    "    # --- Numerical Summaries ---\n",
    "    numeric_cols_pop = [\n",
    "        'age', 'height_m', 'baseline_weight_kg', 'last_aval_weight_kg',\n",
    "        'baseline_bmi', 'final_bmi',\n",
    "        'emotional_eating_value', 'quantity_control', 'impulse_control'\n",
    "    ]\n",
    "    pop_described = get_describe_stats(df, numeric_cols_pop)\n",
    "    for col in pop_described.index:\n",
    "        stats_for_col = {}\n",
    "        stats_for_col['N'] = int(pop_described.loc[col, 'count']) if 'count' in pop_described.columns else np.nan\n",
    "        stats_for_col['Mean (SD)'] = format_mean_sd(df[col])\n",
    "        if '50%' in pop_described.columns and '25%' in pop_described.columns and '75%' in pop_described.columns:\n",
    "             median, iqr_25, iqr_75 = pop_described.loc[col, ['50%', '25%', '75%']]\n",
    "             if not pd.isna([median, iqr_25, iqr_75]).any():\n",
    "                  stats_for_col['Median (IQR)'] = f\"{median:.1f} ({iqr_25:.1f}-{iqr_75:.1f})\"\n",
    "             else: stats_for_col['Median (IQR)'] = np.nan\n",
    "        else: stats_for_col['Median (IQR)'] = np.nan\n",
    "        if 'min' in pop_described.columns and 'max' in pop_described.columns:\n",
    "             min_val, max_val = pop_described.loc[col, ['min', 'max']]\n",
    "             if not pd.isna([min_val, max_val]).any():\n",
    "                  stats_for_col['Min - Max'] = f\"{min_val:.1f} - {max_val:.1f}\"\n",
    "             else: stats_for_col['Min - Max'] = np.nan\n",
    "        else: stats_for_col['Min - Max'] = np.nan\n",
    "        add_stats(col, stats_for_col)\n",
    "\n",
    "    # --- Categorical/Boolean Summaries ---\n",
    "    # Sex ('Female') - Use format_n_percent for string check\n",
    "    if 'sex' in df.columns:\n",
    "        add_stats('Sex: Female', {'N (%)': format_n_percent(df['sex'], 'Female', total_n)})\n",
    "    else: print(\"  Warning: 'sex' column not found.\")\n",
    "\n",
    "    # Yes/No Questions - Use format_n_percent for string check\n",
    "    for col_name, display_name in [('hunger', 'Hunger: Yes'), ('satiety', 'Satiety: Yes'), ('emotional_eating', 'Emotional Eating: Yes')]:\n",
    "        if col_name in df.columns:\n",
    "             add_stats(display_name, {'N (%)': format_n_percent(df[col_name], 'Yes', total_n)})\n",
    "        else: print(f\"  Warning: '{col_name}' column not found.\")\n",
    "\n",
    "    # Availability Checks (Not NULL) - Direct calculation\n",
    "    for col_name, display_name in [('weight_gain_cause', 'Weight Gain Cause Available'), ('genomics_sample_id', 'Genomics Sample ID Available')]:\n",
    "        if col_name in df.columns:\n",
    "            n_not_null = df[col_name].notna().sum()\n",
    "            percent_not_null = (n_not_null / total_n) * 100 if total_n > 0 else 0\n",
    "            add_stats(display_name, {'N (%)': f\"{n_not_null} ({percent_not_null:.1f}%)\"})\n",
    "        else: print(f\"  Warning: '{col_name}' column not found.\")\n",
    "\n",
    "    # Convert dictionary to DataFrame (Variables as index, Stats as columns)\n",
    "    summary_df = pd.DataFrame.from_dict(summary_data_by_var, orient='index')\n",
    "    summary_df.index.name = 'Variable'\n",
    "\n",
    "    # Reorder STAT columns (optional, done before transpose)\n",
    "    desired_col_order = ['N', 'N (%)', 'Mean (SD)', 'Median (IQR)', 'Min - Max']\n",
    "    existing_cols_ordered = [col for col in desired_col_order if col in summary_df.columns]\n",
    "    remaining_cols = [col for col in summary_df.columns if col not in existing_cols_ordered]\n",
    "    final_col_order = existing_cols_ordered + remaining_cols\n",
    "    summary_df = summary_df[final_col_order]\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "\n",
    "def generate_outcome_summary(df, weight_targets, time_windows_list):\n",
    "    \"\"\"Generates the outcome summary data (Variables as index, Stats as columns).\"\"\"\n",
    "    # NOTE: This function returns the data BEFORE transposition.\n",
    "    if df.empty:\n",
    "        print(\"Input DataFrame is empty. Cannot generate outcome summary.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    total_n = len(df)\n",
    "    summary_data_by_var = {}\n",
    "    print(f\"Generating outcome summary data (N={total_n})...\")\n",
    "\n",
    "    def add_stats(var_name, stats_dict):\n",
    "        if var_name not in summary_data_by_var:\n",
    "            summary_data_by_var[var_name] = {}\n",
    "        for key, value in stats_dict.items():\n",
    "            summary_data_by_var[var_name][key] = value\n",
    "\n",
    "    add_stats('Total Population', {'N': total_n})\n",
    "\n",
    "    # --- Overall Adherence & Outcome Summaries ---\n",
    "    numeric_cols_outcome = [\n",
    "        'total_followup_days', 'nr_visits', 'nr_total_measurements', 'avg_days_between_measurements',\n",
    "        'total_wl_kg', 'total_wl_%', 'bmi_reduction'\n",
    "    ]\n",
    "    outcome_described = get_describe_stats(df, numeric_cols_outcome)\n",
    "    for col in outcome_described.index:\n",
    "        stats_for_col = {}\n",
    "        stats_for_col['N'] = int(outcome_described.loc[col, 'count']) if 'count' in outcome_described.columns else np.nan\n",
    "        stats_for_col['Mean (SD)'] = format_mean_sd(df[col])\n",
    "        if '50%' in outcome_described.columns and '25%' in outcome_described.columns and '75%' in outcome_described.columns:\n",
    "             median, iqr_25, iqr_75 = outcome_described.loc[col, ['50%', '25%', '75%']]\n",
    "             if not pd.isna([median, iqr_25, iqr_75]).any():\n",
    "                  stats_for_col['Median (IQR)'] = f\"{median:.1f} ({iqr_25:.1f}-{iqr_75:.1f})\"\n",
    "             else: stats_for_col['Median (IQR)'] = np.nan\n",
    "        else: stats_for_col['Median (IQR)'] = np.nan\n",
    "        if 'min' in outcome_described.columns and 'max' in outcome_described.columns:\n",
    "             min_val, max_val = outcome_described.loc[col, ['min', 'max']]\n",
    "             if not pd.isna([min_val, max_val]).any():\n",
    "                  stats_for_col['Min - Max'] = f\"{min_val:.1f} - {max_val:.1f}\"\n",
    "             else: stats_for_col['Min - Max'] = np.nan\n",
    "        else: stats_for_col['Min - Max'] = np.nan\n",
    "        add_stats(col, stats_for_col)\n",
    "\n",
    "    # --- Specific Outcome Metrics ---\n",
    "    # Instant Dropouts (total_followup_days == 1) - Direct calculation assuming numeric/bool\n",
    "    if 'total_followup_days' in df.columns:\n",
    "        try:\n",
    "            # Attempt direct comparison (works for numbers, might work for bools if 1 used)\n",
    "            instant_dropout_mask = (df['total_followup_days'] == 1)\n",
    "            n_instant_dropout = instant_dropout_mask.sum()\n",
    "            percent_instant = (n_instant_dropout / total_n) * 100 if total_n > 0 else 0\n",
    "            add_stats('Instant Dropouts', {'N (%)': f\"{n_instant_dropout} ({percent_instant:.1f}%)\"})\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Could not calculate Instant Dropouts directly. Error: {e}. Trying format_n_percent.\")\n",
    "            # Fallback to string comparison if direct fails\n",
    "            add_stats('Instant Dropouts', {'N (%)': format_n_percent(df['total_followup_days'], 1, total_n)})\n",
    "    else: print(\"  Warning: 'total_followup_days' column not found for Instant Dropout calculation.\")\n",
    "\n",
    "    # --- Dynamic Time Window Metrics ---\n",
    "    for window in time_windows_list:\n",
    "        dropout_col = f'{window}d_dropout'\n",
    "        wl_kg_col = f'wl_{window}d_kg'\n",
    "        wl_pct_col = f'wl_{window}d_%'\n",
    "        completer_var_name = f'Completers at {window}d'\n",
    "        wl_kg_var_name = f'WL (kg) at {window}d [Completers]'\n",
    "        wl_pct_var_name = f'WL (%) at {window}d [Completers]'\n",
    "\n",
    "        if dropout_col in df.columns:\n",
    "            # N (%) Completers (Not Dropout) - Direct calculation assuming boolean False\n",
    "            try:\n",
    "                completers_mask = (df[dropout_col] == False) # Explicitly check for boolean False\n",
    "                n_completers = completers_mask.sum()\n",
    "                percent_completers = (n_completers / total_n) * 100 if total_n > 0 else 0\n",
    "                add_stats(completer_var_name, {'N (%)': f\"{n_completers} ({percent_completers:.1f}%)\"})\n",
    "\n",
    "                # Mean (SD) Weight Loss for Completers\n",
    "                completers_df = df.loc[completers_mask].copy()\n",
    "                if not completers_df.empty:\n",
    "                    if wl_kg_col in completers_df.columns:\n",
    "                        add_stats(wl_kg_var_name, {\n",
    "                            'Mean (SD)': format_mean_sd(completers_df[wl_kg_col]),\n",
    "                            'N': len(completers_df[wl_kg_col].dropna())\n",
    "                        })\n",
    "                    else:\n",
    "                        print(f\"  Warning: '{wl_kg_col}' column not found.\")\n",
    "                        add_stats(wl_kg_var_name, {'Mean (SD)': np.nan, 'N': 0})\n",
    "                    if wl_pct_col in completers_df.columns:\n",
    "                        add_stats(wl_pct_var_name, {\n",
    "                            'Mean (SD)': format_mean_sd(completers_df[wl_pct_col]),\n",
    "                            'N': len(completers_df[wl_pct_col].dropna())\n",
    "                        })\n",
    "                    else:\n",
    "                        print(f\"  Warning: '{wl_pct_col}' column not found.\")\n",
    "                        add_stats(wl_pct_var_name, {'Mean (SD)': np.nan, 'N': 0})\n",
    "                else:\n",
    "                    # print(f\"  Note: No completers found for {window}d window to calculate WL stats.\") # Less verbose\n",
    "                    add_stats(wl_kg_var_name, {'Mean (SD)': np.nan, 'N': 0})\n",
    "                    add_stats(wl_pct_var_name, {'Mean (SD)': np.nan, 'N': 0})\n",
    "\n",
    "            except Exception as e:\n",
    "                 print(f\"  Error calculating completer stats for {window}d. Column '{dropout_col}' might not be boolean. Error: {e}\")\n",
    "                 add_stats(completer_var_name, {'N (%)': 'Error'})\n",
    "                 add_stats(wl_kg_var_name, {'Mean (SD)': 'Error', 'N': 'Error'})\n",
    "                 add_stats(wl_pct_var_name, {'Mean (SD)': 'Error', 'N': 'Error'})\n",
    "\n",
    "        else:\n",
    "            print(f\"  Warning: Dropout column '{dropout_col}' not found for window {window}d.\")\n",
    "            add_stats(completer_var_name, {'N (%)': np.nan})\n",
    "            add_stats(wl_kg_var_name, {'Mean (SD)': np.nan, 'N': np.nan})\n",
    "            add_stats(wl_pct_var_name, {'Mean (SD)': np.nan, 'N': np.nan})\n",
    "\n",
    "\n",
    "    # --- Dynamic Weight Loss Target Metrics ---\n",
    "    for target in weight_targets:\n",
    "        achieved_col = f'{target}%_wl_achieved'\n",
    "        days_col = f'days_to_{target}%_wl'\n",
    "        achiever_var_name = f'Achieved {target}% WL'\n",
    "        days_var_name = f'Days to {target}% WL [Achievers]'\n",
    "\n",
    "        if achieved_col in df.columns:\n",
    "            # N (%) Achievers - Direct calculation assuming boolean True\n",
    "            try:\n",
    "                achievers_mask = (df[achieved_col] == True) # Explicitly check for boolean True\n",
    "                n_achievers = achievers_mask.sum()\n",
    "                percent_achievers = (n_achievers / total_n) * 100 if total_n > 0 else 0\n",
    "                add_stats(achiever_var_name, {'N (%)': f\"{n_achievers} ({percent_achievers:.1f}%)\"})\n",
    "\n",
    "                # Mean (SD) Days to Achievement (for Achievers)\n",
    "                if days_col in df.columns:\n",
    "                    achievers_df = df.loc[achievers_mask].copy()\n",
    "                    if not achievers_df.empty:\n",
    "                        add_stats(days_var_name, {\n",
    "                            'Mean (SD)': format_mean_sd(achievers_df[days_col]),\n",
    "                            'N': len(achievers_df[days_col].dropna())\n",
    "                        })\n",
    "                    else:\n",
    "                        # print(f\"  Note: No achievers found for {target}% target to calculate days.\") # Less verbose\n",
    "                        add_stats(days_var_name, {'Mean (SD)': np.nan, 'N': 0})\n",
    "                else:\n",
    "                    print(f\"  Warning: Days column '{days_col}' not found for target {target}%.\")\n",
    "                    add_stats(days_var_name, {'Mean (SD)': np.nan, 'N': np.nan})\n",
    "\n",
    "            except Exception as e:\n",
    "                 print(f\"  Error calculating achiever stats for {target}%. Column '{achieved_col}' might not be boolean. Error: {e}\")\n",
    "                 add_stats(achiever_var_name, {'N (%)': 'Error'})\n",
    "                 add_stats(days_var_name, {'Mean (SD)': 'Error', 'N': 'Error'})\n",
    "\n",
    "        else:\n",
    "            print(f\"  Warning: Achievement column '{achieved_col}' not found for target {target}%.\")\n",
    "            add_stats(achiever_var_name, {'N (%)': np.nan})\n",
    "            add_stats(days_var_name, {'Mean (SD)': np.nan, 'N': np.nan})\n",
    "\n",
    "\n",
    "    # Convert dictionary to DataFrame (Variables as index, Stats as columns)\n",
    "    summary_df = pd.DataFrame.from_dict(summary_data_by_var, orient='index')\n",
    "    summary_df.index.name = 'Variable'\n",
    "\n",
    "    # Reorder STAT columns (optional, done before transpose)\n",
    "    desired_col_order = ['N', 'N (%)', 'Mean (SD)', 'Median (IQR)', 'Min - Max']\n",
    "    existing_cols_ordered = [col for col in desired_col_order if col in summary_df.columns]\n",
    "    remaining_cols = [col for col in summary_df.columns if col not in existing_cols_ordered]\n",
    "    final_col_order = existing_cols_ordered + remaining_cols\n",
    "    summary_df = summary_df[final_col_order]\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "MAIN ORCHESTRATION FUNCTION\n",
    "\"\"\"\n",
    "\n",
    "def create_and_save_summary_tables(db_path, input_table, pop_table_out, outcome_table_out, weight_targets, time_windows_list):\n",
    "    \"\"\"Loads data, generates both summary tables, transposes for final output, and saves them.\"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        print(f\"\\nConnecting to database: {db_path}\")\n",
    "        if not os.path.exists(db_path):\n",
    "            print(f\"ERROR: Database file not found at {db_path}\")\n",
    "            return\n",
    "\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        print(f\"Loading input table: {input_table}\")\n",
    "        df = pd.read_sql_query(f\"SELECT * FROM {input_table}\", conn)\n",
    "\n",
    "        if df.empty:\n",
    "            print(f\"Input table '{input_table}' is empty. Cannot generate summaries.\")\n",
    "            return\n",
    "\n",
    "        # --- Generate Population Summary ---\n",
    "        pop_summary_raw = generate_population_summary(df.copy())\n",
    "        if not pop_summary_raw.empty:\n",
    "            print(f\"\\n--- Population Summary ({pop_table_out}) ---\")\n",
    "            # Transpose for final output (Stats as index/rows, Variables as columns)\n",
    "            pop_summary_final = pop_summary_raw #.transpose() OR MAYBE DON'T, Not as nice!\n",
    "            pop_summary_final.index.name = 'Statistic' # Index is now stats\n",
    "            print(pop_summary_final.to_string())\n",
    "            print(f\"\\nSaving population summary to table: {pop_table_out}\")\n",
    "            # Save the transposed DataFrame, index=True saves 'Statistic' column\n",
    "            pop_summary_final.to_sql(pop_table_out, conn, if_exists='replace', index=True)\n",
    "        else:\n",
    "            print(\"Population summary generation failed or resulted in an empty table.\")\n",
    "\n",
    "        # --- Generate Outcome Summary ---\n",
    "        outcome_summary_raw = generate_outcome_summary(df.copy(), weight_targets, time_windows_list)\n",
    "        if not outcome_summary_raw.empty:\n",
    "            print(f\"\\n--- Outcome Summary ({outcome_table_out}) ---\")\n",
    "            # Transpose for final output (Stats as index/rows, Variables as columns)\n",
    "\n",
    "            outcome_summary_final = outcome_summary_raw # .transpose() OR MAYBE DON'T, it is not as nice\n",
    "            outcome_summary_final.index.name = 'Statistic' # Index is now stats\n",
    "            print(outcome_summary_final.to_string())\n",
    "            print(f\"\\nSaving outcome summary to table: {outcome_table_out}\")\n",
    "            # Save the transposed DataFrame, index=True saves 'Statistic' column\n",
    "            outcome_summary_final.to_sql(outcome_table_out, conn, if_exists='replace', index=True)\n",
    "        else:\n",
    "            print(\"Outcome summary generation failed or resulted in an empty table.\")\n",
    "\n",
    "        conn.commit()\n",
    "        print(\"\\nSummary tables saved successfully.\")\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"ERROR: SQLite error - {e}\")\n",
    "        if conn: conn.rollback()\n",
    "    except pd.errors.DatabaseError as e:\n",
    "         print(f\"ERROR: Pandas/Database error during SQL operation - {e}\")\n",
    "         if conn: conn.rollback()\n",
    "    except KeyError as e:\n",
    "        print(f\"ERROR: A required column name was not found in the DataFrame: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: An unexpected error occurred - {e}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "        if conn: conn.rollback()\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "            print(\"Database connection closed.\")\n",
    "\n",
    "\"\"\"\n",
    "EXECUTION\n",
    "\"\"\"\n",
    "# Make sure 'paper1_directory' is defined before this point!\n",
    "if 'paper1_directory' in locals() or 'paper1_directory' in globals():\n",
    "    create_and_save_summary_tables(\n",
    "        db_path,\n",
    "        input_table_name,\n",
    "        output_pop_summary_table,\n",
    "        output_outcome_summary_table,\n",
    "        weight_loss_targets,\n",
    "        time_windows\n",
    "    )\n",
    "else:\n",
    "    print(\"ERROR: 'paper1_directory' variable is not defined. Please define it before running this cell.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
