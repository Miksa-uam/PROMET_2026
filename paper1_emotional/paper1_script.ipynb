{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper 1 - eating behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the relevant directories used in this paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is pulled from the standardized data folder; subsequently, it is stored and managed in the paper 1 folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the source and output directories\n",
    "source_directory = r\"C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\DB2_standard\"\n",
    "paper1_directory = r\"C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(paper1_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a research question-specific SQL database subset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check those medical records where any/3+/all emotional values are available, and filter the database to contain only the specified patients and medical records. Save the data to 3 new SQL files - one with any, one with some, one with all values available. For research purposes, the last one is most likely to be used. The first two may be relevant if trying to increase the sample size for one or a few specific emotional values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any emotional data points are available in 2482 records from 2437 patients\n",
      "At least 3 emotional data points are available in 2169 records from 2132 patients\n",
      "All emotional data points are available in 1853 records from 1826 patients\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Use the above defined directories\n",
    "db_path = os.path.join(source_directory, \"pnk_db2_colclean.sqlite\")\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# List all tables in the database\n",
    "query_tables = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "tables = pd.read_sql_query(query_tables, conn)\n",
    "table_names = tables['name'].tolist()\n",
    "\n",
    "# Define criteria for filtering for any/3+/all emotional values available\n",
    "def create_filtered_database(criteria, output_filename):\n",
    "    # Set up the appropriate query based on the criteria for the three scenarios\n",
    "    if criteria == \"any\":\n",
    "        # to select records where at least one emotional variable is not null\n",
    "        query = \"\"\"\n",
    "        SELECT medical_record_id, patient_id\n",
    "        FROM medical_records_colclean\n",
    "        WHERE hunger IS NOT NULL\n",
    "           OR satiety IS NOT NULL\n",
    "           OR emotional_eating IS NOT NULL\n",
    "           OR emotional_eating_value IS NOT NULL\n",
    "           OR quantity_control IS NOT NULL\n",
    "           OR impulse_control IS NOT NULL;\n",
    "        \"\"\"\n",
    "    elif criteria == \"3plus\":\n",
    "        # to select records where at least three emotional variables are not null\n",
    "        query = \"\"\"\n",
    "        SELECT medical_record_id, patient_id\n",
    "        FROM medical_records_colclean\n",
    "        WHERE (CASE WHEN hunger IS NOT NULL THEN 1 ELSE 0 END +\n",
    "               CASE WHEN satiety IS NOT NULL THEN 1 ELSE 0 END +\n",
    "               CASE WHEN emotional_eating IS NOT NULL THEN 1 ELSE 0 END +\n",
    "               CASE WHEN emotional_eating_value IS NOT NULL THEN 1 ELSE 0 END +\n",
    "               CASE WHEN quantity_control IS NOT NULL THEN 1 ELSE 0 END +\n",
    "               CASE WHEN impulse_control IS NOT NULL THEN 1 ELSE 0 END) >= 3;\n",
    "        \"\"\"\n",
    "    elif criteria == \"all\":\n",
    "        # to select records where all emotional variables are not null \n",
    "        query = \"\"\"\n",
    "        SELECT medical_record_id, patient_id\n",
    "        FROM medical_records_colclean\n",
    "        WHERE hunger IS NOT NULL\n",
    "          AND satiety IS NOT NULL\n",
    "          AND emotional_eating IS NOT NULL\n",
    "          AND emotional_eating_value IS NOT NULL\n",
    "          AND quantity_control IS NOT NULL\n",
    "          AND impulse_control IS NOT NULL;\n",
    "        \"\"\"\n",
    "    \n",
    "    # Get the relevant records, and extract their medical record and patient IDs\n",
    "    relevant_records = pd.read_sql_query(query, conn)\n",
    "    relevant_medical_record_ids = tuple(relevant_records['medical_record_id'])\n",
    "    relevant_patient_ids = tuple(relevant_records['patient_id'])\n",
    "    \n",
    "    # Create a new database in the output directory\n",
    "    output_db_path = os.path.join(paper1_directory, output_filename)\n",
    "    filtered_conn = sqlite3.connect(output_db_path)\n",
    "    \n",
    "    # Filter each table in the source SQl to only contain the records that comply the criteria of the given scenario; \n",
    "    # ie. they have records where any/3+/all emotional variables are available\n",
    "    for table_name in table_names:\n",
    "        if table_name.startswith(\"sqlite_\"):\n",
    "            # Skip any SQLite system tables\n",
    "            continue \n",
    "        # In case of tables that may contain several medical records from the same patient, \n",
    "        # filter by medical_record_id\n",
    "        if table_name == \"medical_records_colclean\" or table_name == \"prescriptions_colclean\":\n",
    "            query = f\"\"\"\n",
    "            SELECT * \n",
    "            FROM {table_name}\n",
    "            WHERE medical_record_id IN {relevant_medical_record_ids}\n",
    "            \"\"\"\n",
    "        else:\n",
    "            # For all other tables, filter by patient_id only, as medical record ID is not available in those\n",
    "            query_check_column = f\"PRAGMA table_info({table_name});\"\n",
    "            columns = pd.read_sql_query(query_check_column, conn)\n",
    "            if 'patient_id' not in columns['name'].values:\n",
    "                continue  # Skip tables without patient_id\n",
    "            query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM {table_name}\n",
    "            WHERE patient_id IN {relevant_patient_ids}\n",
    "            \"\"\"\n",
    "        \n",
    "        # Execute the given query and save the result in a new SQLite database\n",
    "        filtered_data = pd.read_sql_query(query, conn)\n",
    "        filtered_data.to_sql(table_name, filtered_conn, index=False, if_exists=\"replace\")\n",
    "    \n",
    "    filtered_conn.close()\n",
    "    return len(relevant_records), len(set(relevant_records['patient_id']))\n",
    "\n",
    "# Create and save the three databases for the three scenarios - any/3+/all emotional variables available\n",
    "any_count, any_patients = create_filtered_database(\"any\", \"emotional_any_notna.sqlite\")\n",
    "three_plus_count, three_plus_patients = create_filtered_database(\"3plus\", \"emotional_3plus_notna.sqlite\")\n",
    "all_count, all_patients = create_filtered_database(\"all\", \"emotional_all_notna.sqlite\")\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "# Print summary\n",
    "print(f\"Any emotional data points are available in {any_count} records from {any_patients} patients\")\n",
    "print(f\"At least 3 emotional data points are available in {three_plus_count} records from {three_plus_patients} patients\")\n",
    "print(f\"All emotional data points are available in {all_count} records from {all_patients} patients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean and link measurements to prescriptions and medical records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the patient ID and the date of a given measurements, look for prescriptions with the same patient ID that cover the range of time in which the measurement was taken. This way, measurements can be linked to important metadata, such as the prescription and medical record they belong to, the step of the programme they were taken in, etc. \n",
    "\n",
    "In summary, this is a key step in the research, without which data on any measurement's identity would be insufficient, and measurements from different prescriptions of the same individual could be mixed, for example. In a previous attempt, I tried identifying blocks of measurements as those that are taken within two months of each other, but I consider this a much more solid approach. \n",
    "\n",
    "It is important to note that some patients may take repeated measurements on the same occasion. These duplicates need to be removed, as they inflate the dataset. \n",
    "\n",
    "After removing the duplicates, measurements and prescriptions are linked in a two-step process. \n",
    "\n",
    "First, measurements are linked to all possible prescriptions that can belong to them based on the shared patient ID (this scenario where every option is linked to every option is called a Cartesian product). \n",
    "\n",
    "After, these possible links are filtered by date: a measurement belongs to a prescription if it is within its validity period, or is 5 days within its start or end dates. In the latter case, a measurement may be assigned to multiple prescriptions; if this happens, it is assigned to the one it is closer to in time. \n",
    "\n",
    "If a measurement is not succesfully linked to any prescription, it is lost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate measurements removed. There are 35709 measurements from 1826 patients.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Remove duplicate measurements before doing any data frame merging. \n",
    "Any measurement from the same patient on the same day (ignoring time) with the same weight should be considered a duplicate.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# Connect to the database, load the measurements table\n",
    "conn = sqlite3.connect(os.path.join(paper1_directory, \"emotional_all_notna.sqlite\"))\n",
    "measurements = pd.read_sql_query(\"SELECT * FROM measurements_colclean\", conn)\n",
    "\n",
    "# Convert measurement_date to datetime, if not already in that format. \n",
    "# Add a temporary column with the measurement date only; time is ignored, \n",
    "# as repeated measurements are at least a few seconds or minutes apart. \n",
    "measurements['measurement_date'] = pd.to_datetime(measurements['measurement_date'])\n",
    "measurements['measurement_date_date'] = measurements['measurement_date'].dt.date\n",
    "# After, remove duplicates based on patient id, date, and weight. \n",
    "# Drop the temporary column. \n",
    "measurements_rowclean = measurements.drop_duplicates(subset=['patient_id', 'measurement_date_date', 'weight_kg'])\n",
    "measurements_rowclean = measurements_rowclean.drop(columns=['measurement_date_date'])\n",
    "# Save the cleaned measurements back to the database with the _rowclean name code\n",
    "measurements_rowclean.to_sql(\"measurements_rowclean\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "print(f\"Duplicate measurements removed. There are {len(measurements_rowclean)} measurements from {measurements_rowclean['patient_id'].nunique()} patients.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measurements are linked to their corresponding prescriptions and medical records. \n",
      "There are a total of 20976 measurements from 1678 medical records of 1664 patients.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Link metadata from the prescriptions table to measurements. \n",
    "\n",
    "The two dataframes are merged based on patient_id, creating a Cartesian product of the two tables, \n",
    "where every measurement from one patient is linked to every possible prescription from that patient.\n",
    "\n",
    "This Cartesian product is then filtered based on the dates of both the measurement and the prescription, \n",
    "in order to, preferably, only consider a prescription being linked to a given measurement\n",
    "if the measurement date is between the prescription's start and end dates. \n",
    "\n",
    "If a measurement is not within any prescription's validity period, \n",
    "there is a permissivity of 5 days, meaning that a measurement can be linked to a prescription if\n",
    "it is within 5 days from the start or end date of the prescription.\n",
    "If this allows a measurement to be linked to multiple prescriptions,\n",
    "it is linked to the one it is closest to in date. \n",
    "\n",
    "If a measurement is not linked to any valid prescription, \n",
    "it is excluded from the outuput. \n",
    "\"\"\"\n",
    "\n",
    "# Connect to the paper-specific database, load the prescriptions table, and make sure its date values are in datetime format\n",
    "conn = sqlite3.connect(os.path.join(paper1_directory, \"emotional_all_notna.sqlite\"))\n",
    "prescriptions = pd.read_sql_query(\"SELECT * FROM prescriptions_colclean\", conn)\n",
    "prescriptions['prescription_creation_date'] = pd.to_datetime(prescriptions['prescription_creation_date'])\n",
    "prescriptions['prescription_validity_end_date'] = pd.to_datetime(prescriptions['prescription_validity_end_date'])\n",
    "\n",
    "# Merge the measurements and prescriptions data frames on patient ID,\n",
    "# creating the Cartesian product that needs further date-based filtering\n",
    "merged = pd.merge(measurements_rowclean, prescriptions, on=\"patient_id\", how=\"left\", suffixes=('_meas', '_presc'))\n",
    "\n",
    "# To execute date-based filtering: \n",
    "# First, define those measurements that are within the range of a prescription. \n",
    "# If any measurement can be assigned to a prescription based on this criteria, it will be. \n",
    "merged['measurement_in_prescription_range'] = (\n",
    "    (merged['measurement_date'] >= merged['prescription_creation_date']) &\n",
    "    (merged['measurement_date'] <= merged['prescription_validity_end_date'])\n",
    ")\n",
    "# If after this, a measurement is still not linked to any prescription due to not being in the range of any, \n",
    "# it will be linked to the prescription it is closest to, within a 5-day permissivity range. \n",
    "# For these out-of-range measurements, first, the distance from the start/end dates of any prescription is calculated. \n",
    "merged['days_before_prescription_start'] = (merged['prescription_creation_date'] - merged['measurement_date']).dt.days\n",
    "merged['days_after_prescription_end'] = (merged['measurement_date'] - merged['prescription_validity_end_date']).dt.days\n",
    "# After, near-range measurements are defined, \n",
    "# as measurements that are NOT within the range of any prescription, \n",
    "# AND they are at within 5 days before the start/after the end of any prescription. \n",
    "merged['measurement_near_prescription_range'] = (\n",
    "    (~merged['measurement_in_prescription_range']) &\n",
    "    (\n",
    "        ((merged['days_before_prescription_start'] <= 5) & (merged['days_before_prescription_start'] > 0)) |\n",
    "        ((merged['days_after_prescription_end'] <= 5) & (merged['days_after_prescription_end'] > 0))\n",
    "    )\n",
    ")\n",
    "# After, a distance metric calculation determines how far a given measurement is from a prescription. \n",
    "# In-range measurements get a distance metric of 0,\n",
    "# while out-of-range measurements get the minimum distance to any boundary they are close to. \n",
    "merged['measurement_distance_from_prescription_range'] = merged.apply(\n",
    "    lambda row: 0 if row['measurement_in_prescription_range'] else min(max(row['days_before_prescription_start'], 0), max(row['days_after_prescription_end'], 0)),\n",
    "    axis=1\n",
    ")\n",
    "# After defining the in-range and near-range logics, the database (currently containing Cartesian products) \n",
    "# is filtered to keep only in-or near-range measurements. \n",
    "# Any measurements not assigned to a prescription is lost. \n",
    "measurements_with_metadata = merged[merged['measurement_in_prescription_range'] | merged['measurement_near_prescription_range']].copy()\n",
    "# In edge cases where multiple prescriptions are linked to a single measurement, only the closest match is kept. \n",
    "# This is done by sorting the data frame by patient id, measurement date and distance from range, \n",
    "# and if multiple measurement-prescription pairs from the same patient on the same date are found, \n",
    "# duplicates are removed and only the row with the smallest distance from range is kept. \n",
    "measurements_with_metadata = measurements_with_metadata.sort_values(['patient_id', 'measurement_date', 'measurement_distance_from_prescription_range'])\n",
    "measurements_with_metadata = measurements_with_metadata.drop_duplicates(['patient_id', 'measurement_date'])\n",
    "\n",
    "# After filtering the data frame, columns are reordered, and any irrelevant ones, like prescribed supplements, are dropped. \n",
    "column_order = [\n",
    "    'patient_id',\n",
    "    'medical_record_id',\n",
    "    'prescription_id',\n",
    "    'measurement_date',\n",
    "    'prescription_creation_date',\n",
    "    'prescription_validity_end_date',\n",
    "    'prescription_validity_days',\n",
    "    'method',\n",
    "    'step',\n",
    "    'weight_kg',\n",
    "    'bmi',\n",
    "    'bmr_kcal',\n",
    "    'fat_%',\n",
    "    'vat_%',\n",
    "    'muscle_%',\n",
    "    'water_%',\n",
    "    'measurement_in_prescription_range',\n",
    "    'days_before_prescription_start',\n",
    "    'days_after_prescription_end',\n",
    "    'measurement_near_prescription_range',\n",
    "    'measurement_distance_from_prescription_range'\n",
    "]\n",
    "measurements_with_metadata = measurements_with_metadata[column_order]\n",
    "\n",
    "# The measurements_with_metadata data frame is saved within the SQL database, and some summary info is printed. \n",
    "measurements_with_metadata.to_sql(\"measurements_with_metadata\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "print(f\"Measurements are linked to their corresponding prescriptions and medical records. \\n\"\n",
    "    f\"There are a total of {measurements_with_metadata.shape[0]} measurements \"\n",
    "    f\"from {measurements_with_metadata['medical_record_id'].nunique()} medical records \"\n",
    "    f\"of {measurements_with_metadata['patient_id'].nunique()} patients.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add sex, genomics ID and baseline/final weight data to medical records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an effort to create data frames containing the most possible information in one place, the medical records data frame is completed with the sex (originally stored in Patients) as well as the baseline and final weight data (measurements linked to medical records stored in measurements_with_metadata) of patients. Genomics sample IDs are also fetched for patients that have it available. \n",
    "\n",
    "Categorical values (gender and 3/6 eating behavior values) are converted to boolean 0/1 integers instead of str values. 1 means Yes or Female. Accordingly, the sex column is named sex_f downstream to account for the fact that 1 = Female. The eating behavior values have a corresponding _yn or _likert appended to their name. \n",
    "\n",
    "Besides executing these merge operations, the code checks the time passed between baseline and final measurements in each medical record, along with whether the measurements are close to the beginning/end date of the medical record they belong to or not. This helps checking whether the length of the actual followup is similar to that of the medical record or not. \n",
    "\n",
    "Any medical record that has no associated measurements is lost here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting categorical columns (sex, hunger, satiety, emotional_eating) to 0/1 integers...\n",
      "Medical records table completed with sex and baseline/final weight data. \n",
      "There are 1678 records available from 1664 patients.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Complete Medical records by adding sex, baseline/final weight data and genomics sample IDs to it. \n",
    "\n",
    "Sex and genomics sample IDs are fetched from the Patients table, based on the patient_id.\n",
    "\n",
    "Baseline and final weight measurements are obtained from the measurements_with_metadata table created in the previous step. \n",
    "The logic is the following: \n",
    "Measurements are grouped by patient and medical record ID, and the first and last measurements of each group are assigned\n",
    "to the medical records table as baseline and final measurements, respectively.\n",
    "Delta weight is calculated as the difference between final and baseline weights, to obtain negative results. \n",
    "\n",
    "Measurement dates are added and it is checked if they are within the medical record creation and closing dates.\n",
    "\n",
    "If a medical record has no measurements linked to it, it is dropped. \n",
    "\n",
    "Additionally, the 'days_between_measurements' column is added to calculate the number of days between the baseline and final measurements.\n",
    "\n",
    "Sex and categorical eating behavior variables are converted to boolean values (0/1 integers) for easier downstream analysis. \n",
    "1 = yes/female, 0 = no/male. Accordingly, the sex column is renamed to sex_f, and eating behavior values are renamed to [value]_yn or [value]_likert.\n",
    "\n",
    "Finally, the columns are reordered to match the desired order.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Connect to the database, load relevant tables\n",
    "conn = sqlite3.connect(os.path.join(paper1_directory, \"emotional_all_notna.sqlite\"))\n",
    "medical_records = pd.read_sql_query(\"SELECT * FROM medical_records_colclean\", conn)\n",
    "patients = pd.read_sql_query(\"SELECT * FROM patients_colclean\", conn)\n",
    "measurements_with_metadata = pd.read_sql_query(\"SELECT * FROM measurements_with_metadata\", conn)\n",
    "\n",
    "# The following functions complete the original medical records data frame with research-relevant variables.\n",
    "# First, add the sex variable to medical_records_complete by merging patients' sex into medical_records_complete based on patient_id\n",
    "\"\"\"\n",
    "Adding sex data and genomics sample IDs to medical records\n",
    "\"\"\"\n",
    "medical_records_complete = pd.merge(\n",
    "    medical_records,\n",
    "    patients[['patient_id', 'sex', 'genomics_sample_id']],\n",
    "    on='patient_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# After, add baseline and final measurements to medical_records_complete\n",
    "# Treat measurements coming from a given medical record as units\n",
    "# by grouping measurements_with_metadata by patient_id and medical_record_id \n",
    "\"\"\"\n",
    "Adding weight data to medical records\n",
    "\"\"\"\n",
    "grouped_measurements = measurements_with_metadata.groupby(['patient_id', 'medical_record_id'])\n",
    "# Extract the first (baseline) and last (final) measurement for each group\n",
    "baseline = grouped_measurements.first().reset_index()\n",
    "final = grouped_measurements.last().reset_index()\n",
    "# Insert baseline and final measurements into medical_records_complete\n",
    "medical_records_complete = pd.merge(\n",
    "    medical_records_complete,\n",
    "    baseline[['patient_id', 'medical_record_id', 'measurement_date', 'weight_kg', 'bmi']],\n",
    "    on=['patient_id', 'medical_record_id'],\n",
    "    how='left'\n",
    ")\n",
    "medical_records_complete = pd.merge(\n",
    "    medical_records_complete,\n",
    "    final[['patient_id', 'medical_record_id', 'measurement_date', 'weight_kg', 'bmi']],\n",
    "    on=['patient_id', 'medical_record_id'],\n",
    "    how='left',\n",
    "    suffixes=('_baseline', '_final')\n",
    ")\n",
    "# Make sure all dates are in datetime format for further operations, \n",
    "# and calculate delta weight and delta BMI values (final - baseline, so the resulting weight loss value is negative)\n",
    "medical_records_complete['medical_record_creation_date'] = pd.to_datetime(medical_records_complete['medical_record_creation_date'])\n",
    "medical_records_complete['medical_record_closing_date'] = pd.to_datetime(medical_records_complete['medical_record_closing_date'])\n",
    "medical_records_complete['measurement_date_baseline'] = pd.to_datetime(medical_records_complete['measurement_date_baseline'])\n",
    "medical_records_complete['measurement_date_final'] = pd.to_datetime(medical_records_complete['measurement_date_final'])\n",
    "medical_records_complete['delta_weight_kg'] = medical_records_complete['weight_kg_final'] - medical_records_complete['weight_kg_baseline']\n",
    "medical_records_complete['delta_bmi'] = medical_records_complete['bmi_final'] - medical_records_complete['bmi_baseline']\n",
    "# Check if the baseline and final measurements are close to the starting/closing date of the medical record they belong to or not (within a 10-day window). \n",
    "# In some cases, the first measurement is recorded weeks after opening the medical record, or the last one is taken long before closing it. \n",
    "# In other cases, the medical record's closing date is absent, if this happens, the last measurement will be considered as out of range. \n",
    "# This is supposed to help identify cases where the followup has some imperfections. \n",
    "window_days = 10\n",
    "medical_records_complete['baseline_measurement_inrange'] = (\n",
    "    (medical_records_complete['measurement_date_baseline'] >= \n",
    "     medical_records_complete['medical_record_creation_date'] - pd.Timedelta(days=window_days)) &\n",
    "    (medical_records_complete['measurement_date_baseline'] <=\n",
    "     medical_records_complete['measurement_date_baseline'] + pd.Timedelta(days=window_days))\n",
    ")\n",
    "medical_records_complete['final_measurement_inrange'] = (\n",
    "    (medical_records_complete['measurement_date_final'] >= \n",
    "     medical_records_complete['medical_record_closing_date'] - pd.Timedelta(days=window_days)) &\n",
    "    (medical_records_complete['measurement_date_final'] <= \n",
    "     medical_records_complete['medical_record_closing_date'] + pd.Timedelta(days=window_days))\n",
    ")\n",
    "# Add a column that calculates the days passed between baseline and final measurements\n",
    "# This also helps identify cases where the medical record's duration and the actual followup time are very different\n",
    "medical_records_complete['days_between_measurements'] = (\n",
    "    (medical_records_complete['measurement_date_final'] - medical_records_complete['measurement_date_baseline']).dt.days\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Convert categorical columns (sex, hunger, satiety, emotional_eating) \n",
    "to boolean values (0/1 integers) for downstream calculations. \n",
    "In case of gender, female = 0, and the column name is renamed accordingly (sex_f). \n",
    "In case of eating behavior datapoints, yes = 1, no = 0.\n",
    "\"\"\"\n",
    "print(\"Converting categorical columns (sex, hunger, satiety, emotional_eating) to 0/1 integers...\")\n",
    "# Define mappings\n",
    "sex_map = {'female': 1, 'male': 0}\n",
    "yes_no_map = {'yes': 1, 'no': 0}\n",
    "\n",
    "# Apply mappings - use .get() on map to handle potential unexpected values gracefully (assigns NaN)\n",
    "if 'sex' in medical_records_complete.columns:\n",
    "    original_sex_nan_count = medical_records_complete['sex'].isnull().sum()\n",
    "    medical_records_complete['sex'] = medical_records_complete['sex'].map(sex_map)\n",
    "    new_sex_nan_count = medical_records_complete['sex'].isnull().sum()\n",
    "    if new_sex_nan_count > original_sex_nan_count:\n",
    "        print(\"  Warning: Mapping 'sex' introduced NaNs. Check source data for values other than 'Female'/'Male'.\")\n",
    "    \n",
    "# \"\"\"CHECK MAYBE DELETE\"\"\"\n",
    "    \n",
    "    # # Rename sex column AFTER conversion\n",
    "    # medical_records_complete = medical_records_complete.rename(columns={'sex': 'sex_f'})\n",
    "    # print(\"  Renamed 'sex' column to 'sex_f'.\")\n",
    "\n",
    "\n",
    "for col in ['hunger', 'satiety', 'emotional_eating']:\n",
    "    if col in medical_records_complete.columns:\n",
    "        original_nan_count = medical_records_complete[col].isnull().sum()\n",
    "        # Ensure column is string before mapping, handle potential non-string values\n",
    "        medical_records_complete[col] = medical_records_complete[col].astype(str).map(yes_no_map)\n",
    "        new_nan_count = medical_records_complete[col].isnull().sum()\n",
    "        if new_nan_count > original_nan_count:\n",
    "             print(f\"  Warning: Mapping '{col}' introduced NaNs. Check source data for values other than 'Yes'/'No'.\")\n",
    "\n",
    "\"\"\"\n",
    "Removing medical records with no associated measurements\n",
    "\"\"\"\n",
    "# As for some reason (unidentified as of 16Apr25) many medical records have no available measurements associated to them, \n",
    "# any such instances are dropped from the data frame. \n",
    "medical_records_complete = medical_records_complete.dropna(subset=['weight_kg_baseline', 'weight_kg_final'])\n",
    "\n",
    "\"\"\"\n",
    "Presenting and saving the output\n",
    "\"\"\"\n",
    "# Rename and reorder columns for better clarity and interpretability\n",
    "medical_records_complete = medical_records_complete.rename(columns={\n",
    "    'measurement_date_baseline': 'baseline_measurement_date',\n",
    "    'measurement_date_final': 'final_measurement_date',\n",
    "    'weight_kg_baseline': 'baseline_weight_kg',\n",
    "    'weight_kg_final': 'final_weight_kg', \n",
    "    'bmi_baseline': 'baseline_bmi',\n",
    "    'bmi_final': 'final_bmi',\n",
    "    'sex': 'sex_f', \n",
    "    'hunger': 'hunger_yn',\n",
    "    'satiety': 'satiety_yn',\n",
    "    'emotional_eating': 'emotional_eating_yn',\n",
    "    'emotional_eating_value': 'emotional_eating_value_likert',\n",
    "    'quantity_control': 'quantity_control_likert',\n",
    "    'impulse_control': 'impulse_control_likert',\n",
    "})\n",
    "desired_column_order = [\n",
    "    'patient_id',\n",
    "    'medical_record_id',\n",
    "    'genomics_sample_id',\n",
    "    'medical_record_creation_date',\n",
    "    'medical_record_closing_date',\n",
    "    'intervention_duration_days',\n",
    "    'baseline_measurement_date',\n",
    "    'final_measurement_date',\n",
    "    'days_between_measurements',\n",
    "    'baseline_measurement_inrange',\n",
    "    'final_measurement_inrange',\n",
    "    'birth_date',\n",
    "    'age',\n",
    "    'age_when_creating_record',\n",
    "    'sex_f',\n",
    "    'height_m',\n",
    "    'baseline_weight_kg',\n",
    "    'final_weight_kg',\n",
    "    'delta_weight_kg',\n",
    "    'baseline_bmi',\n",
    "    'final_bmi',\n",
    "    'delta_bmi',\n",
    "    'wc_cm_confirm_time',\n",
    "    'pnk_method',\n",
    "    'orders_in_medical_record',\n",
    "    'dietitian_visits',\n",
    "    'physical_activity',\n",
    "    'physical_activity_frequency',\n",
    "    'physical_inactivity_cause',\n",
    "    'weight_gain_cause',\n",
    "    'smoking',\n",
    "    'medications',\n",
    "    'hunger_yn',\n",
    "    'satiety_yn',\n",
    "    'emotional_eating_yn',\n",
    "    'emotional_eating_value_likert',\n",
    "    'quantity_control_likert',\n",
    "    'impulse_control_likert'\n",
    "]\n",
    "medical_records_complete = medical_records_complete[desired_column_order]\n",
    "# Save the complete medical records to the SQL database, and print a summary statement\n",
    "medical_records_complete.to_sql(\"medical_records_complete\", conn, if_exists=\"replace\", index=False)\n",
    "print(f\"Medical records table completed with sex and baseline/final weight data. \\n\" \n",
    "      f\"There are {len(medical_records_complete)} records available from {medical_records_complete['patient_id'].nunique()} patients.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the base input for survival analysis - v0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, data frames specifically prepared for survival analysis are created. Time-to-event (days) of achieving 3 different weight loss targets (5-10-15%) in 3 different time frames (40-60-80 days) is analyzed. Relevant demographic, anthropometric and eating behavior variables are added to each analyzed medical record. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Starting Survival Analysis Script ==========\n",
      "Connecting to input database: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\emotional_all_notna.sqlite\n",
      "Connecting to output database: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\survival_analysis.sqlite\n",
      "--- Processing: sa_40d_5p ---\n",
      "--- Processing: sa_40d_10p ---\n",
      "--- Processing: sa_40d_15p ---\n",
      "--- Processing: sa_60d_5p ---\n",
      "--- Processing: sa_60d_10p ---\n",
      "--- Processing: sa_60d_15p ---\n",
      "--- Processing: sa_80d_5p ---\n",
      "--- Processing: sa_80d_10p ---\n",
      "--- Processing: sa_80d_15p ---\n",
      "--- Saving results to output database: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\survival_analysis.sqlite ---\n",
      "Saving table: sa_40d_5p (1664 rows)\n",
      "Saving table: sa_40d_10p (1664 rows)\n",
      "Saving table: sa_40d_15p (1664 rows)\n",
      "Saving table: sa_60d_5p (1664 rows)\n",
      "Saving table: sa_60d_10p (1664 rows)\n",
      "Saving table: sa_60d_15p (1664 rows)\n",
      "Saving table: sa_80d_5p (1664 rows)\n",
      "Saving table: sa_80d_10p (1664 rows)\n",
      "Saving table: sa_80d_15p (1664 rows)\n",
      "Saving summary table: survival_analysis_summary (9 rows)\n",
      "--- All results saved successfully ---\n",
      "\n",
      "--- Survival Analysis Summary ---\n",
      "  analysis_name  weight_loss_target  time_window  total_patients  achieved_target  dropout_count  avg_weight_loss_pct\n",
      "0     sa_40d_5p                   5           40            1664              987            584             4.211154\n",
      "1    sa_40d_10p                  10           40            1664              328            784             5.682572\n",
      "2    sa_40d_15p                  15           40            1664               24            789             5.747578\n",
      "3     sa_60d_5p                   5           60            1664             1019            611             4.272037\n",
      "4    sa_60d_10p                  10           60            1664              452            989             6.097951\n",
      "5    sa_60d_15p                  15           60            1664               83           1069             6.539477\n",
      "6     sa_80d_5p                   5           80            1664             1027            625             4.289597\n",
      "7    sa_80d_10p                  10           80            1664              506           1061             6.214567\n",
      "8    sa_80d_15p                  15           80            1664              138           1226             6.895883\n",
      "--- End Summary ---\n",
      "Analysis data successfully generated and saved to C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\survival_analysis.sqlite\n",
      "Closing database connections...\n",
      "========== Survival Analysis Script Finished ==========\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sqlite3\n",
    "from datetime import timedelta\n",
    "# Removed: import logging\n",
    "\n",
    "\"\"\"\n",
    "CONFIGURATION\n",
    "\"\"\"\n",
    "# Define directories and database paths - paper1_directory should be defined \n",
    "# in the first cell of this notebook chapter\n",
    "input_db_path = os.path.join(paper1_directory, 'emotional_all_notna.sqlite')\n",
    "input_measurements = \"measurements_with_metadata\"\n",
    "input_medical_records = \"medical_records_complete\"\n",
    "output_db_path = os.path.join(paper1_directory, 'survival_analysis.sqlite')\n",
    "\n",
    "# Define analysis parameters\n",
    "weight_loss_targets = [5, 10, 15]     # Weight loss target percentages\n",
    "time_windows = [40, 60, 80]       # Time windows (centers) in days\n",
    "window_span = 10                   # Permissible span around windows (+/- days)\n",
    "\n",
    "# Define the variables stored in medical_records_complete that are relevant for the analysis. \n",
    "# These include basic metadata like patient and record ID,\n",
    "# basic factors such as age and sex, \n",
    "# as well as the emotional and eating behavior variables pivotal to the research question. \n",
    "# The list can be amended on demand - \n",
    "# for example, right now it does not include medical record creating and closing dates. \n",
    "relevant_medical_values = ['patient_id', 'medical_record_id', 'sex', 'age',\n",
    "                             'height_m', 'baseline_bmi', 'hunger', 'satiety', 'emotional_eating',\n",
    "                             'emotional_eating_value', 'quantity_control', 'impulse_control']\n",
    "\n",
    "\"\"\"\n",
    "DATA LOADING & PREPARATION\n",
    "\"\"\"\n",
    "\n",
    "def load_measurements(connection):\n",
    "    \"\"\"\n",
    "    Load measurements from the measurement_with_metadata table; \n",
    "    make sure key values are in the correct format. \n",
    "    \"\"\"\n",
    "    query = f\"SELECT * FROM {input_measurements}\"\n",
    "    measurements = pd.read_sql_query(query, connection)\n",
    "    measurements['measurement_date'] = pd.to_datetime(measurements['measurement_date'], errors='coerce')\n",
    "    measurements['weight_kg'] = pd.to_numeric(measurements['weight_kg'], errors='coerce')\n",
    "    return measurements\n",
    "\n",
    "def load_medical_records(connection):\n",
    "    \"\"\"\n",
    "    Load medical records from the medical_records_complete table;\n",
    "    make sure date values are in datetime format. \n",
    "    The exact columns to be used are defined in the prepare_patient_data function.\n",
    "    \"\"\"\n",
    "    query = f\"SELECT * FROM {input_medical_records}\"\n",
    "    medical_records = pd.read_sql_query(query, connection)\n",
    "    medical_records['medical_record_creation_date'] = pd.to_datetime(medical_records['medical_record_creation_date'], errors='coerce')\n",
    "    return medical_records\n",
    "\n",
    "def prepare_patient_data(measurements, medical_records):\n",
    "    \"\"\"\n",
    "    Filter measurements to only include those from the earliest medical record for each patient.\n",
    "    Merge measurements with relevant medical record data, including the pivotal eating behavior scores. \n",
    "    \"\"\"\n",
    "    # Filter measurements to only include those from the first treatment record of each patient\n",
    "    earliest_records_with_data = measurements.sort_values('measurement_date')\\\n",
    "        .groupby('patient_id')['medical_record_id']\\\n",
    "        .first()\\\n",
    "        .reset_index()\n",
    "    filtered_measurements = pd.merge(\n",
    "        measurements,\n",
    "        earliest_records_with_data,\n",
    "        on=['patient_id', 'medical_record_id'],\n",
    "        how='inner'\n",
    "    )\n",
    "    # Identify the baseline measurement in each record\n",
    "    baseline_data = filtered_measurements.sort_values('measurement_date')\\\n",
    "                                       .groupby(['patient_id', 'medical_record_id'])\\\n",
    "                                       .first()\\\n",
    "                                       .reset_index()\n",
    "\n",
    "    cols_to_select = [col for col in relevant_medical_values if col in medical_records.columns]\n",
    "    medical_record_data = medical_records[cols_to_select]\n",
    "    # Merge baseline measurements with relevant medical record data\n",
    "    prepared_data = pd.merge(\n",
    "        baseline_data,\n",
    "        medical_record_data,\n",
    "        on=['patient_id', 'medical_record_id'],\n",
    "        how='left' # Keep all baseline data\n",
    "    )\n",
    "    return prepared_data, filtered_measurements\n",
    "\n",
    "\"\"\"\n",
    "CALCULATE WEIGHT LOSS OUTCOMES\n",
    "\"\"\"\n",
    "\n",
    "def _get_patient_baseline(patient_data, patient_id, medical_record_id):\n",
    "    \"\"\"\n",
    "    Get the baseline data for each patient's corresponding medical record.\n",
    "    \"\"\"\n",
    "    patient_baseline = patient_data[\n",
    "        (patient_data['patient_id'] == patient_id) &\n",
    "        (patient_data['medical_record_id'] == medical_record_id)\n",
    "    ]\n",
    "    if len(patient_baseline) == 0:\n",
    "        print(f\"WARN: No baseline data found for patient {patient_id}, record {medical_record_id}. Skipping.\")\n",
    "        return None\n",
    "    return patient_baseline.iloc[0]\n",
    "\n",
    "def _check_target_achievement(measurements_within_window, baseline_weight, weight_loss_target):\n",
    "    \"\"\"\n",
    "    Check if the weight loss target was achieved in some of the given measurements.\n",
    "    \"\"\"\n",
    "    # Set default to False/None\n",
    "    target_achieved = False\n",
    "    first_success_measurement = None\n",
    "    # Calculate weight loss percentage for each measurement in the window, \n",
    "    # and check if it meets the target\n",
    "    for _, row in measurements_within_window.iterrows():\n",
    "        current_weight = row['weight_kg']\n",
    "        if baseline_weight is not None and baseline_weight > 0:\n",
    "            current_weight_loss = ((baseline_weight - current_weight) / baseline_weight) * 100\n",
    "            if round(current_weight_loss, 2) >= weight_loss_target:\n",
    "                target_achieved = True\n",
    "                first_success_measurement = row\n",
    "                break # Stop at the first success; if that is not identified, target_achieved remains False as by default\n",
    "    return target_achieved, first_success_measurement\n",
    "\n",
    "def _determine_final_measurement(target_achieved, first_success_row, measurements_around_cutoff,\n",
    "                                measurements_within_window, baseline_date, window_center):\n",
    "    \"\"\"\n",
    "    Determine the final measurement based on success or censoring (ie. completion without success) rules.\n",
    "    \"\"\"\n",
    "    # Set final measurement to None by default\n",
    "    final_measurement = None\n",
    "    # Set target final date based on the given time window\n",
    "    target_date = baseline_date + timedelta(days=window_center)\n",
    "    # If weight loss target was achieved at any point of the followup time window,\n",
    "    # use the first success measurement as the final measurement.  \n",
    "    if target_achieved:\n",
    "        final_measurement = first_success_row\n",
    "    # In case of no success, the date closest to the target date is used as the final measurement. \n",
    "    elif not measurements_around_cutoff.empty:\n",
    "        measurements_around_cutoff = measurements_around_cutoff.copy()\n",
    "        measurements_around_cutoff['distance_to_center'] = abs(\n",
    "            (measurements_around_cutoff['measurement_date'] - target_date).dt.days\n",
    "        )\n",
    "        closest_measurement_idx = measurements_around_cutoff['distance_to_center'].idxmin()\n",
    "        final_measurement = measurements_around_cutoff.loc[closest_measurement_idx]\n",
    "    # In case of no success nor completion (delayed dropout), use the last available measurement as the final measurement\n",
    "    elif not measurements_within_window.empty:\n",
    "        final_measurement = measurements_within_window.sort_values('measurement_date').iloc[-1]\n",
    "    # Else: Instant dropout, final_measurement remains None, \n",
    "    # and is set to the baseline measurementin the calculate_outcome_metrics function.\n",
    "    return final_measurement\n",
    "\n",
    "def _calculate_outcome_metrics(baseline_row, final_measurement_row):\n",
    "    \"\"\"\n",
    "    Calculate follow-up lenght and weight loss percentage based on baseline and final measurement.\n",
    "    \"\"\"\n",
    "    # Identify the baseline measurement\n",
    "    baseline_date = baseline_row['measurement_date']\n",
    "    baseline_weight = baseline_row['weight_kg']\n",
    "    # In patients that have at least one followup measurement, identify the end date and final weight, \n",
    "    # to calculate followup length and weight loss in kg and %\n",
    "    if final_measurement_row is not None:\n",
    "        end_date = final_measurement_row['measurement_date']\n",
    "        final_weight = final_measurement_row['weight_kg']\n",
    "        followup_period = (end_date - baseline_date).days\n",
    "        weight_loss_kg = baseline_weight - final_weight\n",
    "        weight_loss_pct = ((baseline_weight - final_weight) / baseline_weight) * 100\n",
    "    # In patients that have no followup measurement (instant dropouts), \n",
    "    # the end date and final weight are set to the baseline values, \n",
    "    # and followup length and weight loss are set to 0. \n",
    "    else: \n",
    "        end_date = baseline_date\n",
    "        final_weight = baseline_weight\n",
    "        followup_period = 0\n",
    "        weight_loss_kg = 0\n",
    "        weight_loss_pct = 0\n",
    "    return {\n",
    "        'end_date': end_date,\n",
    "        'final_weight': final_weight,\n",
    "        'followup_period': followup_period,\n",
    "        'weight_loss_kg': weight_loss_kg,\n",
    "        'weight_loss_pct': round(weight_loss_pct, 2)\n",
    "    }\n",
    "\n",
    "\"\"\"\n",
    "CORE ANALYSIS FUNCTION\n",
    "\"\"\"\n",
    "\n",
    "def calculate_weight_loss_outcome(patient_data, filtered_measurements, weight_loss_target, window_center, window_span):\n",
    "    \"\"\"\n",
    "    Calculate weight loss outcomes for each patient in a survival analysis-ready format. \n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store results, and group measurements by patient and medical record ID\n",
    "    results = []\n",
    "    grouped_measurements = filtered_measurements.groupby(['patient_id', 'medical_record_id'])\n",
    "    # Iterate through each group within measurements. \n",
    "    for (patient_id, medical_record_id), group in grouped_measurements:\n",
    "        # 1. Identify baseline measurement date and weight\n",
    "        baseline_row = _get_patient_baseline(patient_data, patient_id, medical_record_id)\n",
    "        if baseline_row is None: continue\n",
    "        baseline_date = baseline_row['measurement_date']\n",
    "        baseline_weight = baseline_row['weight_kg']\n",
    "        # 2. Define observation time windows and group measurements within the defined window\n",
    "        # Calculations are done for both the complete observation period, \n",
    "        # as well as the period strictry around the cutoff date, within the defined permissivity window. \n",
    "        min_window_date = baseline_date + timedelta(days=(window_center - window_span))\n",
    "        max_window_date = baseline_date + timedelta(days=(window_center + window_span))\n",
    "        measurements_within_window = group[\n",
    "            (group['measurement_date'] > baseline_date) &\n",
    "            (group['measurement_date'] <= max_window_date)\n",
    "        ].sort_values('measurement_date')\n",
    "        measurements_around_cutoff = group[\n",
    "            (group['measurement_date'] >= min_window_date) &\n",
    "            (group['measurement_date'] <= max_window_date)\n",
    "        ]\n",
    "        # 3. Check whether target weight loss was achieved in the defined time window\n",
    "        target_achieved, first_success_row = _check_target_achievement(\n",
    "            measurements_within_window, baseline_weight, weight_loss_target\n",
    "        )\n",
    "        # 4. Identify the last measurement date within the time window,\n",
    "        # whether based on target achievment or followup completion\n",
    "        final_measurement_row = _determine_final_measurement(\n",
    "            target_achieved, first_success_row, measurements_around_cutoff,\n",
    "            measurements_within_window, baseline_date, window_center\n",
    "        )\n",
    "        # 5. Check for dropout status - instant dropouts are those who have no second measurement, \n",
    "        # while delayed dropouts are those who have not reached target, \n",
    "        # and their final measurement is before the cutoff window. \n",
    "        is_instant_dropout = final_measurement_row is None\n",
    "        is_delayed_dropout = (not target_achieved and\n",
    "                              final_measurement_row is not None and\n",
    "                              final_measurement_row['measurement_date'] < min_window_date)\n",
    "        dropout = is_instant_dropout or is_delayed_dropout\n",
    "        success = target_achieved\n",
    "        # 6. Calculate metrics like final date and weight, followup length and weight lost. \n",
    "        outcome_metrics = _calculate_outcome_metrics(baseline_row, final_measurement_row)\n",
    "\n",
    "        \"\"\"ARE WE GOING TO MODIFY AVG CALCS?\"\"\"\n",
    "\n",
    "        # # 7. \n",
    "        # # --- NEW: Calculate metrics based *always* on the last measurement within the window ---\n",
    "        # actual_last_measurement_row = None\n",
    "        # if not measurements_within_window.empty:\n",
    "        #     actual_last_measurement_row = measurements_within_window.iloc[-1]\n",
    "\n",
    "        # # Use the same helper, but pass the actual last measurement row\n",
    "        # actual_end_metrics = _calculate_outcome_metrics(baseline_row, actual_last_measurement_row)\n",
    "        # actual_wl_pct_at_window_end = actual_end_metrics['weight_loss_pct']\n",
    "        # # --- End NEW ---\n",
    "\n",
    "\n",
    "\n",
    "        # 8. Assemble the result - this is where the output tables' columns are defined. \n",
    "        # If additional variables are inserted at an earlier part of the code, \n",
    "        # they need to be mentioned here as well. \n",
    "        result = {\n",
    "            'patient_id': patient_id,\n",
    "            'medical_record_id': medical_record_id,\n",
    "            'baseline_date': baseline_date,\n",
    "            'end_date': outcome_metrics['end_date'],\n",
    "            'followup_period': outcome_metrics['followup_period'],\n",
    "            'baseline_weight': baseline_weight,\n",
    "            'final_weight': outcome_metrics['final_weight'],\n",
    "            'weight_loss_kg': outcome_metrics['weight_loss_kg'],\n",
    "            'weight_loss_pct': outcome_metrics['weight_loss_pct'],\n",
    "            # \n",
    "            f'{weight_loss_target}pct_achieved': success,\n",
    "            'dropout': dropout,\n",
    "            # Add baseline characteristics safely using .get()\n",
    "            'sex': baseline_row.get('sex'),\n",
    "            'age': baseline_row.get('age'),\n",
    "            'height_m': baseline_row.get('height_m'),\n",
    "            'baseline_bmi': baseline_row.get('bmi'),\n",
    "            'hunger': baseline_row.get('hunger'),\n",
    "            'satiety': baseline_row.get('satiety'),\n",
    "            'emotional_eating': baseline_row.get('emotional_eating'),\n",
    "            'emotional_eating_value': baseline_row.get('emotional_eating_value'),\n",
    "            'quantity_control': baseline_row.get('quantity_control'),\n",
    "            'impulse_control': baseline_row.get('impulse_control')\n",
    "        }\n",
    "        results.append(result)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "MAIN ORCHESTRATION FUNCTION\n",
    "\"\"\"\n",
    "\n",
    "def generate_survival_analysis_datasets(input_connection, output_connection, weight_loss_targets, time_windows, window_span=10):\n",
    "    \"\"\"\n",
    "    The main function to orchestrate the survival analysis process, calling all previously defined functions in an organized manner. \n",
    "    Generate survival analysis datasets for multiple weight loss targets and observation time windows.\n",
    "    Targets and timeframes are defined in the configuration section at the beginning of the code module.\n",
    "    Save data to a separate SQLite database. \n",
    "    \"\"\"\n",
    "    # 1. Load and prepare input data\n",
    "    measurements = load_measurements(input_connection)\n",
    "    medical_records = load_medical_records(input_connection)\n",
    "    patient_data, filtered_measurements = prepare_patient_data(measurements, medical_records)\n",
    "    if patient_data.empty:\n",
    "        print(\"ERROR: Prepared patient data is empty. Cannot proceed.\")\n",
    "        return {}, pd.DataFrame()\n",
    "    # 2. Calculate weight loss outcomes for each target-timeframe combination. \n",
    "    # Targets and timeframes are defined in the config section of the script. \n",
    "    # Initialize a results dictionary and a list for summary statistics. \n",
    "    results = {}\n",
    "    summary_list = []\n",
    "    for window in sorted(time_windows):\n",
    "        for target in sorted(weight_loss_targets):\n",
    "            # Name each instance accordingly, where sa stands for survival analysis, \n",
    "            # and the numbers indicate the time window and target percentage.\n",
    "            name = f\"sa_{window}d_{target}p\"\n",
    "            print(f\"--- Processing: {name} ---\") # Minimal progress indication\n",
    "            result_df = calculate_weight_loss_outcome(\n",
    "                patient_data,\n",
    "                filtered_measurements,\n",
    "                target,\n",
    "                window,\n",
    "                window_span # Defined in config - the permissivity window around the followup cutoff time\n",
    "            )\n",
    "            results[name] = result_df\n",
    "            # Add the calculated instances to the summary statistics list. \n",
    "            if not result_df.empty:\n",
    "                summary_row = {\n",
    "                    'analysis_name': name,\n",
    "                    'weight_loss_target': target,\n",
    "                    'time_window': window,\n",
    "                    'total_patients': len(result_df),\n",
    "                    'achieved_target': int(result_df[f'{target}pct_achieved'].sum()),\n",
    "                    'dropout_count': int(result_df['dropout'].sum()),\n",
    "                    'avg_weight_loss_pct': result_df['weight_loss_pct'].mean() if not result_df['weight_loss_pct'].isnull().all() else 0\n",
    "                }\n",
    "                summary_list.append(summary_row)\n",
    "            else:\n",
    "                 print(f\"WARN: No results generated for {name}. Skipping summary entry.\")\n",
    "    # Turn the summary statistics list into a data frame\n",
    "    summary = pd.DataFrame(summary_list)\n",
    "\n",
    "    # 3. Save the analysis results (9 tables by default) to the SQLite database defined in the config section\n",
    "    print(f\"--- Saving results to output database: {output_db_path} ---\")\n",
    "    # Save individual tables\n",
    "    for name, df in results.items():\n",
    "        print(f\"Saving table: {name} ({len(df)} rows)\")\n",
    "        df.to_sql(name, output_connection, if_exists='replace', index=False)\n",
    "    # Save the summary stats table in the database as well\n",
    "    print(f\"Saving summary table: survival_analysis_summary ({len(summary)} rows)\")\n",
    "    summary.to_sql('survival_analysis_summary', output_connection, if_exists='replace', index=False)\n",
    "    output_connection.commit() # Ensure changes are saved\n",
    "    print(\"--- All results saved successfully ---\")\n",
    "    return results, summary\n",
    "\n",
    "\"\"\"\n",
    "EXECUTION BLOCK\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "This part of the code calls all the functions and executes the code. \n",
    "Currently it has a lot of debug messages and error handling, which might be an overkill, \n",
    "but overall, it should not affect transparency of the code.\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"========== Starting Survival Analysis Script ==========\")\n",
    "    # By default, connections are set to None, and will be established in the try block.\n",
    "    input_conn = None\n",
    "    output_conn = None\n",
    "    try:\n",
    "        # Connect to in-and output databases\n",
    "        print(f\"Connecting to input database: {input_db_path}\")\n",
    "        if not os.path.exists(input_db_path):\n",
    "             raise FileNotFoundError(f\"Input database not found at {input_db_path}\")\n",
    "        input_conn = sqlite3.connect(input_db_path)\n",
    "        print(f\"Connecting to output database: {output_db_path}\")\n",
    "        output_conn = sqlite3.connect(output_db_path)\n",
    "        # Run the main analysis function\n",
    "        results, summary = generate_survival_analysis_datasets(\n",
    "            input_conn,\n",
    "            output_conn,\n",
    "            weight_loss_targets,\n",
    "            time_windows,\n",
    "            window_span\n",
    "        )\n",
    "        # Display summary if successful\n",
    "        if not summary.empty:\n",
    "            print(\"\\n--- Survival Analysis Summary ---\")\n",
    "            print(summary.to_string()) # Use print for console display\n",
    "            print(\"--- End Summary ---\")\n",
    "        else:\n",
    "            print(\"WARN: Analysis completed, but the summary table is empty.\")\n",
    "        print(f\"Analysis data successfully generated and saved to {output_db_path}\")\n",
    "\n",
    "    # Minimal error handling for critical failures\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"ERROR: Database file not found - {e}\")\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"ERROR: SQLite database error - {e}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"ERROR: Data processing error - {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: An unexpected error occurred - {e}\")\n",
    "        # Consider adding traceback for debugging complex errors:\n",
    "        # import traceback\n",
    "        # print(traceback.format_exc())\n",
    "    finally:\n",
    "        # Ensure connections are closed\n",
    "        print(\"Closing database connections...\")\n",
    "        if input_conn:\n",
    "            input_conn.close()\n",
    "        if output_conn:\n",
    "            output_conn.close()\n",
    "        print(\"========== Survival Analysis Script Finished ==========\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconsidered input structure for SA 18-19 Apr v1 - this is still not perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Starting Wide Survival Analysis Script ==========\n",
      "Connecting to input database: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\emotional_all_notna.sqlite\n",
      "Connecting to output database: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\shukishukishuuu.sqlite\n",
      "Loading measurements...\n",
      "Loading medical records...\n",
      "Preparing patient data...\n",
      "Calculating wide outcomes for all patients...\n",
      "--- Saving results to output database: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\shukishukishuuu.sqlite ---\n",
      "Saving table: shukishukishuuu (1664 rows)\n",
      "--- Wide table saved successfully ---\n",
      "\n",
      "--- Wide Analysis Table Generation Summary ---\n",
      "Generated table 'shukishukishuuu' with 1664 rows and 49 columns.\n",
      "--- End Summary ---\n",
      "Analysis data successfully generated and saved to C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\shukishukishuuu.sqlite\n",
      "Closing database connections...\n",
      "========== Wide Survival Analysis Script Finished ==========\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sqlite3\n",
    "from datetime import timedelta\n",
    "import numpy as np # Import numpy for NaN\n",
    "\n",
    "\"\"\"\n",
    "CONFIGURATION\n",
    "\"\"\"\n",
    "# Define directories and database paths - paper1_directory should be defined\n",
    "# in the first cell of this notebook chapter\n",
    "input_db_path = os.path.join(paper1_directory, 'emotional_all_notna.sqlite')\n",
    "input_measurements = \"measurements_with_metadata\"\n",
    "input_medical_records = \"medical_records_complete\"\n",
    "# Modified: Output database path remains the same, but will contain a different table structure\n",
    "output_db_path = os.path.join(paper1_directory, 'shukishukishuuu.sqlite')\n",
    "output_table_name = \"shukishukishuuu\" # Define the name for the single wide table\n",
    "\n",
    "# Define analysis parameters\n",
    "weight_loss_targets = [5, 10, 15]     # Weight loss target percentages\n",
    "time_windows = [40, 60, 80]       # Time windows (centers) in days\n",
    "window_span = 10                   # Permissible span around windows (+/- days)\n",
    "\n",
    "# Define the variables stored in medical_records_complete that are relevant for the analysis.\n",
    "# These include basic metadata like patient and record ID,\n",
    "# basic factors such as age and sex,\n",
    "# as well as the emotional and eating behavior variables pivotal to the research question.\n",
    "# The list can be amended on demand -\n",
    "# for example, right now it does not include medical record creating and closing dates.\n",
    "relevant_medical_values = ['patient_id', 'medical_record_id', 'sex', 'age',\n",
    "                             'height_m', 'baseline_bmi', 'hunger', 'satiety', 'emotional_eating',\n",
    "                             'emotional_eating_value', 'quantity_control', 'impulse_control', 'genomics_sample_id']\n",
    "\n",
    "\"\"\"\n",
    "DATA LOADING & PREPARATION\n",
    "\"\"\"\n",
    "\n",
    "def load_measurements(connection):\n",
    "    \"\"\"\n",
    "    Load measurements from the measurement_with_metadata table;\n",
    "    make sure key values are in the correct format.\n",
    "    \"\"\"\n",
    "    query = f\"SELECT * FROM {input_measurements}\"\n",
    "    measurements = pd.read_sql_query(query, connection)\n",
    "    measurements['measurement_date'] = pd.to_datetime(measurements['measurement_date'], errors='coerce')\n",
    "    measurements['weight_kg'] = pd.to_numeric(measurements['weight_kg'], errors='coerce')\n",
    "    # Ensure sorting for consistent 'first'/'last' operations later\n",
    "    measurements = measurements.sort_values(['patient_id', 'medical_record_id', 'measurement_date'])\n",
    "    return measurements\n",
    "\n",
    "def load_medical_records(connection):\n",
    "    \"\"\"\n",
    "    Load medical records from the medical_records_complete table;\n",
    "    make sure date values are in datetime format.\n",
    "    The exact columns to be used are defined in the prepare_patient_data function.\n",
    "    \"\"\"\n",
    "    query = f\"SELECT * FROM {input_medical_records}\"\n",
    "    medical_records = pd.read_sql_query(query, connection)\n",
    "    # Ensure relevant date columns are datetime\n",
    "    for col in ['medical_record_creation_date', 'baseline_measurement_date', 'final_measurement_date']:\n",
    "         if col in medical_records.columns:\n",
    "             medical_records[col] = pd.to_datetime(medical_records[col], errors='coerce')\n",
    "    return medical_records\n",
    "\n",
    "def prepare_patient_data(measurements, medical_records):\n",
    "    \"\"\"\n",
    "    Filter measurements to only include those from the earliest medical record for each patient.\n",
    "    Merge measurements with relevant medical record data, including the pivotal eating behavior scores.\n",
    "    \"\"\"\n",
    "    # Filter measurements to only include those from the first treatment record of each patient\n",
    "    # Ensure measurements are sorted before grouping\n",
    "    measurements = measurements.sort_values(['patient_id', 'measurement_date'])\n",
    "    earliest_records_with_data = measurements.groupby('patient_id')['medical_record_id'].first().reset_index()\n",
    "\n",
    "    filtered_measurements = pd.merge(\n",
    "        measurements,\n",
    "        earliest_records_with_data,\n",
    "        on=['patient_id', 'medical_record_id'],\n",
    "        how='inner'\n",
    "    )\n",
    "    # Ensure filtered_measurements are sorted for baseline identification\n",
    "    filtered_measurements = filtered_measurements.sort_values(['patient_id', 'medical_record_id', 'measurement_date'])\n",
    "\n",
    "    # Identify the baseline measurement row for each record (the first measurement in the filtered set)\n",
    "    baseline_data_rows = filtered_measurements.groupby(['patient_id', 'medical_record_id']).first().reset_index()\n",
    "\n",
    "    # Select only relevant columns from medical_records to merge\n",
    "    cols_to_select = [col for col in relevant_medical_values if col in medical_records.columns]\n",
    "    medical_record_subset = medical_records[cols_to_select]\n",
    "\n",
    "    # Merge baseline measurement info with the selected medical record data\n",
    "    # Use baseline_data_rows which contains the actual first measurement details\n",
    "    prepared_data = pd.merge(\n",
    "        baseline_data_rows[['patient_id', 'medical_record_id', 'measurement_date', 'weight_kg']], # Get baseline date/weight from actual first measurement\n",
    "        medical_record_subset,\n",
    "        on=['patient_id', 'medical_record_id'],\n",
    "        how='left' # Keep all baseline measurements\n",
    "    )\n",
    "    # Rename columns for clarity before returning\n",
    "    prepared_data = prepared_data.rename(columns={'measurement_date': 'baseline_date', 'weight_kg': 'baseline_weight_kg'})\n",
    "\n",
    "    # Add baseline_bmi from medical_records if available and not already present from measurement merge\n",
    "    if 'baseline_bmi' in medical_record_subset.columns and 'baseline_bmi' not in prepared_data.columns:\n",
    "         prepared_data = pd.merge(\n",
    "              prepared_data,\n",
    "              medical_record_subset[['patient_id', 'medical_record_id', 'baseline_bmi']],\n",
    "              on=['patient_id', 'medical_record_id'],\n",
    "              how='left'\n",
    "         )\n",
    "\n",
    "    return prepared_data, filtered_measurements\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "MODIFIED: CALCULATE WEIGHT LOSS OUTCOMES FOR WIDE TABLE\n",
    "\"\"\"\n",
    "\n",
    "def _get_patient_baseline(patient_data, patient_id, medical_record_id):\n",
    "    \"\"\"\n",
    "    Get the prepared baseline data for a patient's specific medical record.\n",
    "    (Function remains largely the same, but operates on the prepared_data structure)\n",
    "    \"\"\"\n",
    "    patient_baseline = patient_data[\n",
    "        (patient_data['patient_id'] == patient_id) &\n",
    "        (patient_data['medical_record_id'] == medical_record_id)\n",
    "    ]\n",
    "    if patient_baseline.empty:\n",
    "        print(f\"WARN: No baseline data found for patient {patient_id}, record {medical_record_id}. Skipping.\")\n",
    "        return None\n",
    "    # Ensure we return a Series for consistent access\n",
    "    return patient_baseline.iloc[0]\n",
    "\n",
    "def _calculate_wl_metrics(baseline_weight, current_weight):\n",
    "    \"\"\" Helper to calculate weight loss kg and % \"\"\"\n",
    "    if pd.isna(baseline_weight) or pd.isna(current_weight) or baseline_weight == 0:\n",
    "        return np.nan, np.nan\n",
    "    wl_kg = baseline_weight - current_weight\n",
    "    wl_pct = (wl_kg / baseline_weight) * 100\n",
    "    return wl_kg, round(wl_pct, 2)\n",
    "\n",
    "# Removed _check_target_achievement, _determine_final_measurement, _calculate_outcome_metrics\n",
    "# Their logic will be integrated into the main calculation function.\n",
    "\n",
    "def calculate_wide_patient_outcomes(prepared_patient_data, filtered_measurements, weight_loss_targets, time_windows, window_span):\n",
    "    \"\"\"\n",
    "    Calculate all required outcomes (fixed-time and time-to-event) for each patient\n",
    "    and return a list of dictionaries, each representing a row in the wide table.\n",
    "    \"\"\"\n",
    "    results_list = []\n",
    "    # Group all measurements to only get the relevant (earliest) medical record per patient\n",
    "    grouped_measurements = filtered_measurements.groupby(['patient_id', 'medical_record_id'])\n",
    "\n",
    "    for (patient_id, medical_record_id), group in grouped_measurements:\n",
    "        # 1. Get Baseline Info\n",
    "        baseline_info = _get_patient_baseline(prepared_patient_data, patient_id, medical_record_id)\n",
    "        if baseline_info is None:\n",
    "            continue\n",
    "\n",
    "        baseline_date = baseline_info['baseline_date']\n",
    "        baseline_weight = baseline_info['baseline_weight_kg']\n",
    "\n",
    "        # Initialize result dictionary with baseline info\n",
    "        result = {\n",
    "            'patient_ID': patient_id, \n",
    "            'medical_record_ID': medical_record_id,\n",
    "            'baseline_date': baseline_date,\n",
    "            'baseline_weight_kg': baseline_weight,\n",
    "            # Add other baseline characteristics safely using .get() or direct access\n",
    "            'sex': baseline_info.get('sex'),\n",
    "            'age': baseline_info.get('age'),\n",
    "            'height_m': baseline_info.get('height_m'),\n",
    "            'baseline_bmi': baseline_info.get('baseline_bmi'), # Get BMI from prepared data\n",
    "            'hunger': baseline_info.get('hunger'),\n",
    "            'satiety': baseline_info.get('satiety'),\n",
    "            'emotional_eating': baseline_info.get('emotional_eating'),\n",
    "            'emotional_eating_value': baseline_info.get('emotional_eating_value'),\n",
    "            'quantity_control': baseline_info.get('quantity_control'),\n",
    "            'impulse_control': baseline_info.get('impulse_control')\n",
    "        }\n",
    "\n",
    "        # Get all measurements *after* baseline for this group\n",
    "        followup_measurements = group[group['measurement_date'] > baseline_date].sort_values('measurement_date')\n",
    "\n",
    "        # 2. Calculate Overall Follow-up Metrics\n",
    "        if not followup_measurements.empty:\n",
    "            last_measurement = followup_measurements.iloc[-1]\n",
    "            result['last_aval_date'] = last_measurement['measurement_date']\n",
    "            result['total_followup_days'] = (last_measurement['measurement_date'] - baseline_date).days\n",
    "            result['last_aval_weight_kg'] = last_measurement['weight_kg']\n",
    "            wl_kg, wl_pct = _calculate_wl_metrics(baseline_weight, last_measurement['weight_kg'])\n",
    "            result['total_wl_kg'] = wl_kg\n",
    "            result['total_wl_%'] = wl_pct\n",
    "        else:\n",
    "            # Handle instant dropouts (only baseline measurement exists)\n",
    "            result['last_aval_date'] = baseline_date\n",
    "            result['total_followup_days'] = 0\n",
    "            result['last_aval_weight_kg'] = baseline_weight\n",
    "            result['total_wl_kg'] = 0.0\n",
    "            result['total_wl_%'] = 0.0\n",
    "\n",
    "        # 3. Calculate Fixed-Timepoint Metrics (for each time window)\n",
    "        for window_center in time_windows:\n",
    "            min_window_date = baseline_date + timedelta(days=(window_center - window_span))\n",
    "            max_window_date = baseline_date + timedelta(days=(window_center + window_span))\n",
    "            target_date = baseline_date + timedelta(days=window_center)\n",
    "\n",
    "            # Find measurements around the cutoff window\n",
    "            measurements_around_cutoff = followup_measurements[\n",
    "                (followup_measurements['measurement_date'] >= min_window_date) &\n",
    "                (followup_measurements['measurement_date'] <= max_window_date)\n",
    "            ]\n",
    "\n",
    "            measurement_at_window = None\n",
    "            is_dropout_at_window = True # Assume dropout unless proven otherwise\n",
    "\n",
    "            if not measurements_around_cutoff.empty:\n",
    "                # Find measurement closest to the window center\n",
    "                measurements_around_cutoff = measurements_around_cutoff.copy()\n",
    "                measurements_around_cutoff['distance_to_center'] = abs(\n",
    "                    (measurements_around_cutoff['measurement_date'] - target_date).dt.days\n",
    "                )\n",
    "                closest_measurement_idx = measurements_around_cutoff['distance_to_center'].idxmin()\n",
    "                measurement_at_window = measurements_around_cutoff.loc[closest_measurement_idx]\n",
    "                is_dropout_at_window = False # Measurement found within/around window\n",
    "            elif not followup_measurements.empty:\n",
    "                 # No measurement in cutoff window, check if *any* followup exists before the window\n",
    "                 last_followup_before_window = followup_measurements[followup_measurements['measurement_date'] < min_window_date]\n",
    "                 if not last_followup_before_window.empty:\n",
    "                      # Use the latest measurement before the window started\n",
    "                      measurement_at_window = last_followup_before_window.iloc[-1]\n",
    "                      # Still considered dropout *for this window* as they didn't reach it\n",
    "                      is_dropout_at_window = True\n",
    "                 else:\n",
    "                      \"\"\"!!!CHECK this logic, it might get tricky!!!\"\"\"\n",
    "                      # Followup exists, but only *after* the window (unlikely but possible)\n",
    "                      # Treat as dropout for this window, no relevant measurement\n",
    "                      measurement_at_window = None\n",
    "                      is_dropout_at_window = True\n",
    "            else:\n",
    "                 # Instant dropout (no followup measurements at all)\n",
    "                 measurement_at_window = None\n",
    "                 is_dropout_at_window = True\n",
    "\n",
    "\n",
    "            # Populate results for this time window\n",
    "            prefix = f\"{window_center}d\"\n",
    "            if measurement_at_window is not None:\n",
    "                result[f'{prefix}_weight_kg'] = measurement_at_window['weight_kg']\n",
    "                wl_kg, wl_pct = _calculate_wl_metrics(baseline_weight, measurement_at_window['weight_kg'])\n",
    "                result[f'wl_{prefix}_kg'] = wl_kg\n",
    "                result[f'wl_{prefix}_%'] = wl_pct\n",
    "                result[f'{prefix}_date'] = measurement_at_window['measurement_date']\n",
    "                result[f'days_to_{prefix}_measurement'] = (measurement_at_window['measurement_date'] - baseline_date).days\n",
    "            else:\n",
    "                # No relevant measurement found for this window\n",
    "                result[f'{prefix}_weight_kg'] = np.nan\n",
    "                result[f'wl_{prefix}_kg'] = np.nan\n",
    "                result[f'wl_{prefix}_%'] = np.nan\n",
    "                result[f'{prefix}_date'] = pd.NaT\n",
    "                result[f'days_to_{prefix}_measurement'] = np.nan\n",
    "\n",
    "            result[f'{prefix}_dropout'] = is_dropout_at_window\n",
    "\n",
    "\n",
    "        # 4. Calculate Time-to-Event Metrics (for each weight loss target)\n",
    "        for target in weight_loss_targets:\n",
    "            target_achieved = False\n",
    "            first_success_measurement = None\n",
    "            actual_wl_at_success = np.nan\n",
    "\n",
    "            # Check all followup measurements for the first success\n",
    "            for _, row in followup_measurements.iterrows():\n",
    "                current_weight = row['weight_kg']\n",
    "                if baseline_weight is not None and baseline_weight > 0:\n",
    "                    current_weight_loss_pct = ((baseline_weight - current_weight) / baseline_weight) * 100\n",
    "                    if round(current_weight_loss_pct, 2) >= target:\n",
    "                        target_achieved = True\n",
    "                        first_success_measurement = row\n",
    "                        actual_wl_at_success = round(current_weight_loss_pct, 2)\n",
    "                        break # Stop at the first success\n",
    "\n",
    "            # Populate results for this target\n",
    "            prefix = f\"{target}%_wl\"\n",
    "            result[f'{prefix}_achieved'] = target_achieved\n",
    "            if target_achieved and first_success_measurement is not None:\n",
    "                result[f'{prefix}_%'] = actual_wl_at_success\n",
    "                result[f'{prefix}_date'] = first_success_measurement['measurement_date']\n",
    "                result[f'days_to_{prefix}'] = (first_success_measurement['measurement_date'] - baseline_date).days\n",
    "            else:\n",
    "                result[f'{prefix}_%'] = np.nan\n",
    "                result[f'{prefix}_date'] = pd.NaT\n",
    "                result[f'days_to_{prefix}'] = np.nan # Or perhaps total_followup_days if censored? Check analysis plan needs. NaN is safer.\n",
    "\n",
    "        results_list.append(result)\n",
    "\n",
    "    return pd.DataFrame(results_list)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "MODIFIED: MAIN ORCHESTRATION FUNCTION FOR WIDE TABLE\n",
    "\"\"\"\n",
    "\n",
    "def generate_wide_analysis_dataset(input_connection, output_connection, weight_loss_targets, time_windows, window_span=10):\n",
    "    \"\"\"\n",
    "    Orchestrates the process to generate the single wide survival analysis dataset.\n",
    "    Loads data, prepares patient baseline info, calculates all outcomes per patient,\n",
    "    and saves the resulting wide DataFrame to the output database.\n",
    "    \"\"\"\n",
    "    # 1. Load and prepare input data\n",
    "    print(\"Loading measurements...\")\n",
    "    measurements = load_measurements(input_connection)\n",
    "    print(\"Loading medical records...\")\n",
    "    medical_records = load_medical_records(input_connection)\n",
    "    print(\"Preparing patient data...\")\n",
    "    prepared_data, filtered_measurements = prepare_patient_data(measurements, medical_records)\n",
    "\n",
    "    if prepared_data.empty:\n",
    "        print(\"ERROR: Prepared patient data is empty. Cannot proceed.\")\n",
    "        return pd.DataFrame() # Return empty DataFrame\n",
    "\n",
    "    # 2. Calculate wide outcomes for all patients\n",
    "    print(\"Calculating wide outcomes for all patients...\")\n",
    "    wide_results_df = calculate_wide_patient_outcomes(\n",
    "        prepared_data,\n",
    "        filtered_measurements,\n",
    "        weight_loss_targets,\n",
    "        time_windows,\n",
    "        window_span\n",
    "    )\n",
    "\n",
    "    # 3. Save the single wide table\n",
    "    if not wide_results_df.empty:\n",
    "        print(f\"--- Saving results to output database: {output_db_path} ---\")\n",
    "        print(f\"Saving table: {output_table_name} ({len(wide_results_df)} rows)\")\n",
    "        wide_results_df.to_sql(output_table_name, output_connection, if_exists='replace', index=False)\n",
    "        output_connection.commit() # Ensure changes are saved\n",
    "        print(\"--- Wide table saved successfully ---\")\n",
    "    else:\n",
    "        print(\"WARN: No results generated. Output table will be empty or not created.\")\n",
    "\n",
    "    # Removed the old summary logic based on multiple tables\n",
    "    # A new summary could be generated from wide_results_df if needed\n",
    "\n",
    "    return wide_results_df # Return the generated DataFrame\n",
    "\n",
    "\"\"\"\n",
    "EXECUTION BLOCK (Modified to call the new main function)\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"========== Generating Survival Analysis Input Dataset ==========\")\n",
    "    input_conn = None\n",
    "    output_conn = None\n",
    "    try:\n",
    "        # Connect to in-and output databases\n",
    "        print(f\"Connecting to input database: {input_db_path}\")\n",
    "        if not os.path.exists(input_db_path):\n",
    "             raise FileNotFoundError(f\"Input database not found at {input_db_path}\")\n",
    "        input_conn = sqlite3.connect(input_db_path)\n",
    "\n",
    "        print(f\"Connecting to output database: {output_db_path}\")\n",
    "        output_conn = sqlite3.connect(output_db_path)\n",
    "\n",
    "        # Run the new main analysis function\n",
    "        wide_df = generate_wide_analysis_dataset(\n",
    "            input_conn,\n",
    "            output_conn,\n",
    "            weight_loss_targets,\n",
    "            time_windows,\n",
    "            window_span\n",
    "        )\n",
    "\n",
    "        # Display basic info if successful\n",
    "        if not wide_df.empty:\n",
    "            print(\"\\n--- Survival Analysis Input Table Generation Summary ---\")\n",
    "            print(f\"Generated table '{output_table_name}' with {len(wide_df)} rows and {len(wide_df.columns)} columns.\")\n",
    "            # print(wide_df.head().to_string()) # Optionally print head\n",
    "            print(\"--- End Summary ---\")\n",
    "        else:\n",
    "            print(\"WARN: Analysis completed, but the resulting DataFrame is empty.\")\n",
    "\n",
    "        print(f\"Analysis data saved to {output_db_path}\")\n",
    "\n",
    "    # Error handling remains the same\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"ERROR: Database file not found - {e}\")\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"ERROR: SQLite database error - {e}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"ERROR: Data processing error - {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: An unexpected error occurred - {e}\")\n",
    "        # Consider adding traceback for debugging complex errors:\n",
    "        # import traceback\n",
    "        # print(traceback.format_exc())\n",
    "    finally:\n",
    "        # Ensure connections are closed\n",
    "        print(\"Closing database connections...\")\n",
    "        if input_conn:\n",
    "            input_conn.close()\n",
    "        if output_conn:\n",
    "            output_conn.close()\n",
    "        print(\"========== Survival Analysis Input Data GenerationFinished ==========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconsidered input structure for SA 18-19 Apr v2 - revise this well, I think this is the one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Generating Survival Analysis Input Dataset (Wide Format v3) ==========\n",
      "Connecting to input database: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\emotional_all_notna.sqlite\n",
      "Connecting to output database: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\survival_analysis_wide_v3.sqlite\n",
      "Loading measurements...\n",
      "Loading medical records...\n",
      "Preparing patient data...\n",
      "Calculating wide outcomes for all patients...\n",
      "Reordering columns...\n",
      "--- Saving results to output database: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\survival_analysis_wide_v3.sqlite ---\n",
      "Saving table: survival_analysis_wide_v3 (1664 rows)\n",
      "--- Wide table saved successfully ---\n",
      "\n",
      "--- Survival Analysis Input Table Generation Summary ---\n",
      "Generated table 'survival_analysis_wide_v3' with 1664 rows and 56 columns.\n",
      "--- End Summary ---\n",
      "Analysis data saved to C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\survival_analysis_wide_v3.sqlite\n",
      "Closing database connections...\n",
      "========== Survival Analysis Input Data Generation Finished (Wide Format v3) ==========\n"
     ]
    }
   ],
   "source": [
    "# This code should be placed in a new cell in your Jupyter Notebook.\n",
    "# It incorporates the requested changes into the previous wide-format script.\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import sqlite3\n",
    "from datetime import timedelta\n",
    "import numpy as np # Import numpy for NaN\n",
    "\n",
    "\"\"\"\n",
    "CONFIGURATION\n",
    "\"\"\"\n",
    "# Define directories and database paths - paper1_directory should be defined\n",
    "# in the first cell of this notebook chapter\n",
    "input_db_path = os.path.join(paper1_directory, 'emotional_all_notna.sqlite')\n",
    "input_measurements = \"measurements_with_metadata\"\n",
    "input_medical_records = \"medical_records_complete\"\n",
    "# Modified: Output database path remains the same, but will contain a different table structure\n",
    "output_db_path = os.path.join(paper1_directory, 'survival_analysis_wide_v2.sqlite') # Changed filename for new version\n",
    "output_table_name = \"survival_analysis_wide_v2\" # Changed table name for new version\n",
    "\n",
    "# Define analysis parameters\n",
    "weight_loss_targets = [5, 10, 15]     # Weight loss target percentages\n",
    "time_windows = [40, 60, 80]       # Time windows (centers) in days\n",
    "window_span = 10                   # Permissible span around windows (+/- days)\n",
    "\n",
    "# Define the variables stored in medical_records_complete that are relevant for the analysis.\n",
    "# These include basic metadata like patient and record ID,\n",
    "# basic factors such as age and sex,\n",
    "# as well as the emotional and eating behavior variables pivotal to the research question.\n",
    "# The list can be amended on demand -\n",
    "# for example, right now it does not include medical record creating and closing dates.\n",
    "# --- MODIFIED: Added 'dietitian_visits' ---\n",
    "relevant_medical_values = ['patient_id', 'medical_record_id', 'dietitian_visits', 'sex', 'age',\n",
    "                             'height_m', 'baseline_bmi', 'hunger', 'satiety', 'emotional_eating',\n",
    "                             'emotional_eating_value', 'quantity_control', 'impulse_control', 'weight_gain_cause', 'genomics_sample_id']\n",
    "\n",
    "# --- NEW: Define the desired final column order ---\n",
    "# This list determines the order of columns in the final output table.\n",
    "# It includes baseline info, overall followup, adherence proxies,\n",
    "# fixed-time metrics, time-to-event metrics, and finally confounders/predictors.\n",
    "FINAL_COLUMN_ORDER = [\n",
    "    # IDs\n",
    "    'patient_ID', 'medical_record_ID',\n",
    "    # Followup period info and adherence proxies\n",
    "    'baseline_date', 'last_aval_date', 'total_followup_days', 'nr_visits', 'nr_total_measurements', 'avg_days_between_measurements',\n",
    "    # Total weight change\n",
    "    'baseline_weight_kg', 'last_aval_weight_kg', 'total_wl_kg', 'total_wl_%', 'baseline_bmi', 'final_bmi', 'bmi_reduction',\n",
    "    # Fixed-Timepoint Analysis (Dynamically generated columns will be inserted here by logic below)\n",
    "    # Time-to-Event Analysis (Dynamically generated columns will be inserted here by logic below)\n",
    "    # Confounders / Predictors (from medical records)\n",
    "    'sex', 'age', 'height_m', 'hunger', 'satiety',\n",
    "    'emotional_eating', 'emotional_eating_value', 'quantity_control', 'impulse_control', 'weight_gain_cause', 'genomics_sample_id'\n",
    "]\n",
    "\n",
    "# --- NEW: Dynamically insert fixed-time and time-to-event columns into FINAL_COLUMN_ORDER ---\n",
    "fixed_time_cols = []\n",
    "for window in time_windows:\n",
    "    prefix = f\"{window}d\"\n",
    "    fixed_time_cols.extend([\n",
    "        f'{prefix}_weight_kg', f'wl_{prefix}_kg', f'wl_{prefix}_%',\n",
    "        f'{prefix}_date', f'days_to_{prefix}_measurement', f'{prefix}_dropout'\n",
    "    ])\n",
    "\n",
    "time_to_event_cols = []\n",
    "for target in weight_loss_targets:\n",
    "    prefix = f\"{target}%_wl\"\n",
    "    time_to_event_cols.extend([\n",
    "        f'{prefix}_achieved', f'{prefix}_%', f'{prefix}_date', f'days_to_{prefix}'\n",
    "    ])\n",
    "\n",
    "# Find the insertion point (after adherence proxies)\n",
    "insert_point = FINAL_COLUMN_ORDER.index('total_wl_%') + 1\n",
    "# Insert the dynamic columns\n",
    "FINAL_COLUMN_ORDER[insert_point:insert_point] = fixed_time_cols + time_to_event_cols\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DATA LOADING & PREPARATION (No changes needed here, kept for context)\n",
    "\"\"\"\n",
    "\n",
    "def load_measurements(connection):\n",
    "    \"\"\"\n",
    "    Load measurements from the measurement_with_metadata table;\n",
    "    make sure key values are in the correct format.\n",
    "    \"\"\"\n",
    "    query = f\"SELECT * FROM {input_measurements}\"\n",
    "    measurements = pd.read_sql_query(query, connection)\n",
    "    measurements['measurement_date'] = pd.to_datetime(measurements['measurement_date'], errors='coerce')\n",
    "    measurements['weight_kg'] = pd.to_numeric(measurements['weight_kg'], errors='coerce')\n",
    "    # Ensure sorting for consistent 'first'/'last' operations later\n",
    "    measurements = measurements.sort_values(['patient_id', 'medical_record_id', 'measurement_date'])\n",
    "    return measurements\n",
    "\n",
    "def load_medical_records(connection):\n",
    "    \"\"\"\n",
    "    Load medical records from the medical_records_complete table;\n",
    "    make sure date values are in datetime format.\n",
    "    The exact columns to be used are defined in the prepare_patient_data function.\n",
    "    \"\"\"\n",
    "    query = f\"SELECT * FROM {input_medical_records}\"\n",
    "    medical_records = pd.read_sql_query(query, connection)\n",
    "    # Ensure relevant date columns are datetime\n",
    "    for col in ['medical_record_creation_date', 'baseline_measurement_date', 'final_measurement_date']:\n",
    "         if col in medical_records.columns:\n",
    "             medical_records[col] = pd.to_datetime(medical_records[col], errors='coerce')\n",
    "    # --- NEW: Ensure dietitian_visits is numeric ---\n",
    "    if 'dietitian_visits' in medical_records.columns:\n",
    "        medical_records['dietitian_visits'] = pd.to_numeric(medical_records['dietitian_visits'], errors='coerce')\n",
    "    return medical_records\n",
    "\n",
    "def prepare_patient_data(measurements, medical_records):\n",
    "    \"\"\"\n",
    "    Filter measurements to only include those from the earliest medical record for each patient.\n",
    "    Merge measurements with relevant medical record data, including the pivotal eating behavior scores.\n",
    "    \"\"\"\n",
    "    # Filter measurements to only include those from the first treatment record of each patient\n",
    "    # Ensure measurements are sorted before grouping\n",
    "    measurements = measurements.sort_values(['patient_id', 'measurement_date'])\n",
    "    earliest_records_with_data = measurements.groupby('patient_id')['medical_record_id'].first().reset_index()\n",
    "\n",
    "    filtered_measurements = pd.merge(\n",
    "        measurements,\n",
    "        earliest_records_with_data,\n",
    "        on=['patient_id', 'medical_record_id'],\n",
    "        how='inner'\n",
    "    )\n",
    "    # Ensure filtered_measurements are sorted for baseline identification\n",
    "    filtered_measurements = filtered_measurements.sort_values(['patient_id', 'medical_record_id', 'measurement_date'])\n",
    "\n",
    "    # Identify the baseline measurement row for each record (the first measurement in the filtered set)\n",
    "    baseline_data_rows = filtered_measurements.groupby(['patient_id', 'medical_record_id']).first().reset_index()\n",
    "\n",
    "    # Select only relevant columns from medical_records to merge\n",
    "    # --- MODIFIED: Now includes 'dietitian_visits' if added to relevant_medical_values ---\n",
    "    cols_to_select = [col for col in relevant_medical_values if col in medical_records.columns]\n",
    "    medical_record_subset = medical_records[cols_to_select]\n",
    "\n",
    "    # Merge baseline measurement info with the selected medical record data\n",
    "    # Use baseline_data_rows which contains the actual first measurement details\n",
    "    prepared_data = pd.merge(\n",
    "        baseline_data_rows[['patient_id', 'medical_record_id', 'measurement_date', 'weight_kg']], # Get baseline date/weight from actual first measurement\n",
    "        medical_record_subset,\n",
    "        on=['patient_id', 'medical_record_id'],\n",
    "        how='left' # Keep all baseline measurements\n",
    "    )\n",
    "    # Rename columns for clarity before returning\n",
    "    prepared_data = prepared_data.rename(columns={'measurement_date': 'baseline_date', 'weight_kg': 'baseline_weight_kg'})\n",
    "\n",
    "    # Add baseline_bmi from medical_records if available and not already present from measurement merge\n",
    "    if 'baseline_bmi' in medical_record_subset.columns and 'baseline_bmi' not in prepared_data.columns:\n",
    "         prepared_data = pd.merge(\n",
    "              prepared_data,\n",
    "              medical_record_subset[['patient_id', 'medical_record_id', 'baseline_bmi']],\n",
    "              on=['patient_id', 'medical_record_id'],\n",
    "              how='left'\n",
    "         )\n",
    "\n",
    "    return prepared_data, filtered_measurements\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "MODIFIED: CALCULATE WEIGHT LOSS OUTCOMES FOR WIDE TABLE\n",
    "\"\"\"\n",
    "\n",
    "def _get_patient_baseline(patient_data, patient_id, medical_record_id):\n",
    "    \"\"\"\n",
    "    Get the prepared baseline data for a patient's specific medical record.\n",
    "    (Function remains largely the same, but operates on the prepared_data structure)\n",
    "    \"\"\"\n",
    "    patient_baseline = patient_data[\n",
    "        (patient_data['patient_id'] == patient_id) &\n",
    "        (patient_data['medical_record_id'] == medical_record_id)\n",
    "    ]\n",
    "    if patient_baseline.empty:\n",
    "        print(f\"WARN: No baseline data found for patient {patient_id}, record {medical_record_id}. Skipping.\")\n",
    "        return None\n",
    "    # Ensure we return a Series for consistent access\n",
    "    return patient_baseline.iloc[0]\n",
    "\n",
    "def _calculate_wl_metrics(baseline_weight, current_weight):\n",
    "    \"\"\" Helper to calculate weight loss kg and % \"\"\"\n",
    "    if pd.isna(baseline_weight) or pd.isna(current_weight) or baseline_weight == 0:\n",
    "        return np.nan, np.nan\n",
    "    wl_kg = baseline_weight - current_weight\n",
    "    wl_pct = (wl_kg / baseline_weight) * 100\n",
    "    return wl_kg, round(wl_pct, 2)\n",
    "\n",
    "# Removed _check_target_achievement, _determine_final_measurement, _calculate_outcome_metrics\n",
    "# Their logic will be integrated into the main calculation function.\n",
    "\n",
    "def calculate_wide_patient_outcomes(prepared_patient_data, filtered_measurements, weight_loss_targets, time_windows, window_span):\n",
    "    \"\"\"\n",
    "    Calculate all required outcomes (fixed-time and time-to-event) for each patient\n",
    "    and return a list of dictionaries, each representing a row in the wide table.\n",
    "    \"\"\"\n",
    "    results_list = []\n",
    "    # Group all measurements for the relevant (earliest) medical record per patient\n",
    "    grouped_measurements = filtered_measurements.groupby(['patient_id', 'medical_record_id'])\n",
    "\n",
    "    for (patient_id, medical_record_id), group in grouped_measurements:\n",
    "        # --- NEW: Calculate total number of measurements for this group (record) ---\n",
    "        num_measurements = len(group)\n",
    "\n",
    "        # 1. Get Baseline Info\n",
    "        baseline_info = _get_patient_baseline(prepared_patient_data, patient_id, medical_record_id)\n",
    "        if baseline_info is None:\n",
    "            continue\n",
    "\n",
    "        baseline_date = baseline_info['baseline_date']\n",
    "        baseline_weight = baseline_info['baseline_weight_kg']\n",
    "\n",
    "        # Initialize result dictionary with baseline info AND adherence proxies\n",
    "        result = {\n",
    "            # IDs\n",
    "            'patient_ID': patient_id, # Match Excel header\n",
    "            'medical_record_ID': medical_record_id, # Match Excel header\n",
    "            # Baseline\n",
    "            'baseline_date': baseline_date,\n",
    "            'baseline_weight_kg': baseline_weight,\n",
    "            # Adherence Proxies (Initialize early)\n",
    "            'nr_visits': baseline_info.get('dietitian_visits'), # Get from merged baseline data\n",
    "            'nr_total_measurements': num_measurements, # Use calculated value\n",
    "            'avg_days_between_measurements': np.nan, # Initialize, calculated later\n",
    "            # Confounders / Predictors (Initialize early)\n",
    "            'sex': baseline_info.get('sex'),\n",
    "            'age': baseline_info.get('age'),\n",
    "            'height_m': baseline_info.get('height_m'),\n",
    "            'baseline_bmi': baseline_info.get('baseline_bmi'), # Get BMI from prepared data\n",
    "            'hunger': baseline_info.get('hunger'),\n",
    "            'satiety': baseline_info.get('satiety'),\n",
    "            'emotional_eating': baseline_info.get('emotional_eating'),\n",
    "            'emotional_eating_value': baseline_info.get('emotional_eating_value'),\n",
    "            'quantity_control': baseline_info.get('quantity_control'),\n",
    "            'impulse_control': baseline_info.get('impulse_control'), \n",
    "            'weight_gain_cause': baseline_info.get('weight_gain_cause'),\n",
    "            'genomics_sample_id': baseline_info.get('genomics_sample_id')\n",
    "        }\n",
    "\n",
    "        # Get all measurements *after* baseline for this group\n",
    "        followup_measurements = group[group['measurement_date'] > baseline_date].sort_values('measurement_date')\n",
    "\n",
    "        # 2. Calculate Overall Follow-up Metrics\n",
    "        if not followup_measurements.empty:\n",
    "            last_measurement = followup_measurements.iloc[-1]\n",
    "            result['last_aval_date'] = last_measurement['measurement_date']\n",
    "            # --- MODIFIED: Calculate inclusive total_followup_days (+1) ---\n",
    "            result['total_followup_days'] = (last_measurement['measurement_date'] - baseline_date).days + 1\n",
    "            result['last_aval_weight_kg'] = last_measurement['weight_kg']\n",
    "            wl_kg, wl_pct = _calculate_wl_metrics(baseline_weight, last_measurement['weight_kg'])\n",
    "            result['total_wl_kg'] = wl_kg\n",
    "            result['total_wl_%'] = wl_pct\n",
    "        else:\n",
    "            # Handle instant dropouts (only baseline measurement exists)\n",
    "            result['last_aval_date'] = baseline_date\n",
    "            # --- MODIFIED: Set total_followup_days to 1 for instant dropouts ---\n",
    "            result['total_followup_days'] = 1\n",
    "            result['last_aval_weight_kg'] = baseline_weight\n",
    "            result['total_wl_kg'] = 0.0\n",
    "            result['total_wl_%'] = 0.0\n",
    "\n",
    "        # --- NEW: Calculate avg_days_between_measurements ---\n",
    "        # Requires total_followup_days and nr_total_measurements\n",
    "        if result['nr_total_measurements'] is not None and result['nr_total_measurements'] > 1:\n",
    "             # Use the calculated inclusive total_followup_days\n",
    "             total_days = result['total_followup_days']\n",
    "             # Calculate average based on number of intervals (N measurements = N-1 intervals)\n",
    "             # Ensure total_days is treated as the span covering N points (so N-1 intervals)\n",
    "             # If total_days is 1 (instant dropout), num_measurements is 1, this condition isn't met.\n",
    "             # If total_days > 1, num_measurements must be >= 2.\n",
    "             result['avg_days_between_measurements'] = round( (total_days -1) / (result['nr_total_measurements'] - 1) , 2) if (result['nr_total_measurements'] - 1) > 0 else np.nan\n",
    "        else:\n",
    "             # If 0 or 1 measurements, average days between is undefined\n",
    "             result['avg_days_between_measurements'] = np.nan\n",
    "\n",
    "\n",
    "        # 3. Calculate Fixed-Timepoint Metrics (for each time window)\n",
    "        for window_center in time_windows:\n",
    "            min_window_date = baseline_date + timedelta(days=(window_center - window_span))\n",
    "            max_window_date = baseline_date + timedelta(days=(window_center + window_span))\n",
    "            target_date = baseline_date + timedelta(days=window_center)\n",
    "\n",
    "            # Find measurements strictly *within* the cutoff window span\n",
    "            measurements_around_cutoff = followup_measurements[\n",
    "                (followup_measurements['measurement_date'] >= min_window_date) &\n",
    "                (followup_measurements['measurement_date'] <= max_window_date)\n",
    "            ]\n",
    "\n",
    "            measurement_for_window = None # The measurement to use for this window's stats\n",
    "            is_dropout_at_window = True # Assume dropout unless a measurement is found *in* the window\n",
    "\n",
    "            if not measurements_around_cutoff.empty:\n",
    "                # Measurement exists within the window span. Find the closest one.\n",
    "                is_dropout_at_window = False # Found measurement in window\n",
    "                measurements_around_cutoff = measurements_around_cutoff.copy()\n",
    "                measurements_around_cutoff['distance_to_center'] = abs(\n",
    "                    (measurements_around_cutoff['measurement_date'] - target_date).dt.days\n",
    "                )\n",
    "                closest_measurement_idx = measurements_around_cutoff['distance_to_center'].idxmin()\n",
    "                measurement_for_window = measurements_around_cutoff.loc[closest_measurement_idx]\n",
    "            # else: If measurements_around_cutoff is empty, is_dropout_at_window remains True.\n",
    "            # No need to check for measurements *before* the window for populating data,\n",
    "            # as per the requirement to leave fields blank for dropouts.\n",
    "\n",
    "            # --- MODIFIED: Populate results based *strictly* on dropout status for the window ---\n",
    "            prefix = f\"{window_center}d\"\n",
    "            result[f'{prefix}_dropout'] = is_dropout_at_window # Set dropout status first\n",
    "\n",
    "            if is_dropout_at_window:\n",
    "                # If dropout for this window, set all related metrics to NaN/NaT\n",
    "                result[f'{prefix}_weight_kg'] = np.nan\n",
    "                result[f'wl_{prefix}_kg'] = np.nan\n",
    "                result[f'wl_{prefix}_%'] = np.nan\n",
    "                result[f'{prefix}_date'] = pd.NaT\n",
    "                result[f'days_to_{prefix}_measurement'] = np.nan\n",
    "            else:\n",
    "                # If NOT dropout, populate metrics using the found measurement_for_window\n",
    "                # (This block only runs if measurement_for_window is not None)\n",
    "                result[f'{prefix}_weight_kg'] = measurement_for_window['weight_kg']\n",
    "                wl_kg, wl_pct = _calculate_wl_metrics(baseline_weight, measurement_for_window['weight_kg'])\n",
    "                result[f'wl_{prefix}_kg'] = wl_kg\n",
    "                result[f'wl_{prefix}_%'] = wl_pct\n",
    "                result[f'{prefix}_date'] = measurement_for_window['measurement_date']\n",
    "                # Calculate days from baseline to this specific measurement\n",
    "                result[f'days_to_{prefix}_measurement'] = (measurement_for_window['measurement_date'] - baseline_date).days + 1 # Inclusive days\n",
    "\n",
    "\n",
    "        # 4. Calculate Time-to-Event Metrics (for each weight loss target)\n",
    "        # (No changes needed in this section based on discussion)\n",
    "        for target in weight_loss_targets:\n",
    "            target_achieved = False\n",
    "            first_success_measurement = None\n",
    "            actual_wl_at_success = np.nan\n",
    "\n",
    "            # Check all followup measurements for the first success\n",
    "            for _, row in followup_measurements.iterrows():\n",
    "                current_weight = row['weight_kg']\n",
    "                if baseline_weight is not None and baseline_weight > 0:\n",
    "                    current_weight_loss_pct = ((baseline_weight - current_weight) / baseline_weight) * 100\n",
    "                    if round(current_weight_loss_pct, 2) >= target:\n",
    "                        target_achieved = True\n",
    "                        first_success_measurement = row\n",
    "                        actual_wl_at_success = round(current_weight_loss_pct, 2)\n",
    "                        break # Stop at the first success\n",
    "\n",
    "            # Populate results for this target\n",
    "            prefix = f\"{target}%_wl\"\n",
    "            result[f'{prefix}_achieved'] = target_achieved\n",
    "            if target_achieved and first_success_measurement is not None:\n",
    "                result[f'{prefix}_%'] = actual_wl_at_success\n",
    "                result[f'{prefix}_date'] = first_success_measurement['measurement_date']\n",
    "                # Calculate inclusive days to achieve target\n",
    "                result[f'days_to_{prefix}'] = (first_success_measurement['measurement_date'] - baseline_date).days + 1\n",
    "            else:\n",
    "                result[f'{prefix}_%'] = np.nan\n",
    "                result[f'{prefix}_date'] = pd.NaT\n",
    "                result[f'days_to_{prefix}'] = np.nan\n",
    "\n",
    "        results_list.append(result)\n",
    "\n",
    "    return pd.DataFrame(results_list)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "MODIFIED: MAIN ORCHESTRATION FUNCTION FOR WIDE TABLE\n",
    "\"\"\"\n",
    "\n",
    "def generate_wide_analysis_dataset(input_connection, output_connection, weight_loss_targets, time_windows, window_span=10):\n",
    "    \"\"\"\n",
    "    Orchestrates the process to generate the single wide survival analysis dataset.\n",
    "    Loads data, prepares patient baseline info, calculates all outcomes per patient,\n",
    "    reorders columns, and saves the resulting wide DataFrame to the output database.\n",
    "    \"\"\"\n",
    "    # 1. Load and prepare input data\n",
    "    print(\"Loading measurements...\")\n",
    "    measurements = load_measurements(input_connection)\n",
    "    print(\"Loading medical records...\")\n",
    "    medical_records = load_medical_records(input_connection)\n",
    "    print(\"Preparing patient data...\")\n",
    "    prepared_data, filtered_measurements = prepare_patient_data(measurements, medical_records)\n",
    "\n",
    "    if prepared_data.empty:\n",
    "        print(\"ERROR: Prepared patient data is empty. Cannot proceed.\")\n",
    "        return pd.DataFrame() # Return empty DataFrame\n",
    "\n",
    "    # 2. Calculate wide outcomes for all patients\n",
    "    print(\"Calculating wide outcomes for all patients...\")\n",
    "    wide_results_df = calculate_wide_patient_outcomes(\n",
    "        prepared_data,\n",
    "        filtered_measurements,\n",
    "        weight_loss_targets,\n",
    "        time_windows,\n",
    "        window_span\n",
    "    )\n",
    "\n",
    "    # --- NEW: Reorder columns according to FINAL_COLUMN_ORDER defined in config ---\n",
    "    if not wide_results_df.empty:\n",
    "        print(\"Reordering columns...\")\n",
    "        # Ensure all columns in FINAL_COLUMN_ORDER exist in the DataFrame, handle potential missing ones\n",
    "        final_columns_present = [col for col in FINAL_COLUMN_ORDER if col in wide_results_df.columns]\n",
    "        missing_cols = [col for col in FINAL_COLUMN_ORDER if col not in wide_results_df.columns]\n",
    "        if missing_cols:\n",
    "             print(f\"WARN: The following columns defined in FINAL_COLUMN_ORDER were not found in the generated data and will be skipped: {missing_cols}\")\n",
    "        # Add any columns present in DataFrame but not in FINAL_COLUMN_ORDER to the end, just in case\n",
    "        extra_cols = [col for col in wide_results_df.columns if col not in final_columns_present]\n",
    "        if extra_cols:\n",
    "             print(f\"WARN: The following columns were generated but not included in FINAL_COLUMN_ORDER; they will be added to the end: {extra_cols}\")\n",
    "\n",
    "        wide_results_df = wide_results_df[final_columns_present + extra_cols]\n",
    "\n",
    "\n",
    "    # 3. Save the single wide table\n",
    "    if not wide_results_df.empty:\n",
    "        print(f\"--- Saving results to output database: {output_db_path} ---\")\n",
    "        print(f\"Saving table: {output_table_name} ({len(wide_results_df)} rows)\")\n",
    "        wide_results_df.to_sql(output_table_name, output_connection, if_exists='replace', index=False)\n",
    "        output_connection.commit() # Ensure changes are saved\n",
    "        print(\"--- Wide table saved successfully ---\")\n",
    "    else:\n",
    "        print(\"WARN: No results generated. Output table will be empty or not created.\")\n",
    "\n",
    "    # Removed the old summary logic based on multiple tables\n",
    "    # A new summary could be generated from wide_results_df if needed\n",
    "\n",
    "    return wide_results_df # Return the generated DataFrame\n",
    "\n",
    "\"\"\"\n",
    "EXECUTION BLOCK (Modified to call the new main function and use new output names)\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"========== Generating Survival Analysis Input Dataset (Wide Format v2) ==========\") # Updated title\n",
    "    input_conn = None\n",
    "    output_conn = None\n",
    "    try:\n",
    "        # Connect to in-and output databases\n",
    "        print(f\"Connecting to input database: {input_db_path}\")\n",
    "        if not os.path.exists(input_db_path):\n",
    "             raise FileNotFoundError(f\"Input database not found at {input_db_path}\")\n",
    "        input_conn = sqlite3.connect(input_db_path)\n",
    "\n",
    "        print(f\"Connecting to output database: {output_db_path}\")\n",
    "        output_conn = sqlite3.connect(output_db_path)\n",
    "\n",
    "        # Run the new main analysis function\n",
    "        wide_df = generate_wide_analysis_dataset(\n",
    "            input_conn,\n",
    "            output_conn,\n",
    "            weight_loss_targets,\n",
    "            time_windows,\n",
    "            window_span\n",
    "        )\n",
    "\n",
    "        # Display basic info if successful\n",
    "        if not wide_df.empty:\n",
    "            print(\"\\n--- Survival Analysis Input Table Generation Summary ---\")\n",
    "            print(f\"Generated table '{output_table_name}' with {len(wide_df)} rows and {len(wide_df.columns)} columns.\")\n",
    "            # print(wide_df.head().to_string()) # Optionally print head\n",
    "            print(\"--- End Summary ---\")\n",
    "        else:\n",
    "            print(\"WARN: Analysis completed, but the resulting DataFrame is empty.\")\n",
    "\n",
    "        print(f\"Analysis data saved to {output_db_path}\")\n",
    "\n",
    "    # Error handling remains the same\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"ERROR: Database file not found - {e}\")\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"ERROR: SQLite database error - {e}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"ERROR: Data processing error - {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: An unexpected error occurred - {e}\")\n",
    "        # Consider adding traceback for debugging complex errors:\n",
    "        # import traceback\n",
    "        # print(traceback.format_exc())\n",
    "    finally:\n",
    "        # Ensure connections are closed\n",
    "        print(\"Closing database connections...\")\n",
    "        if input_conn:\n",
    "            input_conn.close()\n",
    "        if output_conn:\n",
    "            output_conn.close()\n",
    "        print(\"========== Survival Analysis Input Data Generation Finished (Wide Format v2) ==========\") # Updated title\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actually, this may be the one, Apr 20 v3 - added final BMI too, and fetching BMI from measurements instead of medical records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Generating Survival Analysis Input Dataset (Wide Format v3) ==========\n",
      "Connecting to input database: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\emotional_all_notna.sqlite\n",
      "Connecting to output database: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\survival_analysis.sqlite\n",
      "Loading measurements...\n",
      "Loading medical records...\n",
      "Preparing patient data...\n",
      "Calculating wide outcomes for all patients...\n",
      "Reordering columns...\n",
      "--- Saving results to output database: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\survival_analysis.sqlite ---\n",
      "Saving table: sa_input_table (1664 rows)\n",
      "--- Wide table saved successfully ---\n",
      "\n",
      "--- Survival Analysis Input Table Generation Summary ---\n",
      "Generated table 'sa_input_table' with 1664 rows and 56 columns.\n",
      "--- End Summary ---\n",
      "Analysis data saved to C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\survival_analysis.sqlite\n",
      "Closing database connections...\n",
      "========== Survival Analysis Input Data Generation Finished (Wide Format v3) ==========\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# filepath: c:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\paper1_script.ipynb\n",
    "# This code should be placed in a new cell in your Jupyter Notebook.\n",
    "# It incorporates the requested changes for BMI handling into the wide-format script.\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import sqlite3\n",
    "from datetime import timedelta\n",
    "import numpy as np # Import numpy for NaN\n",
    "\n",
    "\"\"\"\n",
    "CONFIGURATION\n",
    "\"\"\"\n",
    "# Define directories and database paths - paper1_directory should be defined\n",
    "# in the first cell of this notebook chapter\n",
    "input_db_path = os.path.join(paper1_directory, 'emotional_all_notna.sqlite')\n",
    "input_measurements = \"measurements_with_metadata\"\n",
    "input_medical_records = \"medical_records_complete\"\n",
    "# Output database path and table name\n",
    "output_db_path = os.path.join(paper1_directory, 'survival_analysis.sqlite') # Changed filename for new version\n",
    "output_table_name = \"sa_input_table\" # Changed table name for new version\n",
    "\n",
    "# Define analysis parameters\n",
    "weight_loss_targets = [5, 10, 15]     # Weight loss target percentages\n",
    "time_windows = [40, 60, 80]       # Time windows (centers) in days\n",
    "window_span = 10                   # Permissible span around windows (+/- days)\n",
    "\n",
    "# Define the variables stored in medical_records_complete that are relevant for the analysis.\n",
    "# --- MODIFIED: Removed 'baseline_bmi' as it will be sourced from measurements ---\n",
    "relevant_medical_values = ['patient_id', 'medical_record_id', 'dietitian_visits', 'sex_f', 'age_when_creating_record',\n",
    "                             'height_m', 'hunger_yn', 'satiety_yn', 'emotional_eating_yn', \n",
    "                             'emotional_eating_value_likert', 'quantity_control_likert', 'impulse_control_likert', 'weight_gain_cause', 'genomics_sample_id']\n",
    "\n",
    "# --- MODIFIED: Renamed to output_column_order and updated for BMI ---\n",
    "# This list determines the order of columns in the final output table.\n",
    "output_column_order = [\n",
    "    # IDs\n",
    "    'patient_ID', 'medical_record_ID',\n",
    "    # Followup period info and adherence proxies\n",
    "    'baseline_date', 'last_aval_date', 'total_followup_days', 'nr_visits', 'nr_total_measurements', 'avg_days_between_measurements',\n",
    "    # Total weight change & BMI change\n",
    "    'baseline_weight_kg', 'last_aval_weight_kg', 'total_wl_kg', 'total_wl_%',\n",
    "    'baseline_bmi', 'final_bmi', 'bmi_reduction', # Added BMI columns here\n",
    "    # Fixed-Timepoint Analysis (Dynamically generated columns will be inserted here by logic below)\n",
    "    # Time-to-Event Analysis (Dynamically generated columns will be inserted here by logic below)\n",
    "    # Confounders / Predictors (from medical records)\n",
    "    'sex_f', 'age', 'height_m', 'hunger_yn', 'satiety_yn', 'emotional_eating_yn', \n",
    "    'emotional_eating_value_likert', 'quantity_control_likert', 'impulse_control_likert', 'weight_gain_cause', 'genomics_sample_id'\n",
    "]\n",
    "\n",
    "# --- MODIFIED: Dynamically insert fixed-time and time-to-event columns into output_column_order ---\n",
    "fixed_time_cols = []\n",
    "for window in time_windows:\n",
    "    prefix = f\"{window}d\"\n",
    "    # --- MODIFIED: Excluded BMI calculation for fixed timepoints ---\n",
    "    fixed_time_cols.extend([\n",
    "        f'{prefix}_weight_kg', f'wl_{prefix}_kg', f'wl_{prefix}_%',\n",
    "        f'{prefix}_date', f'days_to_{prefix}_measurement', f'{prefix}_dropout'\n",
    "        # Removed: f'{prefix}_bmi'\n",
    "    ])\n",
    "\n",
    "time_to_event_cols = []\n",
    "for target in weight_loss_targets:\n",
    "    prefix = f\"{target}%_wl\"\n",
    "    time_to_event_cols.extend([\n",
    "        f'{prefix}_achieved', f'{prefix}_%', f'{prefix}_date', f'days_to_{prefix}'\n",
    "    ])\n",
    "\n",
    "# Find the insertion point (after BMI reduction)\n",
    "# --- MODIFIED: Insertion point updated ---\n",
    "insert_point = output_column_order.index('bmi_reduction') + 1\n",
    "# Insert the dynamic columns\n",
    "output_column_order[insert_point:insert_point] = fixed_time_cols + time_to_event_cols\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DATA LOADING & PREPARATION\n",
    "\"\"\"\n",
    "\n",
    "def load_measurements(connection):\n",
    "    \"\"\"\n",
    "    Load measurements from the measurement_with_metadata table;\n",
    "    make sure key values are in the correct format.\n",
    "    --- MODIFIED: Ensure BMI is numeric ---\n",
    "    \"\"\"\n",
    "    query = f\"SELECT * FROM {input_measurements}\"\n",
    "    measurements = pd.read_sql_query(query, connection)\n",
    "    measurements['measurement_date'] = pd.to_datetime(measurements['measurement_date'], errors='coerce')\n",
    "    measurements['weight_kg'] = pd.to_numeric(measurements['weight_kg'], errors='coerce')\n",
    "    # --- NEW: Ensure BMI is numeric ---\n",
    "    measurements['bmi'] = pd.to_numeric(measurements['bmi'], errors='coerce')\n",
    "    # Ensure sorting for consistent 'first'/'last' operations later\n",
    "    measurements = measurements.sort_values(['patient_id', 'medical_record_id', 'measurement_date'])\n",
    "    return measurements\n",
    "\n",
    "def load_medical_records(connection):\n",
    "    \"\"\"\n",
    "    Load medical records from the medical_records_complete table;\n",
    "    make sure date values are in datetime format.\n",
    "    The exact columns to be used are defined in the prepare_patient_data function.\n",
    "    \"\"\"\n",
    "    query = f\"SELECT * FROM {input_medical_records}\"\n",
    "    medical_records = pd.read_sql_query(query, connection)\n",
    "    # Ensure relevant date columns are datetime\n",
    "    for col in ['medical_record_creation_date', 'baseline_measurement_date', 'final_measurement_date']:\n",
    "         if col in medical_records.columns:\n",
    "             medical_records[col] = pd.to_datetime(medical_records[col], errors='coerce')\n",
    "    # Ensure dietitian_visits is numeric\n",
    "    if 'dietitian_visits' in medical_records.columns:\n",
    "        medical_records['dietitian_visits'] = pd.to_numeric(medical_records['dietitian_visits'], errors='coerce')\n",
    "    return medical_records\n",
    "\n",
    "def prepare_patient_data(measurements, medical_records):\n",
    "    \"\"\"\n",
    "    Filter measurements to only include those from the earliest medical record for each patient.\n",
    "    Merge measurements with relevant medical record data, including the pivotal eating behavior scores.\n",
    "    --- MODIFIED: Fetch baseline BMI from measurements ---\n",
    "    \"\"\"\n",
    "    # Filter measurements to only include those from the first treatment record of each patient\n",
    "    measurements = measurements.sort_values(['patient_id', 'measurement_date'])\n",
    "    earliest_records_with_data = measurements.groupby('patient_id')['medical_record_id'].first().reset_index()\n",
    "\n",
    "    filtered_measurements = pd.merge(\n",
    "        measurements,\n",
    "        earliest_records_with_data,\n",
    "        on=['patient_id', 'medical_record_id'],\n",
    "        how='inner'\n",
    "    )\n",
    "    filtered_measurements = filtered_measurements.sort_values(['patient_id', 'medical_record_id', 'measurement_date'])\n",
    "\n",
    "    # Identify the baseline measurement row for each record (the first measurement in the filtered set)\n",
    "    # --- MODIFIED: Include 'bmi' ---\n",
    "    baseline_data_rows = filtered_measurements.groupby(['patient_id', 'medical_record_id']).first().reset_index()\n",
    "\n",
    "    # Select only relevant columns from medical_records to merge\n",
    "    cols_to_select = [col for col in relevant_medical_values if col in medical_records.columns]\n",
    "    medical_record_subset = medical_records[cols_to_select]\n",
    "\n",
    "    # Merge baseline measurement info with the selected medical record data\n",
    "    # --- MODIFIED: Include 'bmi' from baseline_data_rows ---\n",
    "    prepared_data = pd.merge(\n",
    "        baseline_data_rows[['patient_id', 'medical_record_id', 'measurement_date', 'weight_kg', 'bmi']], # Added 'bmi'\n",
    "        medical_record_subset,\n",
    "        on=['patient_id', 'medical_record_id'],\n",
    "        how='left' # Keep all baseline measurements\n",
    "    )\n",
    "    # Rename columns for clarity before returning\n",
    "    # --- MODIFIED: Rename 'bmi' to 'baseline_bmi' ---\n",
    "    prepared_data = prepared_data.rename(columns={\n",
    "        'measurement_date': 'baseline_date',\n",
    "        'weight_kg': 'baseline_weight_kg',\n",
    "        'bmi': 'baseline_bmi' # Rename BMI from measurement\n",
    "    })\n",
    "\n",
    "    # --- REMOVED: Separate merge for baseline_bmi from medical_records is no longer needed ---\n",
    "    # if 'baseline_bmi' in medical_record_subset.columns and 'baseline_bmi' not in prepared_data.columns:\n",
    "    #      ... (old merge logic removed) ...\n",
    "\n",
    "    return prepared_data, filtered_measurements\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "MODIFIED: CALCULATE WEIGHT LOSS OUTCOMES FOR WIDE TABLE\n",
    "\"\"\"\n",
    "\n",
    "def _get_patient_baseline(patient_data, patient_id, medical_record_id):\n",
    "    \"\"\"\n",
    "    Get the prepared baseline data for a patient's specific medical record.\n",
    "    \"\"\"\n",
    "    patient_baseline = patient_data[\n",
    "        (patient_data['patient_id'] == patient_id) &\n",
    "        (patient_data['medical_record_id'] == medical_record_id)\n",
    "    ]\n",
    "    if patient_baseline.empty:\n",
    "        print(f\"WARN: No baseline data found for patient {patient_id}, record {medical_record_id}. Skipping.\")\n",
    "        return None\n",
    "    return patient_baseline.iloc[0]\n",
    "\n",
    "def _calculate_wl_metrics(baseline_weight, current_weight):\n",
    "    \"\"\" Helper to calculate weight loss kg and % \"\"\"\n",
    "    if pd.isna(baseline_weight) or pd.isna(current_weight) or baseline_weight == 0:\n",
    "        return np.nan, np.nan\n",
    "    wl_kg = baseline_weight - current_weight\n",
    "    wl_pct = (wl_kg / baseline_weight) * 100\n",
    "    return wl_kg, round(wl_pct, 2)\n",
    "\n",
    "# --- NEW: Helper to calculate BMI reduction ---\n",
    "def _calculate_bmi_reduction(baseline_bmi, final_bmi):\n",
    "    \"\"\" Helper to calculate BMI reduction (final - baseline) \"\"\"\n",
    "    if pd.isna(baseline_bmi) or pd.isna(final_bmi):\n",
    "        return np.nan\n",
    "    return round(final_bmi - baseline_bmi, 2)\n",
    "\n",
    "\n",
    "def calculate_wide_patient_outcomes(prepared_patient_data, filtered_measurements, weight_loss_targets, time_windows, window_span):\n",
    "    \"\"\"\n",
    "    Calculate all required outcomes (fixed-time and time-to-event) for each patient\n",
    "    and return a list of dictionaries, each representing a row in the wide table.\n",
    "    --- MODIFIED: Incorporates BMI from measurements and BMI reduction ---\n",
    "    \"\"\"\n",
    "    results_list = []\n",
    "    grouped_measurements = filtered_measurements.groupby(['patient_id', 'medical_record_id'])\n",
    "\n",
    "    for (patient_id, medical_record_id), group in grouped_measurements:\n",
    "        num_measurements = len(group)\n",
    "\n",
    "        # 1. Get Baseline Info\n",
    "        baseline_info = _get_patient_baseline(prepared_patient_data, patient_id, medical_record_id)\n",
    "        if baseline_info is None:\n",
    "            continue\n",
    "\n",
    "        baseline_date = baseline_info['baseline_date']\n",
    "        baseline_weight = baseline_info['baseline_weight_kg']\n",
    "        # --- MODIFIED: Get baseline_bmi from prepared_data (sourced from measurement) ---\n",
    "        baseline_bmi = baseline_info.get('baseline_bmi') # Use .get() for safety\n",
    "\n",
    "        # Initialize result dictionary\n",
    "        result = {\n",
    "            # IDs\n",
    "            'patient_ID': patient_id,\n",
    "            'medical_record_ID': medical_record_id,\n",
    "            # Baseline\n",
    "            'baseline_date': baseline_date,\n",
    "            'baseline_weight_kg': baseline_weight,\n",
    "            'baseline_bmi': baseline_bmi, # Add baseline BMI here\n",
    "            # Adherence Proxies\n",
    "            'nr_visits': baseline_info.get('dietitian_visits'),\n",
    "            'nr_total_measurements': num_measurements,\n",
    "            'avg_days_between_measurements': np.nan,\n",
    "            # Confounders / Predictors\n",
    "            'sex_f': baseline_info.get('sex_f'),\n",
    "            'age': baseline_info.get('age_when_creating_record'),\n",
    "            'height_m': baseline_info.get('height_m'),\n",
    "            'hunger_yn': baseline_info.get('hunger_yn'),\n",
    "            'satiety_yn': baseline_info.get('satiety_yn'),\n",
    "            'emotional_eating_yn': baseline_info.get('emotional_eating_yn'),\n",
    "            'emotional_eating_value_likert': baseline_info.get('emotional_eating_value_likert'),\n",
    "            'quantity_control_likert': baseline_info.get('quantity_control_likert'),\n",
    "            'impulse_control_likert': baseline_info.get('impulse_control_likert'),\n",
    "            'weight_gain_cause': baseline_info.get('weight_gain_cause'),\n",
    "            'genomics_sample_id': baseline_info.get('genomics_sample_id')\n",
    "        }\n",
    "\n",
    "        followup_measurements = group[group['measurement_date'] > baseline_date].sort_values('measurement_date')\n",
    "\n",
    "        # 2. Calculate Overall Follow-up Metrics\n",
    "        if not followup_measurements.empty:\n",
    "            last_measurement = followup_measurements.iloc[-1]\n",
    "            result['last_aval_date'] = last_measurement['measurement_date']\n",
    "            result['total_followup_days'] = (last_measurement['measurement_date'] - baseline_date).days + 1\n",
    "            result['last_aval_weight_kg'] = last_measurement['weight_kg']\n",
    "            wl_kg, wl_pct = _calculate_wl_metrics(baseline_weight, last_measurement['weight_kg'])\n",
    "            result['total_wl_kg'] = wl_kg\n",
    "            result['total_wl_%'] = wl_pct\n",
    "            # --- NEW: Get final BMI and calculate reduction ---\n",
    "            result['final_bmi'] = last_measurement.get('bmi') # Get BMI from last measurement\n",
    "            result['bmi_reduction'] = _calculate_bmi_reduction(result['baseline_bmi'], result['final_bmi'])\n",
    "        else:\n",
    "            # Handle instant dropouts\n",
    "            result['last_aval_date'] = baseline_date\n",
    "            result['total_followup_days'] = 1\n",
    "            result['last_aval_weight_kg'] = baseline_weight\n",
    "            result['total_wl_kg'] = 0.0\n",
    "            result['total_wl_%'] = 0.0\n",
    "            # --- NEW: Set final BMI and reduction for instant dropouts ---\n",
    "            result['final_bmi'] = result['baseline_bmi'] # Final BMI is baseline BMI\n",
    "            result['bmi_reduction'] = 0.0 # No change\n",
    "\n",
    "        # Calculate avg_days_between_measurements\n",
    "        if result['nr_total_measurements'] is not None and result['nr_total_measurements'] > 1:\n",
    "             total_days = result['total_followup_days']\n",
    "             result['avg_days_between_measurements'] = round( (total_days -1) / (result['nr_total_measurements'] - 1) , 2) if (result['nr_total_measurements'] - 1) > 0 else np.nan\n",
    "        else:\n",
    "             result['avg_days_between_measurements'] = np.nan\n",
    "\n",
    "\n",
    "        # 3. Calculate Fixed-Timepoint Metrics (for each time window)\n",
    "        # --- MODIFIED: Excluded BMI calculation ---\n",
    "        for window_center in time_windows:\n",
    "            min_window_date = baseline_date + timedelta(days=(window_center - window_span))\n",
    "            max_window_date = baseline_date + timedelta(days=(window_center + window_span))\n",
    "            target_date = baseline_date + timedelta(days=window_center)\n",
    "\n",
    "            measurements_around_cutoff = followup_measurements[\n",
    "                (followup_measurements['measurement_date'] >= min_window_date) &\n",
    "                (followup_measurements['measurement_date'] <= max_window_date)\n",
    "            ]\n",
    "\n",
    "            measurement_for_window = None\n",
    "            is_dropout_at_window = True\n",
    "\n",
    "            if not measurements_around_cutoff.empty:\n",
    "                is_dropout_at_window = False\n",
    "                measurements_around_cutoff = measurements_around_cutoff.copy()\n",
    "                measurements_around_cutoff['distance_to_center'] = abs(\n",
    "                    (measurements_around_cutoff['measurement_date'] - target_date).dt.days\n",
    "                )\n",
    "                closest_measurement_idx = measurements_around_cutoff['distance_to_center'].idxmin()\n",
    "                measurement_for_window = measurements_around_cutoff.loc[closest_measurement_idx]\n",
    "\n",
    "            prefix = f\"{window_center}d\"\n",
    "            result[f'{prefix}_dropout'] = is_dropout_at_window\n",
    "\n",
    "            if is_dropout_at_window:\n",
    "                result[f'{prefix}_weight_kg'] = np.nan\n",
    "                result[f'wl_{prefix}_kg'] = np.nan\n",
    "                result[f'wl_{prefix}_%'] = np.nan\n",
    "                result[f'{prefix}_date'] = pd.NaT\n",
    "                result[f'days_to_{prefix}_measurement'] = np.nan\n",
    "                # Removed: result[f'{prefix}_bmi'] = np.nan\n",
    "            else:\n",
    "                result[f'{prefix}_weight_kg'] = measurement_for_window['weight_kg']\n",
    "                wl_kg, wl_pct = _calculate_wl_metrics(baseline_weight, measurement_for_window['weight_kg'])\n",
    "                result[f'wl_{prefix}_kg'] = wl_kg\n",
    "                result[f'wl_{prefix}_%'] = wl_pct\n",
    "                result[f'{prefix}_date'] = measurement_for_window['measurement_date']\n",
    "                result[f'days_to_{prefix}_measurement'] = (measurement_for_window['measurement_date'] - baseline_date).days + 1\n",
    "                # Removed: result[f'{prefix}_bmi'] = measurement_for_window.get('bmi')\n",
    "\n",
    "\n",
    "        # 4. Calculate Time-to-Event Metrics (for each weight loss target)\n",
    "        # (No changes needed in this section)\n",
    "        for target in weight_loss_targets:\n",
    "            target_achieved = False\n",
    "            first_success_measurement = None\n",
    "            actual_wl_at_success = np.nan\n",
    "\n",
    "            for _, row in followup_measurements.iterrows():\n",
    "                current_weight = row['weight_kg']\n",
    "                if baseline_weight is not None and baseline_weight > 0:\n",
    "                    current_weight_loss_pct = ((baseline_weight - current_weight) / baseline_weight) * 100\n",
    "                    if round(current_weight_loss_pct, 2) >= target:\n",
    "                        target_achieved = True\n",
    "                        first_success_measurement = row\n",
    "                        actual_wl_at_success = round(current_weight_loss_pct, 2)\n",
    "                        break\n",
    "\n",
    "            prefix = f\"{target}%_wl\"\n",
    "            result[f'{prefix}_achieved'] = target_achieved\n",
    "            if target_achieved and first_success_measurement is not None:\n",
    "                result[f'{prefix}_%'] = actual_wl_at_success\n",
    "                result[f'{prefix}_date'] = first_success_measurement['measurement_date']\n",
    "                result[f'days_to_{prefix}'] = (first_success_measurement['measurement_date'] - baseline_date).days + 1\n",
    "            else:\n",
    "                result[f'{prefix}_%'] = np.nan\n",
    "                result[f'{prefix}_date'] = pd.NaT\n",
    "                result[f'days_to_{prefix}'] = np.nan\n",
    "\n",
    "        results_list.append(result)\n",
    "\n",
    "    return pd.DataFrame(results_list)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "MODIFIED: MAIN ORCHESTRATION FUNCTION FOR WIDE TABLE\n",
    "\"\"\"\n",
    "\n",
    "def generate_wide_analysis_dataset(input_connection, output_connection, weight_loss_targets, time_windows, window_span=10):\n",
    "    \"\"\"\n",
    "    Orchestrates the process to generate the single wide survival analysis dataset.\n",
    "    Loads data, prepares patient baseline info, calculates all outcomes per patient,\n",
    "    reorders columns, and saves the resulting wide DataFrame to the output database.\n",
    "    --- MODIFIED: Uses output_column_order ---\n",
    "    \"\"\"\n",
    "    # 1. Load and prepare input data\n",
    "    print(\"Loading measurements...\")\n",
    "    measurements = load_measurements(input_connection)\n",
    "    print(\"Loading medical records...\")\n",
    "    medical_records = load_medical_records(input_connection)\n",
    "    print(\"Preparing patient data...\")\n",
    "    prepared_data, filtered_measurements = prepare_patient_data(measurements, medical_records)\n",
    "\n",
    "    if prepared_data.empty:\n",
    "        print(\"ERROR: Prepared patient data is empty. Cannot proceed.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # 2. Calculate wide outcomes for all patients\n",
    "    print(\"Calculating wide outcomes for all patients...\")\n",
    "    wide_results_df = calculate_wide_patient_outcomes(\n",
    "        prepared_data,\n",
    "        filtered_measurements,\n",
    "        weight_loss_targets,\n",
    "        time_windows,\n",
    "        window_span\n",
    "    )\n",
    "\n",
    "    # --- MODIFIED: Reorder columns according to output_column_order ---\n",
    "    if not wide_results_df.empty:\n",
    "        print(\"Reordering columns...\")\n",
    "        # Ensure all columns in output_column_order exist in the DataFrame\n",
    "        final_columns_present = [col for col in output_column_order if col in wide_results_df.columns]\n",
    "        missing_cols = [col for col in output_column_order if col not in wide_results_df.columns]\n",
    "        if missing_cols:\n",
    "             print(f\"WARN: The following columns defined in output_column_order were not found and will be skipped: {missing_cols}\")\n",
    "        # Add any extra columns not in the defined order to the end\n",
    "        extra_cols = [col for col in wide_results_df.columns if col not in final_columns_present]\n",
    "        if extra_cols:\n",
    "             print(f\"WARN: The following columns were generated but not in output_column_order; adding to the end: {extra_cols}\")\n",
    "\n",
    "        wide_results_df = wide_results_df[final_columns_present + extra_cols]\n",
    "\n",
    "\n",
    "    # 3. Save the single wide table\n",
    "    if not wide_results_df.empty:\n",
    "        print(f\"--- Saving results to output database: {output_db_path} ---\")\n",
    "        print(f\"Saving table: {output_table_name} ({len(wide_results_df)} rows)\")\n",
    "        wide_results_df.to_sql(output_table_name, output_connection, if_exists='replace', index=False)\n",
    "        output_connection.commit()\n",
    "        print(\"--- Wide table saved successfully ---\")\n",
    "    else:\n",
    "        print(\"WARN: No results generated. Output table will be empty or not created.\")\n",
    "\n",
    "    return wide_results_df\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "EXECUTION BLOCK (Modified to use new output names)\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"========== Generating Survival Analysis Input Dataset (Wide Format v3) ==========\") # Updated title\n",
    "    input_conn = None\n",
    "    output_conn = None\n",
    "    try:\n",
    "        print(f\"Connecting to input database: {input_db_path}\")\n",
    "        if not os.path.exists(input_db_path):\n",
    "             raise FileNotFoundError(f\"Input database not found at {input_db_path}\")\n",
    "        input_conn = sqlite3.connect(input_db_path)\n",
    "\n",
    "        print(f\"Connecting to output database: {output_db_path}\")\n",
    "        output_conn = sqlite3.connect(output_db_path)\n",
    "\n",
    "        wide_df = generate_wide_analysis_dataset(\n",
    "            input_conn,\n",
    "            output_conn,\n",
    "            weight_loss_targets,\n",
    "            time_windows,\n",
    "            window_span\n",
    "        )\n",
    "\n",
    "        if not wide_df.empty:\n",
    "            print(\"\\n--- Survival Analysis Input Table Generation Summary ---\")\n",
    "            print(f\"Generated table '{output_table_name}' with {len(wide_df)} rows and {len(wide_df.columns)} columns.\")\n",
    "            # print(wide_df.head().to_string())\n",
    "            print(\"--- End Summary ---\")\n",
    "        else:\n",
    "            print(\"WARN: Analysis completed, but the resulting DataFrame is empty.\")\n",
    "\n",
    "        print(f\"Analysis data saved to {output_db_path}\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"ERROR: Database file not found - {e}\")\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"ERROR: SQLite database error - {e}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"ERROR: Data processing error - {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: An unexpected error occurred - {e}\")\n",
    "        # import traceback\n",
    "        # print(traceback.format_exc())\n",
    "    finally:\n",
    "        print(\"Closing database connections...\")\n",
    "        if input_conn:\n",
    "            input_conn.close()\n",
    "        if output_conn:\n",
    "            output_conn.close()\n",
    "        print(\"========== Survival Analysis Input Data Generation Finished (Wide Format v3) ==========\") # Updated title\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation of summary stats tables in the database - the code is not revised, the output is draft-level, but it is already insightful and seems correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Connecting to database: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\survival_analysis.sqlite\n",
      "Loading input table: sa_input_table\n",
      "Generating population summary data (N=1664)...\n",
      "\n",
      "--- Population Summary (population_summary) ---\n",
      "                                    N         N (%)    Mean (SD)      Median (IQR)     Min - Max\n",
      "Statistic                                                                                       \n",
      "Total Population               1664.0           NaN          NaN               NaN           NaN\n",
      "age                            1664.0           NaN  47.2 ± 10.2  47.0 (41.0-54.0)   18.0 - 83.0\n",
      "height_m                       1664.0           NaN    1.7 ± 0.1     1.6 (1.6-1.7)     1.4 - 2.1\n",
      "baseline_weight_kg             1664.0           NaN  82.8 ± 12.1  82.0 (74.0-89.1)  57.5 - 131.2\n",
      "last_aval_weight_kg            1664.0           NaN  76.3 ± 10.8  74.7 (68.4-82.4)  53.7 - 130.5\n",
      "baseline_bmi                   1664.0           NaN   30.2 ± 3.0  30.1 (27.7-32.5)   25.0 - 41.0\n",
      "final_bmi                      1664.0           NaN   27.8 ± 2.7  27.0 (25.4-29.7)   25.0 - 40.5\n",
      "emotional_eating_value_likert  1664.0           NaN    6.8 ± 2.4     7.0 (5.0-8.0)    1.0 - 10.0\n",
      "quantity_control_likert        1664.0           NaN    5.8 ± 2.4     6.0 (5.0-8.0)    1.0 - 10.0\n",
      "impulse_control_likert         1664.0           NaN    6.0 ± 2.3     6.0 (5.0-8.0)    1.0 - 10.0\n",
      "Sex: Female                       NaN  1368 (82.2%)          NaN               NaN           NaN\n",
      "Hunger: Yes                       NaN  1098 (66.0%)          NaN               NaN           NaN\n",
      "Satiety: Yes                      NaN  1009 (60.6%)          NaN               NaN           NaN\n",
      "Emotional Eating: Yes             NaN  1301 (78.2%)          NaN               NaN           NaN\n",
      "Weight Gain Cause Available       NaN   504 (30.3%)          NaN               NaN           NaN\n",
      "Genomics Sample ID Available      NaN   332 (20.0%)          NaN               NaN           NaN\n",
      "\n",
      "Saving population summary to table: population_summary\n",
      "Generating outcome summary data (N=1664)...\n",
      "\n",
      "--- Outcome Summary (outcome_summary) ---\n",
      "                                    N         N (%)    Mean (SD)      Median (IQR)    Min - Max\n",
      "Statistic                                                                                      \n",
      "Total Population               1664.0           NaN          NaN               NaN          NaN\n",
      "total_followup_days            1664.0           NaN  59.7 ± 72.3  36.0 (13.0-79.0)  1.0 - 632.0\n",
      "nr_visits                      1664.0           NaN    5.1 ± 4.4     4.0 (2.0-7.0)   1.0 - 33.0\n",
      "nr_total_measurements          1664.0           NaN  12.5 ± 15.7    8.0 (3.0-16.0)  1.0 - 188.0\n",
      "avg_days_between_measurements  1499.0           NaN    6.6 ± 5.7     5.5 (3.0-8.0)   0.0 - 60.0\n",
      "total_wl_kg                    1664.0           NaN    6.5 ± 5.6     5.2 (2.2-9.7)  -7.7 - 39.6\n",
      "total_wl_%                     1664.0           NaN    7.6 ± 6.1    6.6 (2.8-11.3)  -9.6 - 36.1\n",
      "bmi_reduction                  1664.0           NaN   -2.4 ± 2.0  -1.9 (-3.5--0.8)  -14.7 - 2.7\n",
      "WL (kg) at 40d [Completers]     874.0           NaN    7.0 ± 3.0               NaN          NaN\n",
      "WL (%) at 40d [Completers]      874.0           NaN    8.1 ± 2.9               NaN          NaN\n",
      "WL (kg) at 60d [Completers]     592.0           NaN    9.2 ± 3.9               NaN          NaN\n",
      "WL (%) at 60d [Completers]      592.0           NaN   10.4 ± 3.8               NaN          NaN\n",
      "WL (kg) at 80d [Completers]     417.0           NaN   10.8 ± 4.3               NaN          NaN\n",
      "WL (%) at 80d [Completers]      417.0           NaN   12.2 ± 4.1               NaN          NaN\n",
      "Days to 5% WL [Achievers]      1034.0           NaN  22.6 ± 14.3               NaN          NaN\n",
      "Days to 10% WL [Achievers]      546.0           NaN  53.2 ± 25.5               NaN          NaN\n",
      "Days to 15% WL [Achievers]      227.0           NaN  88.6 ± 36.0               NaN          NaN\n",
      "Instant Dropouts                  NaN   187 (11.2%)          NaN               NaN          NaN\n",
      "Completers at 40d                 NaN   874 (52.5%)          NaN               NaN          NaN\n",
      "Completers at 60d                 NaN   592 (35.6%)          NaN               NaN          NaN\n",
      "Completers at 80d                 NaN   417 (25.1%)          NaN               NaN          NaN\n",
      "Achieved 5% WL                    NaN  1034 (62.1%)          NaN               NaN          NaN\n",
      "Achieved 10% WL                   NaN   546 (32.8%)          NaN               NaN          NaN\n",
      "Achieved 15% WL                   NaN   227 (13.6%)          NaN               NaN          NaN\n",
      "\n",
      "Saving outcome summary to table: outcome_summary\n",
      "\n",
      "Summary tables saved successfully.\n",
      "Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "# This code calculates detailed population and outcome summary statistics,\n",
    "# ensures variables are columns and stats are rows in the final output,\n",
    "# fixes categorical counts (both population and outcome),\n",
    "# and saves them to the specified table names in the SQLite database.\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "CONFIGURATION\n",
    "\"\"\"\n",
    "# Define database path, input table name, and output table names\n",
    "# Ensure 'paper1_directory' is defined in your notebook environment before running this cell\n",
    "if 'paper1_directory' not in locals() and 'paper1_directory' not in globals():\n",
    "     raise NameError(\"'paper1_directory' is not defined. Please define it in a previous cell.\")\n",
    "\n",
    "db_path = os.path.join(paper1_directory, 'survival_analysis.sqlite') # DB with the final wide table\n",
    "input_table_name = \"sa_input_table\" # The wide table created previously\n",
    "output_pop_summary_table = \"population_summary\"\n",
    "output_outcome_summary_table = \"outcome_summary\"\n",
    "\n",
    "# Define dynamic parameters (should match those used to create sa_input_table)\n",
    "weight_loss_targets = [5, 10, 15]\n",
    "time_windows = [40, 60, 80]\n",
    "\n",
    "\"\"\"\n",
    "HELPER FUNCTIONS\n",
    "\"\"\"\n",
    "\n",
    "def format_mean_sd(series, decimals=1):\n",
    "    \"\"\"Calculates mean and SD, returns formatted string 'mean ± SD'.\"\"\"\n",
    "    numeric_series = pd.to_numeric(series, errors='coerce')\n",
    "    if numeric_series.isnull().all() or numeric_series.empty:\n",
    "        return np.nan\n",
    "    mean = numeric_series.mean()\n",
    "    std = numeric_series.std()\n",
    "    if pd.isna(mean) or pd.isna(std):\n",
    "         return np.nan\n",
    "    return f\"{mean:.{decimals}f} ± {std:.{decimals}f}\"\n",
    "\n",
    "def format_n_percent(series, condition_value, total_n, decimals=1):\n",
    "    \"\"\"Calculates N and % matching a condition (primarily for strings),\n",
    "       returns formatted string 'N (X.X%)'. Case-insensitive for strings.\n",
    "    \"\"\"\n",
    "    if total_n == 0:\n",
    "        return \"0 (NaN%)\"\n",
    "\n",
    "    # Primarily designed for string comparison now\n",
    "    condition_str = str(condition_value).lower()\n",
    "    try:\n",
    "        # Convert series to string, strip whitespace, convert to lower case, and compare\n",
    "        condition_mask = series.astype(str).str.strip().str.lower().eq(condition_str)\n",
    "        n = condition_mask.sum()\n",
    "    except Exception as e: # Broad exception catch if string methods fail\n",
    "        print(f\"  Warning: Could not perform string comparison for series '{series.name}' with value '{condition_value}'. Error: {e}. Falling back to direct comparison.\")\n",
    "        # Fallback for non-string types or errors during string conversion\n",
    "        try:\n",
    "            condition_mask = (series == condition_value)\n",
    "            n = condition_mask.sum()\n",
    "        except TypeError:\n",
    "             print(f\"  Error: Type error during fallback comparison for series '{series.name}' with value '{condition_value}'. Setting N to 0.\")\n",
    "             n = 0\n",
    "        except Exception as e_fallback:\n",
    "             print(f\"  Error: Unexpected error during fallback comparison for series '{series.name}' with value '{condition_value}'. Error: {e_fallback}. Setting N to 0.\")\n",
    "             n = 0\n",
    "\n",
    "    percent = (n / total_n) * 100 if total_n > 0 else 0\n",
    "    return f\"{int(n)} ({percent:.{decimals}f}%)\" # Ensure n is int for formatting\n",
    "\n",
    "\n",
    "def get_describe_stats(df, columns):\n",
    "    \"\"\"Runs describe() and extracts key stats for specified columns.\"\"\"\n",
    "    actual_cols = [col for col in columns if col in df.columns]\n",
    "    if not actual_cols:\n",
    "        return pd.DataFrame()\n",
    "    numeric_subset = df[actual_cols].select_dtypes(include=np.number)\n",
    "    if numeric_subset.empty:\n",
    "        # print(f\"  Warning: No numeric columns found among {actual_cols} for describe.\") # Less verbose\n",
    "        return pd.DataFrame()\n",
    "    described = numeric_subset.describe(percentiles=[.25, .5, .75]).transpose()\n",
    "    stats_to_keep = ['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']\n",
    "    described = described[[col for col in stats_to_keep if col in described.columns]]\n",
    "    return described\n",
    "\n",
    "\"\"\"\n",
    "SUMMARY TABLE GENERATION FUNCTIONS\n",
    "\"\"\"\n",
    "\n",
    "def generate_population_summary(df):\n",
    "    \"\"\"Generates the population summary data (Variables as index, Stats as columns).\"\"\"\n",
    "    # NOTE: This function returns the data BEFORE transposition.\n",
    "    if df.empty:\n",
    "        print(\"Input DataFrame is empty. Cannot generate population summary.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    total_n = len(df)\n",
    "    summary_data_by_var = {}\n",
    "    print(f\"Generating population summary data (N={total_n})...\")\n",
    "\n",
    "    def add_stats(var_name, stats_dict):\n",
    "        if var_name not in summary_data_by_var:\n",
    "            summary_data_by_var[var_name] = {}\n",
    "        summary_data_by_var[var_name].update(stats_dict)\n",
    "\n",
    "    add_stats('Total Population', {'N': total_n})\n",
    "\n",
    "    # --- Numerical Summaries ---\n",
    "    numeric_cols_pop = [\n",
    "        'age', 'height_m', 'baseline_weight_kg', 'last_aval_weight_kg',\n",
    "        'baseline_bmi', 'final_bmi',\n",
    "        'emotional_eating_value_likert', 'quantity_control_likert', 'impulse_control_likert'\n",
    "    ]\n",
    "    pop_described = get_describe_stats(df, numeric_cols_pop)\n",
    "    for col in pop_described.index:\n",
    "        stats_for_col = {}\n",
    "        stats_for_col['N'] = int(pop_described.loc[col, 'count']) if 'count' in pop_described.columns else np.nan\n",
    "        stats_for_col['Mean (SD)'] = format_mean_sd(df[col])\n",
    "        if '50%' in pop_described.columns and '25%' in pop_described.columns and '75%' in pop_described.columns:\n",
    "             median, iqr_25, iqr_75 = pop_described.loc[col, ['50%', '25%', '75%']]\n",
    "             if not pd.isna([median, iqr_25, iqr_75]).any():\n",
    "                  stats_for_col['Median (IQR)'] = f\"{median:.1f} ({iqr_25:.1f}-{iqr_75:.1f})\"\n",
    "             else: stats_for_col['Median (IQR)'] = np.nan\n",
    "        else: stats_for_col['Median (IQR)'] = np.nan\n",
    "        if 'min' in pop_described.columns and 'max' in pop_described.columns:\n",
    "             min_val, max_val = pop_described.loc[col, ['min', 'max']]\n",
    "             if not pd.isna([min_val, max_val]).any():\n",
    "                  stats_for_col['Min - Max'] = f\"{min_val:.1f} - {max_val:.1f}\"\n",
    "             else: stats_for_col['Min - Max'] = np.nan\n",
    "        else: stats_for_col['Min - Max'] = np.nan\n",
    "        add_stats(col, stats_for_col)\n",
    "\n",
    "    # --- Categorical/Boolean Summaries ---\n",
    "    # Sex ('Female') - Use format_n_percent for string check\n",
    "    if 'sex_f' in df.columns:\n",
    "        add_stats('Sex: Female', {'N (%)': format_n_percent(df['sex_f'], '1', total_n)})\n",
    "    else: print(\"  Warning: 'sex_f' column not found.\")\n",
    "\n",
    "    # Yes/No Questions - Use format_n_percent for string check\n",
    "    for col_name, display_name in [('hunger_yn', 'Hunger: Yes'), ('satiety_yn', 'Satiety: Yes'), ('emotional_eating_yn', 'Emotional Eating: Yes')]:\n",
    "        if col_name in df.columns:\n",
    "             add_stats(display_name, {'N (%)': format_n_percent(df[col_name], '1', total_n)})\n",
    "        else: print(f\"  Warning: '{col_name}' column not found.\")\n",
    "\n",
    "    # Availability Checks (Not NULL) - Direct calculation\n",
    "    for col_name, display_name in [('weight_gain_cause', 'Weight Gain Cause Available'), ('genomics_sample_id', 'Genomics Sample ID Available')]:\n",
    "        if col_name in df.columns:\n",
    "            n_not_null = df[col_name].notna().sum()\n",
    "            percent_not_null = (n_not_null / total_n) * 100 if total_n > 0 else 0\n",
    "            add_stats(display_name, {'N (%)': f\"{n_not_null} ({percent_not_null:.1f}%)\"})\n",
    "        else: print(f\"  Warning: '{col_name}' column not found.\")\n",
    "\n",
    "    # Convert dictionary to DataFrame (Variables as index, Stats as columns)\n",
    "    summary_df = pd.DataFrame.from_dict(summary_data_by_var, orient='index')\n",
    "    summary_df.index.name = 'Variable'\n",
    "\n",
    "    # Reorder STAT columns (optional, done before transpose)\n",
    "    desired_col_order = ['N', 'N (%)', 'Mean (SD)', 'Median (IQR)', 'Min - Max']\n",
    "    existing_cols_ordered = [col for col in desired_col_order if col in summary_df.columns]\n",
    "    remaining_cols = [col for col in summary_df.columns if col not in existing_cols_ordered]\n",
    "    final_col_order = existing_cols_ordered + remaining_cols\n",
    "    summary_df = summary_df[final_col_order]\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "\n",
    "def generate_outcome_summary(df, weight_targets, time_windows_list):\n",
    "    \"\"\"Generates the outcome summary data (Variables as index, Stats as columns).\"\"\"\n",
    "    # NOTE: This function returns the data BEFORE transposition.\n",
    "    if df.empty:\n",
    "        print(\"Input DataFrame is empty. Cannot generate outcome summary.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    total_n = len(df)\n",
    "    summary_data_by_var = {}\n",
    "    print(f\"Generating outcome summary data (N={total_n})...\")\n",
    "\n",
    "    def add_stats(var_name, stats_dict):\n",
    "        if var_name not in summary_data_by_var:\n",
    "            summary_data_by_var[var_name] = {}\n",
    "        for key, value in stats_dict.items():\n",
    "            summary_data_by_var[var_name][key] = value\n",
    "\n",
    "    add_stats('Total Population', {'N': total_n})\n",
    "\n",
    "    # --- Overall Adherence & Outcome Summaries ---\n",
    "    numeric_cols_outcome = [\n",
    "        'total_followup_days', 'nr_visits', 'nr_total_measurements', 'avg_days_between_measurements',\n",
    "        'total_wl_kg', 'total_wl_%', 'bmi_reduction'\n",
    "    ]\n",
    "    outcome_described = get_describe_stats(df, numeric_cols_outcome)\n",
    "    for col in outcome_described.index:\n",
    "        stats_for_col = {}\n",
    "        stats_for_col['N'] = int(outcome_described.loc[col, 'count']) if 'count' in outcome_described.columns else np.nan\n",
    "        stats_for_col['Mean (SD)'] = format_mean_sd(df[col])\n",
    "        if '50%' in outcome_described.columns and '25%' in outcome_described.columns and '75%' in outcome_described.columns:\n",
    "             median, iqr_25, iqr_75 = outcome_described.loc[col, ['50%', '25%', '75%']]\n",
    "             if not pd.isna([median, iqr_25, iqr_75]).any():\n",
    "                  stats_for_col['Median (IQR)'] = f\"{median:.1f} ({iqr_25:.1f}-{iqr_75:.1f})\"\n",
    "             else: stats_for_col['Median (IQR)'] = np.nan\n",
    "        else: stats_for_col['Median (IQR)'] = np.nan\n",
    "        if 'min' in outcome_described.columns and 'max' in outcome_described.columns:\n",
    "             min_val, max_val = outcome_described.loc[col, ['min', 'max']]\n",
    "             if not pd.isna([min_val, max_val]).any():\n",
    "                  stats_for_col['Min - Max'] = f\"{min_val:.1f} - {max_val:.1f}\"\n",
    "             else: stats_for_col['Min - Max'] = np.nan\n",
    "        else: stats_for_col['Min - Max'] = np.nan\n",
    "        add_stats(col, stats_for_col)\n",
    "\n",
    "    # --- Specific Outcome Metrics ---\n",
    "    # Instant Dropouts (total_followup_days == 1) - Direct calculation assuming numeric/bool\n",
    "    if 'total_followup_days' in df.columns:\n",
    "        try:\n",
    "            # Attempt direct comparison (works for numbers, might work for bools if 1 used)\n",
    "            instant_dropout_mask = (df['total_followup_days'] == 1)\n",
    "            n_instant_dropout = instant_dropout_mask.sum()\n",
    "            percent_instant = (n_instant_dropout / total_n) * 100 if total_n > 0 else 0\n",
    "            add_stats('Instant Dropouts', {'N (%)': f\"{n_instant_dropout} ({percent_instant:.1f}%)\"})\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Could not calculate Instant Dropouts directly. Error: {e}. Trying format_n_percent.\")\n",
    "            # Fallback to string comparison if direct fails\n",
    "            add_stats('Instant Dropouts', {'N (%)': format_n_percent(df['total_followup_days'], 1, total_n)})\n",
    "    else: print(\"  Warning: 'total_followup_days' column not found for Instant Dropout calculation.\")\n",
    "\n",
    "    # --- Dynamic Time Window Metrics ---\n",
    "    for window in time_windows_list:\n",
    "        dropout_col = f'{window}d_dropout'\n",
    "        wl_kg_col = f'wl_{window}d_kg'\n",
    "        wl_pct_col = f'wl_{window}d_%'\n",
    "        completer_var_name = f'Completers at {window}d'\n",
    "        wl_kg_var_name = f'WL (kg) at {window}d [Completers]'\n",
    "        wl_pct_var_name = f'WL (%) at {window}d [Completers]'\n",
    "\n",
    "        if dropout_col in df.columns:\n",
    "            # N (%) Completers (Not Dropout) - Direct calculation assuming boolean False\n",
    "            try:\n",
    "                completers_mask = (df[dropout_col] == False) # Explicitly check for boolean False\n",
    "                n_completers = completers_mask.sum()\n",
    "                percent_completers = (n_completers / total_n) * 100 if total_n > 0 else 0\n",
    "                add_stats(completer_var_name, {'N (%)': f\"{n_completers} ({percent_completers:.1f}%)\"})\n",
    "\n",
    "                # Mean (SD) Weight Loss for Completers\n",
    "                completers_df = df.loc[completers_mask].copy()\n",
    "                if not completers_df.empty:\n",
    "                    if wl_kg_col in completers_df.columns:\n",
    "                        add_stats(wl_kg_var_name, {\n",
    "                            'Mean (SD)': format_mean_sd(completers_df[wl_kg_col]),\n",
    "                            'N': len(completers_df[wl_kg_col].dropna())\n",
    "                        })\n",
    "                    else:\n",
    "                        print(f\"  Warning: '{wl_kg_col}' column not found.\")\n",
    "                        add_stats(wl_kg_var_name, {'Mean (SD)': np.nan, 'N': 0})\n",
    "                    if wl_pct_col in completers_df.columns:\n",
    "                        add_stats(wl_pct_var_name, {\n",
    "                            'Mean (SD)': format_mean_sd(completers_df[wl_pct_col]),\n",
    "                            'N': len(completers_df[wl_pct_col].dropna())\n",
    "                        })\n",
    "                    else:\n",
    "                        print(f\"  Warning: '{wl_pct_col}' column not found.\")\n",
    "                        add_stats(wl_pct_var_name, {'Mean (SD)': np.nan, 'N': 0})\n",
    "                else:\n",
    "                    # print(f\"  Note: No completers found for {window}d window to calculate WL stats.\") # Less verbose\n",
    "                    add_stats(wl_kg_var_name, {'Mean (SD)': np.nan, 'N': 0})\n",
    "                    add_stats(wl_pct_var_name, {'Mean (SD)': np.nan, 'N': 0})\n",
    "\n",
    "            except Exception as e:\n",
    "                 print(f\"  Error calculating completer stats for {window}d. Column '{dropout_col}' might not be boolean. Error: {e}\")\n",
    "                 add_stats(completer_var_name, {'N (%)': 'Error'})\n",
    "                 add_stats(wl_kg_var_name, {'Mean (SD)': 'Error', 'N': 'Error'})\n",
    "                 add_stats(wl_pct_var_name, {'Mean (SD)': 'Error', 'N': 'Error'})\n",
    "\n",
    "        else:\n",
    "            print(f\"  Warning: Dropout column '{dropout_col}' not found for window {window}d.\")\n",
    "            add_stats(completer_var_name, {'N (%)': np.nan})\n",
    "            add_stats(wl_kg_var_name, {'Mean (SD)': np.nan, 'N': np.nan})\n",
    "            add_stats(wl_pct_var_name, {'Mean (SD)': np.nan, 'N': np.nan})\n",
    "\n",
    "\n",
    "    # --- Dynamic Weight Loss Target Metrics ---\n",
    "    for target in weight_targets:\n",
    "        achieved_col = f'{target}%_wl_achieved'\n",
    "        days_col = f'days_to_{target}%_wl'\n",
    "        achiever_var_name = f'Achieved {target}% WL'\n",
    "        days_var_name = f'Days to {target}% WL [Achievers]'\n",
    "\n",
    "        if achieved_col in df.columns:\n",
    "            # N (%) Achievers - Direct calculation assuming boolean True\n",
    "            try:\n",
    "                achievers_mask = (df[achieved_col] == True) # Explicitly check for boolean True\n",
    "                n_achievers = achievers_mask.sum()\n",
    "                percent_achievers = (n_achievers / total_n) * 100 if total_n > 0 else 0\n",
    "                add_stats(achiever_var_name, {'N (%)': f\"{n_achievers} ({percent_achievers:.1f}%)\"})\n",
    "\n",
    "                # Mean (SD) Days to Achievement (for Achievers)\n",
    "                if days_col in df.columns:\n",
    "                    achievers_df = df.loc[achievers_mask].copy()\n",
    "                    if not achievers_df.empty:\n",
    "                        add_stats(days_var_name, {\n",
    "                            'Mean (SD)': format_mean_sd(achievers_df[days_col]),\n",
    "                            'N': len(achievers_df[days_col].dropna())\n",
    "                        })\n",
    "                    else:\n",
    "                        # print(f\"  Note: No achievers found for {target}% target to calculate days.\") # Less verbose\n",
    "                        add_stats(days_var_name, {'Mean (SD)': np.nan, 'N': 0})\n",
    "                else:\n",
    "                    print(f\"  Warning: Days column '{days_col}' not found for target {target}%.\")\n",
    "                    add_stats(days_var_name, {'Mean (SD)': np.nan, 'N': np.nan})\n",
    "\n",
    "            except Exception as e:\n",
    "                 print(f\"  Error calculating achiever stats for {target}%. Column '{achieved_col}' might not be boolean. Error: {e}\")\n",
    "                 add_stats(achiever_var_name, {'N (%)': 'Error'})\n",
    "                 add_stats(days_var_name, {'Mean (SD)': 'Error', 'N': 'Error'})\n",
    "\n",
    "        else:\n",
    "            print(f\"  Warning: Achievement column '{achieved_col}' not found for target {target}%.\")\n",
    "            add_stats(achiever_var_name, {'N (%)': np.nan})\n",
    "            add_stats(days_var_name, {'Mean (SD)': np.nan, 'N': np.nan})\n",
    "\n",
    "\n",
    "    # Convert dictionary to DataFrame (Variables as index, Stats as columns)\n",
    "    summary_df = pd.DataFrame.from_dict(summary_data_by_var, orient='index')\n",
    "    summary_df.index.name = 'Variable'\n",
    "\n",
    "    # Reorder STAT columns (optional, done before transpose)\n",
    "    desired_col_order = ['N', 'N (%)', 'Mean (SD)', 'Median (IQR)', 'Min - Max']\n",
    "    existing_cols_ordered = [col for col in desired_col_order if col in summary_df.columns]\n",
    "    remaining_cols = [col for col in summary_df.columns if col not in existing_cols_ordered]\n",
    "    final_col_order = existing_cols_ordered + remaining_cols\n",
    "    summary_df = summary_df[final_col_order]\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "MAIN ORCHESTRATION FUNCTION\n",
    "\"\"\"\n",
    "\n",
    "def create_and_save_summary_tables(db_path, input_table, pop_table_out, outcome_table_out, weight_targets, time_windows_list):\n",
    "    \"\"\"Loads data, generates both summary tables, transposes for final output, and saves them.\"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        print(f\"\\nConnecting to database: {db_path}\")\n",
    "        if not os.path.exists(db_path):\n",
    "            print(f\"ERROR: Database file not found at {db_path}\")\n",
    "            return\n",
    "\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        print(f\"Loading input table: {input_table}\")\n",
    "        df = pd.read_sql_query(f\"SELECT * FROM {input_table}\", conn)\n",
    "\n",
    "        if df.empty:\n",
    "            print(f\"Input table '{input_table}' is empty. Cannot generate summaries.\")\n",
    "            return\n",
    "\n",
    "        # --- Generate Population Summary ---\n",
    "        pop_summary_raw = generate_population_summary(df.copy())\n",
    "        if not pop_summary_raw.empty:\n",
    "            print(f\"\\n--- Population Summary ({pop_table_out}) ---\")\n",
    "            # Transpose for final output (Stats as index/rows, Variables as columns)\n",
    "            pop_summary_final = pop_summary_raw #.transpose() OR MAYBE DON'T, Not as nice!\n",
    "            pop_summary_final.index.name = 'Statistic' # Index is now stats\n",
    "            print(pop_summary_final.to_string())\n",
    "            print(f\"\\nSaving population summary to table: {pop_table_out}\")\n",
    "            # Save the transposed DataFrame, index=True saves 'Statistic' column\n",
    "            pop_summary_final.to_sql(pop_table_out, conn, if_exists='replace', index=True)\n",
    "        else:\n",
    "            print(\"Population summary generation failed or resulted in an empty table.\")\n",
    "\n",
    "        # --- Generate Outcome Summary ---\n",
    "        outcome_summary_raw = generate_outcome_summary(df.copy(), weight_targets, time_windows_list)\n",
    "        if not outcome_summary_raw.empty:\n",
    "            print(f\"\\n--- Outcome Summary ({outcome_table_out}) ---\")\n",
    "            # Transpose for final output (Stats as index/rows, Variables as columns)\n",
    "\n",
    "            outcome_summary_final = outcome_summary_raw # .transpose() OR MAYBE DON'T, it is not as nice\n",
    "            outcome_summary_final.index.name = 'Statistic' # Index is now stats\n",
    "            print(outcome_summary_final.to_string())\n",
    "            print(f\"\\nSaving outcome summary to table: {outcome_table_out}\")\n",
    "            # Save the transposed DataFrame, index=True saves 'Statistic' column\n",
    "            outcome_summary_final.to_sql(outcome_table_out, conn, if_exists='replace', index=True)\n",
    "        else:\n",
    "            print(\"Outcome summary generation failed or resulted in an empty table.\")\n",
    "\n",
    "        conn.commit()\n",
    "        print(\"\\nSummary tables saved successfully.\")\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"ERROR: SQLite error - {e}\")\n",
    "        if conn: conn.rollback()\n",
    "    except pd.errors.DatabaseError as e:\n",
    "         print(f\"ERROR: Pandas/Database error during SQL operation - {e}\")\n",
    "         if conn: conn.rollback()\n",
    "    except KeyError as e:\n",
    "        print(f\"ERROR: A required column name was not found in the DataFrame: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: An unexpected error occurred - {e}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "        if conn: conn.rollback()\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "            print(\"Database connection closed.\")\n",
    "\n",
    "\"\"\"\n",
    "EXECUTION\n",
    "\"\"\"\n",
    "# Make sure 'paper1_directory' is defined before this point!\n",
    "if 'paper1_directory' in locals() or 'paper1_directory' in globals():\n",
    "    create_and_save_summary_tables(\n",
    "        db_path,\n",
    "        input_table_name,\n",
    "        output_pop_summary_table,\n",
    "        output_outcome_summary_table,\n",
    "        weight_loss_targets,\n",
    "        time_windows\n",
    "    )\n",
    "else:\n",
    "    print(\"ERROR: 'paper1_directory' variable is not defined. Please define it before running this cell.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To-do: 2-split segmented population descriptions; multiple linear regressions to predict WL from EE; multiple logistic regressions to predict WL target success from EE; KM plots stratified by EE groups; maybe more? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparative summary stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanation of the first draft pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "Configuration:\n",
    "\n",
    "rows_config: Defines each variable (row) and specifies if it's continuous or categorical, and how it should be formatted (mean_sd or n_perc). The special N row uses n_perc_pop.\n",
    "strata_config: Defines each stratification (column group). It specifies the column name in the DataFrame, the type of stratification (median_split, value_split, binary, isna), the cutoff value (for value_split), and the desired labels for the two resulting groups.\n",
    "Helper Functions:\n",
    "\n",
    "format_mean_sd: Calculates and formats mean ± standard deviation.\n",
    "format_n_perc: Calculates N and % for the positive class (1 or True) in binary/boolean columns.\n",
    "format_n_perc_pop: Calculates N and % relative to the total input population size (for the 'N' row).\n",
    "calculate_p_value: Performs Welch's t-test (equal_var=False) for continuous variables or Chi-squared test for categorical variables. It checks the expected frequencies from chi2_contingency and issues a warnings.warn if any expected count is < 5.\n",
    "format_p_value: Formats the p-value for display (e.g., \"<0.001\").\n",
    "generate_comparative_summary Function:\n",
    "\n",
    "Takes the input DataFrame (df), rows_config, and strata_config.\n",
    "Calculates total_population_n.\n",
    "Pre-calculates Stratification Columns: It iterates through strata_config first to create temporary binary (0/1) columns in the DataFrame (_strata_...) based on the specified type (median split, value split, binary check, isna check). This avoids recalculating medians repeatedly. It stores the mapping between the stratification name and the temporary column name.\n",
    "Iterates through Stratifications: For each defined stratification:\n",
    "It retrieves the pre-calculated stratification column and labels.\n",
    "It splits the DataFrame into group0_df and group1_df based on the stratification column, dropping rows where the stratification value is missing.\n",
    "Iterates through Row Variables: For each variable defined in rows_config:\n",
    "It calculates the statistic (Mean±SD or N(%)) for both group0_df and group1_df using the appropriate helper function.\n",
    "It calculates the p-value comparing the two groups using calculate_p_value.\n",
    "It stores the formatted stats and p-value in the summary_results dictionary.\n",
    "Formats Output: Converts the summary_results dictionary into a pandas DataFrame, reorders the columns logically (Group 0, Group 1, p-value for each stratification), ensures the row order matches rows_config, and returns the final table.\n",
    "Example Usage (if __name__ == \"__main__\":)\n",
    "\n",
    "LOAD YOUR DATA: Replace the example SQLite loading with how you actually load your sa_input_table DataFrame.\n",
    "Preprocessing: Includes crucial steps to:\n",
    "Ensure boolean-like columns are consistently represented as numeric (0.0/1.0) to handle potential NaNs. You might need to adjust the .map() dictionary if your string representations are different.\n",
    "Create the genomics_available column based on whether genomics_sample_id is null/NaN.\n",
    "Create the instant_dropout column.\n",
    "Ensure continuous columns are numeric, coercing errors to NaN.\n",
    "Calls generate_comparative_summary.\n",
    "Prints the resulting DataFrame (using options to show all rows/columns)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code of the first draft pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 'paper1_directory' not defined. Using current directory for output.\n",
      "Connecting to input database: .\\survival_analysis.sqlite\n",
      "Loading input table: sa_input_table\n",
      "Starting data preprocessing...\n",
      "Preprocessing finished.\n",
      "Generating comparative summary table...\n",
      "Comparative Summary Table generated.\n",
      "                               Age: < Median Age: >= Median Age: p-value  \\\n",
      "Variable                                                                   \n",
      "N                                762 (45.8%)    902 (54.2%)          N/A   \n",
      "age                             38.52 ± 6.38   54.54 ± 6.22       <0.001   \n",
      "sex_f                            628 (82.4%)    740 (82.0%)        0.893   \n",
      "height_m                         1.67 ± 0.08    1.64 ± 0.08       <0.001   \n",
      "baseline_weight_kg             84.01 ± 11.91  81.86 ± 12.27       <0.001   \n",
      "baseline_bmi                    30.13 ± 3.00   30.21 ± 3.05        0.609   \n",
      "hunger_yn                        501 (65.7%)    597 (66.2%)        0.892   \n",
      "satiety_yn                       456 (59.8%)    553 (61.3%)        0.576   \n",
      "emotional_eating_yn              613 (80.4%)    688 (76.3%)        0.046   \n",
      "emotional_eating_value_likert    6.96 ± 2.34    6.68 ± 2.37        0.015   \n",
      "quantity_control_likert          5.97 ± 2.38    5.70 ± 2.44        0.020   \n",
      "impulse_control_likert           6.08 ± 2.31    5.94 ± 2.38        0.231   \n",
      "total_followup_days            51.09 ± 64.14  66.92 ± 77.87       <0.001   \n",
      "avg_days_between_measurements    6.16 ± 5.67    6.88 ± 5.80        0.016   \n",
      "genomics_available               146 (19.2%)    186 (20.6%)        0.496   \n",
      "total_wl_%                       7.15 ± 5.69    7.90 ± 6.34        0.011   \n",
      "bmi_reduction                   -2.22 ± 1.89   -2.47 ± 2.13        0.010   \n",
      "instant_dropout                   83 (10.9%)    104 (11.5%)        0.740   \n",
      "40d_dropout                      404 (53.0%)    386 (42.8%)       <0.001   \n",
      "wl_40d_%                         8.17 ± 2.93    8.02 ± 2.94        0.458   \n",
      "60d_dropout                      543 (71.3%)    529 (58.6%)       <0.001   \n",
      "wl_60d_%                        10.47 ± 3.94   10.39 ± 3.74        0.800   \n",
      "80d_dropout                      611 (80.2%)    636 (70.5%)       <0.001   \n",
      "wl_80d_%                        12.23 ± 4.47   12.25 ± 3.86        0.973   \n",
      "5%_wl_achieved                   462 (60.6%)    572 (63.4%)        0.264   \n",
      "days_to_5%_wl                  21.10 ± 14.52  23.83 ± 14.06        0.002   \n",
      "10%_wl_achieved                  216 (28.3%)    330 (36.6%)       <0.001   \n",
      "days_to_10%_wl                 51.12 ± 28.02  54.57 ± 23.70        0.137   \n",
      "15%_wl_achieved                   80 (10.5%)    147 (16.3%)       <0.001   \n",
      "days_to_15%_wl                 85.55 ± 37.94  90.18 ± 34.93        0.368   \n",
      "\n",
      "                               Sex: Male (0) Sex: Female (1) Sex: p-value  \\\n",
      "Variable                                                                    \n",
      "N                                296 (17.8%)    1368 (82.2%)          N/A   \n",
      "age                            47.02 ± 10.23   47.24 ± 10.15        0.737   \n",
      "sex_f                               0 (0.0%)   1368 (100.0%)       <0.001   \n",
      "height_m                         1.76 ± 0.07     1.63 ± 0.06       <0.001   \n",
      "baseline_weight_kg             99.08 ± 12.58    79.33 ± 8.72       <0.001   \n",
      "baseline_bmi                    31.91 ± 3.64    29.79 ± 2.74       <0.001   \n",
      "hunger_yn                        197 (66.6%)     901 (65.9%)        0.873   \n",
      "satiety_yn                       166 (56.1%)     843 (61.6%)        0.088   \n",
      "emotional_eating_yn              223 (75.3%)    1078 (78.8%)        0.218   \n",
      "emotional_eating_value_likert    6.28 ± 2.47     6.93 ± 2.32       <0.001   \n",
      "quantity_control_likert          6.11 ± 2.40     5.76 ± 2.42        0.023   \n",
      "impulse_control_likert           5.92 ± 2.44     6.03 ± 2.33        0.503   \n",
      "total_followup_days            62.58 ± 61.69   59.04 ± 74.42        0.390   \n",
      "avg_days_between_measurements    5.13 ± 4.18     6.88 ± 6.00       <0.001   \n",
      "genomics_available                56 (18.9%)     276 (20.2%)        0.682   \n",
      "total_wl_%                       9.63 ± 6.44     7.11 ± 5.89       <0.001   \n",
      "bmi_reduction                   -3.16 ± 2.35    -2.18 ± 1.91       <0.001   \n",
      "instant_dropout                    18 (6.1%)     169 (12.4%)        0.003   \n",
      "40d_dropout                      112 (37.8%)     678 (49.6%)       <0.001   \n",
      "wl_40d_%                         9.88 ± 3.16     7.61 ± 2.68       <0.001   \n",
      "60d_dropout                      170 (57.4%)     902 (65.9%)        0.007   \n",
      "wl_60d_%                        12.78 ± 3.71     9.78 ± 3.58       <0.001   \n",
      "80d_dropout                      213 (72.0%)    1034 (75.6%)        0.218   \n",
      "wl_80d_%                        14.66 ± 4.67    11.64 ± 3.70       <0.001   \n",
      "5%_wl_achieved                   221 (74.7%)     813 (59.4%)       <0.001   \n",
      "days_to_5%_wl                   16.59 ± 8.80   24.25 ± 15.08       <0.001   \n",
      "10%_wl_achieved                  135 (45.6%)     411 (30.0%)       <0.001   \n",
      "days_to_10%_wl                 41.80 ± 26.92   56.95 ± 23.93       <0.001   \n",
      "15%_wl_achieved                   61 (20.6%)     166 (12.1%)       <0.001   \n",
      "days_to_15%_wl                 68.48 ± 33.63   95.93 ± 34.07       <0.001   \n",
      "\n",
      "                              Height: < Median Height: >= Median  \\\n",
      "Variable                                                           \n",
      "N                                  816 (49.0%)       848 (51.0%)   \n",
      "age                              48.69 ± 10.46      45.78 ± 9.66   \n",
      "sex_f                              804 (98.5%)       564 (66.5%)   \n",
      "height_m                           1.59 ± 0.04       1.72 ± 0.06   \n",
      "baseline_weight_kg                76.05 ± 7.97     89.38 ± 11.89   \n",
      "baseline_bmi                      30.06 ± 2.99      30.27 ± 3.07   \n",
      "hunger_yn                          522 (64.0%)       576 (67.9%)   \n",
      "satiety_yn                         508 (62.3%)       501 (59.1%)   \n",
      "emotional_eating_yn                639 (78.3%)       662 (78.1%)   \n",
      "emotional_eating_value_likert      6.87 ± 2.33       6.75 ± 2.38   \n",
      "quantity_control_likert            5.59 ± 2.41       6.04 ± 2.41   \n",
      "impulse_control_likert             5.92 ± 2.33       6.09 ± 2.37   \n",
      "total_followup_days              61.58 ± 77.48     57.83 ± 66.98   \n",
      "avg_days_between_measurements      7.04 ± 6.20       6.10 ± 5.25   \n",
      "genomics_available                 162 (19.9%)       170 (20.0%)   \n",
      "total_wl_%                         7.35 ± 6.28       7.76 ± 5.85   \n",
      "bmi_reduction                     -2.29 ± 2.09      -2.42 ± 1.96   \n",
      "instant_dropout                    105 (12.9%)         82 (9.7%)   \n",
      "40d_dropout                        405 (49.6%)       385 (45.4%)   \n",
      "wl_40d_%                           7.73 ± 2.74       8.40 ± 3.06   \n",
      "60d_dropout                        526 (64.5%)       546 (64.4%)   \n",
      "wl_60d_%                           9.85 ± 3.69      10.96 ± 3.85   \n",
      "80d_dropout                        603 (73.9%)       644 (75.9%)   \n",
      "wl_80d_%                          11.96 ± 3.95      12.53 ± 4.21   \n",
      "5%_wl_achieved                     488 (59.8%)       546 (64.4%)   \n",
      "days_to_5%_wl                    23.73 ± 14.01     21.61 ± 14.54   \n",
      "10%_wl_achieved                    250 (30.6%)       296 (34.9%)   \n",
      "days_to_10%_wl                   56.92 ± 25.36     50.06 ± 25.29   \n",
      "15%_wl_achieved                    112 (13.7%)       115 (13.6%)   \n",
      "days_to_15%_wl                   93.52 ± 35.56     83.71 ± 35.93   \n",
      "\n",
      "                              Height: p-value Baseline Weight: < Median  \\\n",
      "Variable                                                                  \n",
      "N                                         N/A               830 (49.9%)   \n",
      "age                                    <0.001              48.19 ± 9.98   \n",
      "sex_f                                  <0.001               804 (96.9%)   \n",
      "height_m                               <0.001               1.61 ± 0.06   \n",
      "baseline_weight_kg                     <0.001              73.51 ± 5.51   \n",
      "baseline_bmi                            0.161              28.29 ± 2.30   \n",
      "hunger_yn                               0.099               531 (64.0%)   \n",
      "satiety_yn                              0.202               519 (62.5%)   \n",
      "emotional_eating_yn                     0.952               630 (75.9%)   \n",
      "emotional_eating_value_likert           0.318               6.71 ± 2.35   \n",
      "quantity_control_likert                <0.001               5.65 ± 2.40   \n",
      "impulse_control_likert                  0.136               5.97 ± 2.30   \n",
      "total_followup_days                     0.292             44.22 ± 64.23   \n",
      "avg_days_between_measurements           0.002               6.21 ± 5.47   \n",
      "genomics_available                      0.970               164 (19.8%)   \n",
      "total_wl_%                              0.164               5.84 ± 5.06   \n",
      "bmi_reduction                           0.185              -1.71 ± 1.57   \n",
      "instant_dropout                         0.047               140 (16.9%)   \n",
      "40d_dropout                             0.093               503 (60.6%)   \n",
      "wl_40d_%                               <0.001               7.88 ± 2.59   \n",
      "60d_dropout                             1.000               635 (76.5%)   \n",
      "wl_60d_%                               <0.001               9.72 ± 3.77   \n",
      "80d_dropout                             0.365               707 (85.2%)   \n",
      "wl_80d_%                                0.157              11.48 ± 3.72   \n",
      "5%_wl_achieved                          0.061               440 (53.0%)   \n",
      "days_to_5%_wl                           0.017             21.31 ± 13.40   \n",
      "10%_wl_achieved                         0.072               174 (21.0%)   \n",
      "days_to_10%_wl                          0.002             52.79 ± 22.88   \n",
      "15%_wl_achieved                         0.979                 50 (6.0%)   \n",
      "days_to_15%_wl                          0.040             86.22 ± 34.73   \n",
      "\n",
      "                              Baseline Weight: >= Median  \\\n",
      "Variable                                                   \n",
      "N                                            834 (50.1%)   \n",
      "age                                        46.23 ± 10.26   \n",
      "sex_f                                        564 (67.6%)   \n",
      "height_m                                     1.70 ± 0.08   \n",
      "baseline_weight_kg                          92.13 ± 9.55   \n",
      "baseline_bmi                                32.04 ± 2.45   \n",
      "hunger_yn                                    567 (68.0%)   \n",
      "satiety_yn                                   490 (58.8%)   \n",
      "emotional_eating_yn                          671 (80.5%)   \n",
      "emotional_eating_value_likert                6.91 ± 2.37   \n",
      "quantity_control_likert                      6.00 ± 2.42   \n",
      "impulse_control_likert                       6.05 ± 2.39   \n",
      "total_followup_days                        75.04 ± 76.56   \n",
      "avg_days_between_measurements                6.86 ± 5.97   \n",
      "genomics_available                           168 (20.1%)   \n",
      "total_wl_%                                   9.27 ± 6.48   \n",
      "bmi_reduction                               -3.00 ± 2.22   \n",
      "instant_dropout                                47 (5.6%)   \n",
      "40d_dropout                                  287 (34.4%)   \n",
      "wl_40d_%                                     8.21 ± 3.12   \n",
      "60d_dropout                                  437 (52.4%)   \n",
      "wl_60d_%                                    10.76 ± 3.79   \n",
      "80d_dropout                                  540 (64.7%)   \n",
      "wl_80d_%                                    12.56 ± 4.19   \n",
      "5%_wl_achieved                               594 (71.2%)   \n",
      "days_to_5%_wl                              23.57 ± 14.90   \n",
      "10%_wl_achieved                              372 (44.6%)   \n",
      "days_to_10%_wl                             53.40 ± 26.71   \n",
      "15%_wl_achieved                              177 (21.2%)   \n",
      "days_to_15%_wl                             89.21 ± 36.43   \n",
      "\n",
      "                              Baseline Weight: p-value Baseline BMI: < 30  \\\n",
      "Variable                                                                    \n",
      "N                                                  N/A        802 (48.2%)   \n",
      "age                                             <0.001      47.07 ± 10.07   \n",
      "sex_f                                           <0.001        702 (87.5%)   \n",
      "height_m                                        <0.001        1.65 ± 0.07   \n",
      "baseline_weight_kg                              <0.001       75.32 ± 8.11   \n",
      "baseline_bmi                                    <0.001       27.55 ± 1.40   \n",
      "hunger_yn                                        0.094        516 (64.3%)   \n",
      "satiety_yn                                       0.127        496 (61.8%)   \n",
      "emotional_eating_yn                              0.029        603 (75.2%)   \n",
      "emotional_eating_value_likert                    0.076        6.62 ± 2.39   \n",
      "quantity_control_likert                          0.004        5.67 ± 2.42   \n",
      "impulse_control_likert                           0.511        5.94 ± 2.36   \n",
      "total_followup_days                             <0.001      37.20 ± 54.99   \n",
      "avg_days_between_measurements                    0.026        5.64 ± 5.32   \n",
      "genomics_available                               0.893        156 (19.5%)   \n",
      "total_wl_%                                      <0.001        5.53 ± 4.29   \n",
      "bmi_reduction                                   <0.001       -1.56 ± 1.26   \n",
      "instant_dropout                                 <0.001        128 (16.0%)   \n",
      "40d_dropout                                     <0.001        505 (63.0%)   \n",
      "wl_40d_%                                         0.100        7.90 ± 2.56   \n",
      "60d_dropout                                     <0.001        649 (80.9%)   \n",
      "wl_60d_%                                         0.002        9.33 ± 3.59   \n",
      "80d_dropout                                     <0.001        724 (90.3%)   \n",
      "wl_80d_%                                         0.010       10.43 ± 3.11   \n",
      "5%_wl_achieved                                  <0.001        423 (52.7%)   \n",
      "days_to_5%_wl                                    0.011      19.87 ± 12.48   \n",
      "10%_wl_achieved                                 <0.001        149 (18.6%)   \n",
      "days_to_10%_wl                                   0.786      52.03 ± 30.93   \n",
      "15%_wl_achieved                                 <0.001          12 (1.5%)   \n",
      "days_to_15%_wl                                   0.596     103.33 ± 56.44   \n",
      "\n",
      "                              Baseline BMI: >= 30 Baseline BMI: p-value  \\\n",
      "Variable                                                                  \n",
      "N                                     862 (51.8%)                   N/A   \n",
      "age                                 47.32 ± 10.26                 0.616   \n",
      "sex_f                                 666 (77.3%)                <0.001   \n",
      "height_m                              1.66 ± 0.09                 0.107   \n",
      "baseline_weight_kg                  89.84 ± 11.06                <0.001   \n",
      "baseline_bmi                         32.61 ± 1.89                <0.001   \n",
      "hunger_yn                             582 (67.5%)                 0.188   \n",
      "satiety_yn                            513 (59.5%)                 0.356   \n",
      "emotional_eating_yn                   698 (81.0%)                 0.005   \n",
      "emotional_eating_value_likert         6.99 ± 2.31                 0.002   \n",
      "quantity_control_likert               5.96 ± 2.41                 0.015   \n",
      "impulse_control_likert                6.07 ± 2.34                 0.248   \n",
      "total_followup_days                 80.57 ± 79.88                <0.001   \n",
      "avg_days_between_measurements         7.33 ± 5.98                <0.001   \n",
      "genomics_available                    176 (20.4%)                 0.666   \n",
      "total_wl_%                            9.45 ± 6.81                <0.001   \n",
      "bmi_reduction                        -3.10 ± 2.31                <0.001   \n",
      "instant_dropout                         59 (6.8%)                <0.001   \n",
      "40d_dropout                           285 (33.1%)                <0.001   \n",
      "wl_40d_%                              8.18 ± 3.11                 0.160   \n",
      "60d_dropout                           423 (49.1%)                <0.001   \n",
      "wl_60d_%                             10.80 ± 3.82                <0.001   \n",
      "80d_dropout                           523 (60.7%)                <0.001   \n",
      "wl_80d_%                             12.66 ± 4.17                <0.001   \n",
      "5%_wl_achieved                        611 (70.9%)                <0.001   \n",
      "days_to_5%_wl                       24.51 ± 15.19                <0.001   \n",
      "10%_wl_achieved                       397 (46.1%)                <0.001   \n",
      "days_to_10%_wl                      53.64 ± 23.21                 0.565   \n",
      "15%_wl_achieved                       215 (24.9%)                <0.001   \n",
      "days_to_15%_wl                      87.73 ± 34.53                 0.363   \n",
      "\n",
      "                              Total WL%: < Median Total WL%: >= Median  \\\n",
      "Variable                                                                 \n",
      "N                                     832 (50.0%)          832 (50.0%)   \n",
      "age                                 46.84 ± 10.08        47.56 ± 10.24   \n",
      "sex_f                                 731 (87.9%)          637 (76.6%)   \n",
      "height_m                              1.65 ± 0.08          1.66 ± 0.08   \n",
      "baseline_weight_kg                  79.78 ± 11.78        85.91 ± 11.74   \n",
      "baseline_bmi                         29.25 ± 3.16         31.09 ± 2.59   \n",
      "hunger_yn                             550 (66.1%)          548 (65.9%)   \n",
      "satiety_yn                            503 (60.5%)          506 (60.8%)   \n",
      "emotional_eating_yn                   645 (77.5%)          656 (78.8%)   \n",
      "emotional_eating_value_likert         6.81 ± 2.38          6.81 ± 2.34   \n",
      "quantity_control_likert               5.76 ± 2.41          5.88 ± 2.42   \n",
      "impulse_control_likert                5.89 ± 2.32          6.13 ± 2.37   \n",
      "total_followup_days                 24.93 ± 47.11        94.40 ± 76.36   \n",
      "avg_days_between_measurements         6.45 ± 6.67          6.63 ± 4.88   \n",
      "genomics_available                    145 (17.4%)          187 (22.5%)   \n",
      "total_wl_%                            2.74 ± 2.23         12.38 ± 4.70   \n",
      "bmi_reduction                        -0.81 ± 0.68         -3.90 ± 1.73   \n",
      "instant_dropout                       187 (22.5%)             0 (0.0%)   \n",
      "40d_dropout                           671 (80.6%)          119 (14.3%)   \n",
      "wl_40d_%                              4.41 ± 2.29          8.92 ± 2.37   \n",
      "60d_dropout                           757 (91.0%)          315 (37.9%)   \n",
      "wl_60d_%                              4.53 ± 3.07         11.27 ± 3.08   \n",
      "80d_dropout                           796 (95.7%)          451 (54.2%)   \n",
      "wl_80d_%                              5.51 ± 3.12         12.88 ± 3.56   \n",
      "5%_wl_achieved                        202 (24.3%)         832 (100.0%)   \n",
      "days_to_5%_wl                       27.15 ± 19.52        21.51 ± 12.51   \n",
      "10%_wl_achieved                          7 (0.8%)          539 (64.8%)   \n",
      "days_to_10%_wl                      51.29 ± 17.62        53.23 ± 25.63   \n",
      "15%_wl_achieved                          2 (0.2%)          225 (27.0%)   \n",
      "days_to_15%_wl                       88.00 ± 2.83        88.56 ± 36.17   \n",
      "\n",
      "                              Total WL%: p-value BMI Reduction: < Median  \\\n",
      "Variable                                                                   \n",
      "N                                            N/A             831 (49.9%)   \n",
      "age                                        0.148           47.53 ± 10.25   \n",
      "sex_f                                     <0.001             635 (76.4%)   \n",
      "height_m                                   0.012             1.66 ± 0.08   \n",
      "baseline_weight_kg                        <0.001           86.34 ± 11.64   \n",
      "baseline_bmi                              <0.001            31.26 ± 2.52   \n",
      "hunger_yn                                  0.959             545 (65.6%)   \n",
      "satiety_yn                                 0.920             501 (60.3%)   \n",
      "emotional_eating_yn                        0.553             653 (78.6%)   \n",
      "emotional_eating_value_likert              0.967             6.82 ± 2.31   \n",
      "quantity_control_likert                    0.301             5.89 ± 2.41   \n",
      "impulse_control_likert                     0.038             6.12 ± 2.38   \n",
      "total_followup_days                       <0.001           94.93 ± 74.74   \n",
      "avg_days_between_measurements              0.551             6.69 ± 4.91   \n",
      "genomics_available                         0.012             187 (22.5%)   \n",
      "total_wl_%                                <0.001            12.36 ± 4.73   \n",
      "bmi_reduction                             <0.001            -3.92 ± 1.71   \n",
      "instant_dropout                           <0.001                0 (0.0%)   \n",
      "40d_dropout                               <0.001             111 (13.4%)   \n",
      "wl_40d_%                                  <0.001             8.88 ± 2.40   \n",
      "60d_dropout                               <0.001             309 (37.2%)   \n",
      "wl_60d_%                                  <0.001            11.22 ± 3.13   \n",
      "80d_dropout                               <0.001             445 (53.5%)   \n",
      "wl_80d_%                                  <0.001            12.79 ± 3.64   \n",
      "5%_wl_achieved                            <0.001            831 (100.0%)   \n",
      "days_to_5%_wl                             <0.001           21.93 ± 13.00   \n",
      "10%_wl_achieved                           <0.001             540 (65.0%)   \n",
      "days_to_10%_wl                             0.783           53.22 ± 25.60   \n",
      "15%_wl_achieved                           <0.001             226 (27.2%)   \n",
      "days_to_15%_wl                             0.865           88.54 ± 36.09   \n",
      "\n",
      "                              BMI Reduction: >= Median BMI Reduction: p-value  \\\n",
      "Variable                                                                        \n",
      "N                                          833 (50.1%)                    N/A   \n",
      "age                                      46.88 ± 10.07                  0.186   \n",
      "sex_f                                      733 (88.0%)                 <0.001   \n",
      "height_m                                   1.65 ± 0.08                  0.010   \n",
      "baseline_weight_kg                       79.35 ± 11.63                 <0.001   \n",
      "baseline_bmi                              29.09 ± 3.10                 <0.001   \n",
      "hunger_yn                                  553 (66.4%)                  0.769   \n",
      "satiety_yn                                 508 (61.0%)                  0.810   \n",
      "emotional_eating_yn                        648 (77.8%)                  0.741   \n",
      "emotional_eating_value_likert              6.80 ± 2.40                  0.904   \n",
      "quantity_control_likert                    5.76 ± 2.42                  0.293   \n",
      "impulse_control_likert                     5.89 ± 2.31                  0.046   \n",
      "total_followup_days                      24.49 ± 48.99                 <0.001   \n",
      "avg_days_between_measurements              6.37 ± 6.64                  0.299   \n",
      "genomics_available                         145 (17.4%)                  0.011   \n",
      "total_wl_%                                 2.76 ± 2.27                 <0.001   \n",
      "bmi_reduction                             -0.80 ± 0.65                 <0.001   \n",
      "instant_dropout                            187 (22.4%)                 <0.001   \n",
      "40d_dropout                                679 (81.5%)                 <0.001   \n",
      "wl_40d_%                                   4.38 ± 2.29                 <0.001   \n",
      "60d_dropout                                763 (91.6%)                 <0.001   \n",
      "wl_60d_%                                   4.46 ± 3.10                 <0.001   \n",
      "80d_dropout                                802 (96.3%)                 <0.001   \n",
      "wl_80d_%                                   5.34 ± 2.82                 <0.001   \n",
      "5%_wl_achieved                             203 (24.4%)                 <0.001   \n",
      "days_to_5%_wl                            25.38 ± 18.56                  0.013   \n",
      "10%_wl_achieved                               6 (0.7%)                 <0.001   \n",
      "days_to_10%_wl                           51.67 ± 19.27                  0.852   \n",
      "15%_wl_achieved                               1 (0.1%)                 <0.001   \n",
      "days_to_15%_wl                             90.00 ± nan                    N/A   \n",
      "\n",
      "                              Hunger: No (0) Hunger: Yes (1) Hunger: p-value  \\\n",
      "Variable                                                                       \n",
      "N                                566 (34.0%)    1098 (66.0%)             N/A   \n",
      "age                            47.50 ± 10.00   47.05 ± 10.25           0.394   \n",
      "sex_f                            467 (82.5%)     901 (82.1%)           0.873   \n",
      "height_m                         1.65 ± 0.08     1.66 ± 0.08           0.224   \n",
      "baseline_weight_kg             82.30 ± 11.83   83.12 ± 12.31           0.186   \n",
      "baseline_bmi                    30.09 ± 2.96    30.21 ± 3.07           0.460   \n",
      "hunger_yn                           0 (0.0%)   1098 (100.0%)          <0.001   \n",
      "satiety_yn                       279 (49.3%)     730 (66.5%)          <0.001   \n",
      "emotional_eating_yn              386 (68.2%)     915 (83.3%)          <0.001   \n",
      "emotional_eating_value_likert    6.59 ± 2.63     6.93 ± 2.20           0.009   \n",
      "quantity_control_likert          5.61 ± 2.48     5.93 ± 2.37           0.011   \n",
      "impulse_control_likert           5.81 ± 2.42     6.11 ± 2.31           0.016   \n",
      "total_followup_days            62.28 ± 81.33   58.32 ± 67.21           0.320   \n",
      "avg_days_between_measurements    6.40 ± 4.92     6.63 ± 6.13           0.427   \n",
      "genomics_available               114 (20.1%)     218 (19.9%)           0.941   \n",
      "total_wl_%                       7.54 ± 5.89     7.57 ± 6.15           0.940   \n",
      "bmi_reduction                   -2.34 ± 1.95    -2.37 ± 2.07           0.770   \n",
      "instant_dropout                   63 (11.1%)     124 (11.3%)           0.986   \n",
      "40d_dropout                      270 (47.7%)     520 (47.4%)           0.935   \n",
      "wl_40d_%                         8.11 ± 2.87     8.07 ± 2.97           0.839   \n",
      "60d_dropout                      358 (63.3%)     714 (65.0%)           0.507   \n",
      "wl_60d_%                        10.04 ± 3.81    10.62 ± 3.80           0.080   \n",
      "80d_dropout                      430 (76.0%)     817 (74.4%)           0.524   \n",
      "wl_80d_%                        12.04 ± 3.85    12.34 ± 4.20           0.466   \n",
      "5%_wl_achieved                   370 (65.4%)     664 (60.5%)           0.058   \n",
      "days_to_5%_wl                  22.56 ± 15.13   22.64 ± 13.86           0.934   \n",
      "10%_wl_achieved                  180 (31.8%)     366 (33.3%)           0.565   \n",
      "days_to_10%_wl                 51.99 ± 24.55   53.80 ± 26.01           0.429   \n",
      "15%_wl_achieved                   75 (13.3%)     152 (13.8%)           0.796   \n",
      "days_to_15%_wl                 93.56 ± 41.26   86.08 ± 32.98           0.173   \n",
      "\n",
      "                              Satiety: No (0) Satiety: Yes (1)  \\\n",
      "Variable                                                         \n",
      "N                                 655 (39.4%)     1009 (60.6%)   \n",
      "age                             47.12 ± 10.23    47.26 ± 10.12   \n",
      "sex_f                             525 (80.2%)      843 (83.5%)   \n",
      "height_m                          1.66 ± 0.08      1.65 ± 0.08   \n",
      "baseline_weight_kg              83.59 ± 12.10    82.36 ± 12.17   \n",
      "baseline_bmi                     30.26 ± 2.99     30.11 ± 3.05   \n",
      "hunger_yn                         368 (56.2%)      730 (72.3%)   \n",
      "satiety_yn                           0 (0.0%)    1009 (100.0%)   \n",
      "emotional_eating_yn               519 (79.2%)      782 (77.5%)   \n",
      "emotional_eating_value_likert     7.05 ± 2.35      6.66 ± 2.35   \n",
      "quantity_control_likert           5.84 ± 2.50      5.81 ± 2.36   \n",
      "impulse_control_likert            5.87 ± 2.46      6.10 ± 2.27   \n",
      "total_followup_days             65.55 ± 78.70    55.85 ± 67.62   \n",
      "avg_days_between_measurements     6.77 ± 5.87      6.41 ± 5.67   \n",
      "genomics_available                132 (20.2%)      200 (19.8%)   \n",
      "total_wl_%                        7.72 ± 6.20      7.45 ± 5.97   \n",
      "bmi_reduction                    -2.43 ± 2.09     -2.31 ± 1.99   \n",
      "instant_dropout                    73 (11.1%)      114 (11.3%)   \n",
      "40d_dropout                       302 (46.1%)      488 (48.4%)   \n",
      "wl_40d_%                          8.08 ± 2.91      8.09 ± 2.95   \n",
      "60d_dropout                       398 (60.8%)      674 (66.8%)   \n",
      "wl_60d_%                         10.34 ± 3.90     10.48 ± 3.74   \n",
      "80d_dropout                       475 (72.5%)      772 (76.5%)   \n",
      "wl_80d_%                         12.18 ± 4.15     12.28 ± 4.04   \n",
      "5%_wl_achieved                    419 (64.0%)      615 (61.0%)   \n",
      "days_to_5%_wl                   23.39 ± 16.43    22.08 ± 12.67   \n",
      "10%_wl_achieved                   223 (34.0%)      323 (32.0%)   \n",
      "days_to_10%_wl                  55.09 ± 30.65    51.90 ± 21.24   \n",
      "15%_wl_achieved                    96 (14.7%)      131 (13.0%)   \n",
      "days_to_15%_wl                  90.11 ± 37.32    87.40 ± 35.11   \n",
      "\n",
      "                              Satiety: p-value Emotional Eating YN: No (0)  \\\n",
      "Variable                                                                     \n",
      "N                                          N/A                 363 (21.8%)   \n",
      "age                                      0.787               48.21 ± 10.24   \n",
      "sex_f                                    0.088                 290 (79.9%)   \n",
      "height_m                                 0.024                 1.66 ± 0.09   \n",
      "baseline_weight_kg                       0.042               81.70 ± 13.28   \n",
      "baseline_bmi                             0.320                29.64 ± 3.07   \n",
      "hunger_yn                               <0.001                 183 (50.4%)   \n",
      "satiety_yn                              <0.001                 227 (62.5%)   \n",
      "emotional_eating_yn                      0.438                    0 (0.0%)   \n",
      "emotional_eating_value_likert           <0.001                 4.12 ± 2.03   \n",
      "quantity_control_likert                  0.810                 4.91 ± 2.12   \n",
      "impulse_control_likert                   0.056                 5.03 ± 2.16   \n",
      "total_followup_days                      0.010               56.74 ± 71.88   \n",
      "avg_days_between_measurements            0.232                 5.99 ± 4.96   \n",
      "genomics_available                       0.919                  73 (20.1%)   \n",
      "total_wl_%                               0.379                 7.46 ± 6.00   \n",
      "bmi_reduction                            0.257                -2.31 ± 2.00   \n",
      "instant_dropout                          0.986                  45 (12.4%)   \n",
      "40d_dropout                              0.395                 185 (51.0%)   \n",
      "wl_40d_%                                 0.971                 8.34 ± 2.74   \n",
      "60d_dropout                              0.014                 238 (65.6%)   \n",
      "wl_60d_%                                 0.644                10.80 ± 3.52   \n",
      "80d_dropout                              0.075                 279 (76.9%)   \n",
      "wl_80d_%                                 0.809                12.77 ± 3.40   \n",
      "5%_wl_achieved                           0.235                 222 (61.2%)   \n",
      "days_to_5%_wl                            0.168               20.99 ± 13.47   \n",
      "10%_wl_achieved                          0.418                 116 (32.0%)   \n",
      "days_to_10%_wl                           0.179               49.47 ± 18.18   \n",
      "15%_wl_achieved                          0.369                  48 (13.2%)   \n",
      "days_to_15%_wl                           0.580               88.65 ± 38.11   \n",
      "\n",
      "                              Emotional Eating YN: Yes (1)  \\\n",
      "Variable                                                     \n",
      "N                                             1301 (78.2%)   \n",
      "age                                          46.92 ± 10.13   \n",
      "sex_f                                         1078 (82.9%)   \n",
      "height_m                                       1.65 ± 0.08   \n",
      "baseline_weight_kg                           83.16 ± 11.80   \n",
      "baseline_bmi                                  30.32 ± 3.00   \n",
      "hunger_yn                                      915 (70.3%)   \n",
      "satiety_yn                                     782 (60.1%)   \n",
      "emotional_eating_yn                          1301 (100.0%)   \n",
      "emotional_eating_value_likert                  7.56 ± 1.84   \n",
      "quantity_control_likert                        6.08 ± 2.43   \n",
      "impulse_control_likert                         6.28 ± 2.33   \n",
      "total_followup_days                          60.48 ± 72.45   \n",
      "avg_days_between_measurements                  6.71 ± 5.94   \n",
      "genomics_available                             259 (19.9%)   \n",
      "total_wl_%                                     7.59 ± 6.08   \n",
      "bmi_reduction                                 -2.37 ± 2.03   \n",
      "instant_dropout                                142 (10.9%)   \n",
      "40d_dropout                                    605 (46.5%)   \n",
      "wl_40d_%                                       8.02 ± 2.98   \n",
      "60d_dropout                                    834 (64.1%)   \n",
      "wl_60d_%                                      10.32 ± 3.88   \n",
      "80d_dropout                                    968 (74.4%)   \n",
      "wl_80d_%                                      12.11 ± 4.23   \n",
      "5%_wl_achieved                                 812 (62.4%)   \n",
      "days_to_5%_wl                                23.05 ± 14.52   \n",
      "10%_wl_achieved                                430 (33.1%)   \n",
      "days_to_10%_wl                               54.21 ± 27.10   \n",
      "15%_wl_achieved                                179 (13.8%)   \n",
      "days_to_15%_wl                               88.53 ± 35.53   \n",
      "\n",
      "                              Emotional Eating YN: p-value  \\\n",
      "Variable                                                     \n",
      "N                                                      N/A   \n",
      "age                                                  0.034   \n",
      "sex_f                                                0.218   \n",
      "height_m                                             0.560   \n",
      "baseline_weight_kg                                   0.057   \n",
      "baseline_bmi                                        <0.001   \n",
      "hunger_yn                                           <0.001   \n",
      "satiety_yn                                           0.438   \n",
      "emotional_eating_yn                                 <0.001   \n",
      "emotional_eating_value_likert                       <0.001   \n",
      "quantity_control_likert                             <0.001   \n",
      "impulse_control_likert                              <0.001   \n",
      "total_followup_days                                  0.381   \n",
      "avg_days_between_measurements                        0.027   \n",
      "genomics_available                                   0.991   \n",
      "total_wl_%                                           0.720   \n",
      "bmi_reduction                                        0.632   \n",
      "instant_dropout                                      0.486   \n",
      "40d_dropout                                          0.148   \n",
      "wl_40d_%                                             0.178   \n",
      "60d_dropout                                          0.651   \n",
      "wl_60d_%                                             0.185   \n",
      "80d_dropout                                          0.376   \n",
      "wl_80d_%                                             0.130   \n",
      "5%_wl_achieved                                       0.707   \n",
      "days_to_5%_wl                                        0.047   \n",
      "10%_wl_achieved                                      0.741   \n",
      "days_to_10%_wl                                       0.027   \n",
      "15%_wl_achieved                                      0.860   \n",
      "days_to_15%_wl                                       0.984   \n",
      "\n",
      "                              Emotional Eating Likert: < Median  \\\n",
      "Variable                                                          \n",
      "N                                                   629 (37.8%)   \n",
      "age                                               48.17 ± 10.07   \n",
      "sex_f                                               488 (77.6%)   \n",
      "height_m                                            1.66 ± 0.08   \n",
      "baseline_weight_kg                                82.44 ± 12.77   \n",
      "baseline_bmi                                       29.91 ± 3.12   \n",
      "hunger_yn                                           381 (60.6%)   \n",
      "satiety_yn                                          404 (64.2%)   \n",
      "emotional_eating_yn                                 302 (48.0%)   \n",
      "emotional_eating_value_likert                       4.31 ± 1.65   \n",
      "quantity_control_likert                             4.88 ± 2.08   \n",
      "impulse_control_likert                              5.10 ± 2.02   \n",
      "total_followup_days                               61.00 ± 74.66   \n",
      "avg_days_between_measurements                       6.24 ± 5.33   \n",
      "genomics_available                                  126 (20.0%)   \n",
      "total_wl_%                                          7.85 ± 6.17   \n",
      "bmi_reduction                                      -2.44 ± 2.06   \n",
      "instant_dropout                                      71 (11.3%)   \n",
      "40d_dropout                                         297 (47.2%)   \n",
      "wl_40d_%                                            8.34 ± 2.86   \n",
      "60d_dropout                                         390 (62.0%)   \n",
      "wl_60d_%                                           10.60 ± 3.62   \n",
      "80d_dropout                                         466 (74.1%)   \n",
      "wl_80d_%                                           12.46 ± 3.65   \n",
      "5%_wl_achieved                                      399 (63.4%)   \n",
      "days_to_5%_wl                                     21.40 ± 13.73   \n",
      "10%_wl_achieved                                     221 (35.1%)   \n",
      "days_to_10%_wl                                    51.35 ± 22.52   \n",
      "15%_wl_achieved                                      89 (14.1%)   \n",
      "days_to_15%_wl                                    87.55 ± 37.25   \n",
      "\n",
      "                              Emotional Eating Likert: >= Median  \\\n",
      "Variable                                                           \n",
      "N                                                   1035 (62.2%)   \n",
      "age                                                46.62 ± 10.18   \n",
      "sex_f                                                880 (85.0%)   \n",
      "height_m                                             1.65 ± 0.08   \n",
      "baseline_weight_kg                                 83.09 ± 11.76   \n",
      "baseline_bmi                                        30.33 ± 2.96   \n",
      "hunger_yn                                            717 (69.3%)   \n",
      "satiety_yn                                           605 (58.5%)   \n",
      "emotional_eating_yn                                  999 (96.5%)   \n",
      "emotional_eating_value_likert                        8.33 ± 1.07   \n",
      "quantity_control_likert                              6.40 ± 2.43   \n",
      "impulse_control_likert                               6.56 ± 2.36   \n",
      "total_followup_days                                58.85 ± 70.88   \n",
      "avg_days_between_measurements                        6.74 ± 5.98   \n",
      "genomics_available                                   206 (19.9%)   \n",
      "total_wl_%                                           7.38 ± 5.99   \n",
      "bmi_reduction                                       -2.31 ± 2.00   \n",
      "instant_dropout                                      116 (11.2%)   \n",
      "40d_dropout                                          493 (47.6%)   \n",
      "wl_40d_%                                             7.93 ± 2.97   \n",
      "60d_dropout                                          682 (65.9%)   \n",
      "wl_60d_%                                            10.30 ± 3.94   \n",
      "80d_dropout                                          781 (75.5%)   \n",
      "wl_80d_%                                            12.10 ± 4.34   \n",
      "5%_wl_achieved                                       635 (61.4%)   \n",
      "days_to_5%_wl                                      23.37 ± 14.64   \n",
      "10%_wl_achieved                                      325 (31.4%)   \n",
      "days_to_10%_wl                                     54.46 ± 27.35   \n",
      "15%_wl_achieved                                      138 (13.3%)   \n",
      "days_to_15%_wl                                     89.20 ± 35.30   \n",
      "\n",
      "                              Emotional Eating Likert: p-value  \\\n",
      "Variable                                                         \n",
      "N                                                          N/A   \n",
      "age                                                      0.002   \n",
      "sex_f                                                   <0.001   \n",
      "height_m                                                 0.273   \n",
      "baseline_weight_kg                                       0.296   \n",
      "baseline_bmi                                             0.006   \n",
      "hunger_yn                                               <0.001   \n",
      "satiety_yn                                               0.022   \n",
      "emotional_eating_yn                                     <0.001   \n",
      "emotional_eating_value_likert                           <0.001   \n",
      "quantity_control_likert                                 <0.001   \n",
      "impulse_control_likert                                  <0.001   \n",
      "total_followup_days                                      0.562   \n",
      "avg_days_between_measurements                            0.095   \n",
      "genomics_available                                       1.000   \n",
      "total_wl_%                                               0.129   \n",
      "bmi_reduction                                            0.202   \n",
      "instant_dropout                                          1.000   \n",
      "40d_dropout                                              0.909   \n",
      "wl_40d_%                                                 0.041   \n",
      "60d_dropout                                              0.120   \n",
      "wl_60d_%                                                 0.332   \n",
      "80d_dropout                                              0.570   \n",
      "wl_80d_%                                                 0.356   \n",
      "5%_wl_achieved                                           0.426   \n",
      "days_to_5%_wl                                            0.029   \n",
      "10%_wl_achieved                                          0.129   \n",
      "days_to_10%_wl                                           0.148   \n",
      "15%_wl_achieved                                          0.692   \n",
      "days_to_15%_wl                                           0.741   \n",
      "\n",
      "                              Quantity Control Likert: < Median  \\\n",
      "Variable                                                          \n",
      "N                                                   753 (45.3%)   \n",
      "age                                               47.69 ± 10.15   \n",
      "sex_f                                               625 (83.0%)   \n",
      "height_m                                            1.65 ± 0.08   \n",
      "baseline_weight_kg                                81.93 ± 11.83   \n",
      "baseline_bmi                                       29.98 ± 3.10   \n",
      "hunger_yn                                           465 (61.8%)   \n",
      "satiety_yn                                          438 (58.2%)   \n",
      "emotional_eating_yn                                 517 (68.7%)   \n",
      "emotional_eating_value_likert                       6.08 ± 2.47   \n",
      "quantity_control_likert                             3.65 ± 1.55   \n",
      "impulse_control_likert                              4.76 ± 2.16   \n",
      "total_followup_days                               59.49 ± 72.93   \n",
      "avg_days_between_measurements                       6.41 ± 5.42   \n",
      "genomics_available                                  141 (18.7%)   \n",
      "total_wl_%                                          7.55 ± 6.33   \n",
      "bmi_reduction                                      -2.36 ± 2.14   \n",
      "instant_dropout                                      84 (11.2%)   \n",
      "40d_dropout                                         361 (47.9%)   \n",
      "wl_40d_%                                            8.08 ± 2.90   \n",
      "60d_dropout                                         475 (63.1%)   \n",
      "wl_60d_%                                           10.27 ± 3.94   \n",
      "80d_dropout                                         568 (75.4%)   \n",
      "wl_80d_%                                           12.56 ± 3.91   \n",
      "5%_wl_achieved                                      454 (60.3%)   \n",
      "days_to_5%_wl                                     22.38 ± 13.37   \n",
      "10%_wl_achieved                                     249 (33.1%)   \n",
      "days_to_10%_wl                                    52.13 ± 21.15   \n",
      "15%_wl_achieved                                      99 (13.1%)   \n",
      "days_to_15%_wl                                    85.41 ± 32.61   \n",
      "\n",
      "                              Quantity Control Likert: >= Median  \\\n",
      "Variable                                                           \n",
      "N                                                    911 (54.7%)   \n",
      "age                                                46.80 ± 10.16   \n",
      "sex_f                                                743 (81.6%)   \n",
      "height_m                                             1.66 ± 0.08   \n",
      "baseline_weight_kg                                 83.60 ± 12.37   \n",
      "baseline_bmi                                        30.33 ± 2.96   \n",
      "hunger_yn                                            633 (69.5%)   \n",
      "satiety_yn                                           571 (62.7%)   \n",
      "emotional_eating_yn                                  784 (86.1%)   \n",
      "emotional_eating_value_likert                        7.42 ± 2.07   \n",
      "quantity_control_likert                              7.62 ± 1.24   \n",
      "impulse_control_likert                               7.04 ± 1.96   \n",
      "total_followup_days                                59.81 ± 71.85   \n",
      "avg_days_between_measurements                        6.67 ± 6.00   \n",
      "genomics_available                                   191 (21.0%)   \n",
      "total_wl_%                                           7.56 ± 5.84   \n",
      "bmi_reduction                                       -2.36 ± 1.92   \n",
      "instant_dropout                                      103 (11.3%)   \n",
      "40d_dropout                                          429 (47.1%)   \n",
      "wl_40d_%                                             8.09 ± 2.97   \n",
      "60d_dropout                                          597 (65.5%)   \n",
      "wl_60d_%                                            10.55 ± 3.69   \n",
      "80d_dropout                                          679 (74.5%)   \n",
      "wl_80d_%                                            11.98 ± 4.21   \n",
      "5%_wl_achieved                                       580 (63.7%)   \n",
      "days_to_5%_wl                                      22.79 ± 15.03   \n",
      "10%_wl_achieved                                      297 (32.6%)   \n",
      "days_to_10%_wl                                     54.10 ± 28.69   \n",
      "15%_wl_achieved                                      128 (14.1%)   \n",
      "days_to_15%_wl                                     90.98 ± 38.38   \n",
      "\n",
      "                              Quantity Control Likert: p-value  \\\n",
      "Variable                                                         \n",
      "N                                                          N/A   \n",
      "age                                                      0.076   \n",
      "sex_f                                                    0.483   \n",
      "height_m                                                 0.093   \n",
      "baseline_weight_kg                                       0.005   \n",
      "baseline_bmi                                             0.022   \n",
      "hunger_yn                                                0.001   \n",
      "satiety_yn                                               0.068   \n",
      "emotional_eating_yn                                     <0.001   \n",
      "emotional_eating_value_likert                           <0.001   \n",
      "quantity_control_likert                                 <0.001   \n",
      "impulse_control_likert                                  <0.001   \n",
      "total_followup_days                                      0.929   \n",
      "avg_days_between_measurements                            0.393   \n",
      "genomics_available                                       0.282   \n",
      "total_wl_%                                               0.988   \n",
      "bmi_reduction                                            0.974   \n",
      "instant_dropout                                          0.985   \n",
      "40d_dropout                                              0.767   \n",
      "wl_40d_%                                                 0.980   \n",
      "60d_dropout                                              0.323   \n",
      "wl_60d_%                                                 0.385   \n",
      "80d_dropout                                              0.716   \n",
      "wl_80d_%                                                 0.145   \n",
      "5%_wl_achieved                                           0.173   \n",
      "days_to_5%_wl                                            0.641   \n",
      "10%_wl_achieved                                          0.881   \n",
      "days_to_10%_wl                                           0.357   \n",
      "15%_wl_achieved                                          0.644   \n",
      "days_to_15%_wl                                           0.240   \n",
      "\n",
      "                              Impulse Control Likert: < Median  \\\n",
      "Variable                                                         \n",
      "N                                                  705 (42.4%)   \n",
      "age                                              47.61 ± 10.42   \n",
      "sex_f                                              575 (81.6%)   \n",
      "height_m                                           1.66 ± 0.08   \n",
      "baseline_weight_kg                               82.44 ± 12.17   \n",
      "baseline_bmi                                      30.02 ± 3.08   \n",
      "hunger_yn                                          428 (60.7%)   \n",
      "satiety_yn                                         398 (56.5%)   \n",
      "emotional_eating_yn                                470 (66.7%)   \n",
      "emotional_eating_value_likert                      5.99 ± 2.47   \n",
      "quantity_control_likert                            4.47 ± 2.13   \n",
      "impulse_control_likert                             3.76 ± 1.47   \n",
      "total_followup_days                              59.31 ± 73.31   \n",
      "avg_days_between_measurements                      6.06 ± 4.70   \n",
      "genomics_available                                 130 (18.4%)   \n",
      "total_wl_%                                         7.41 ± 6.10   \n",
      "bmi_reduction                                     -2.31 ± 2.05   \n",
      "instant_dropout                                     76 (10.8%)   \n",
      "40d_dropout                                        338 (47.9%)   \n",
      "wl_40d_%                                           8.06 ± 2.91   \n",
      "60d_dropout                                        451 (64.0%)   \n",
      "wl_60d_%                                          10.13 ± 3.92   \n",
      "80d_dropout                                        537 (76.2%)   \n",
      "wl_80d_%                                          12.07 ± 3.93   \n",
      "5%_wl_achieved                                     430 (61.0%)   \n",
      "days_to_5%_wl                                    22.06 ± 13.58   \n",
      "10%_wl_achieved                                    220 (31.2%)   \n",
      "days_to_10%_wl                                   51.27 ± 20.52   \n",
      "15%_wl_achieved                                     91 (12.9%)   \n",
      "days_to_15%_wl                                   87.03 ± 32.41   \n",
      "\n",
      "                              Impulse Control Likert: >= Median  \\\n",
      "Variable                                                          \n",
      "N                                                   959 (57.6%)   \n",
      "age                                                46.91 ± 9.96   \n",
      "sex_f                                               793 (82.7%)   \n",
      "height_m                                            1.65 ± 0.08   \n",
      "baseline_weight_kg                                83.14 ± 12.14   \n",
      "baseline_bmi                                       30.28 ± 2.98   \n",
      "hunger_yn                                           670 (69.9%)   \n",
      "satiety_yn                                          611 (63.7%)   \n",
      "emotional_eating_yn                                 831 (86.7%)   \n",
      "emotional_eating_value_likert                       7.42 ± 2.07   \n",
      "quantity_control_likert                             6.82 ± 2.11   \n",
      "impulse_control_likert                              7.66 ± 1.25   \n",
      "total_followup_days                               59.93 ± 71.62   \n",
      "avg_days_between_measurements                       6.92 ± 6.39   \n",
      "genomics_available                                  202 (21.1%)   \n",
      "total_wl_%                                          7.67 ± 6.04   \n",
      "bmi_reduction                                      -2.39 ± 2.01   \n",
      "instant_dropout                                     111 (11.6%)   \n",
      "40d_dropout                                         452 (47.1%)   \n",
      "wl_40d_%                                            8.10 ± 2.95   \n",
      "60d_dropout                                         621 (64.8%)   \n",
      "wl_60d_%                                           10.64 ± 3.72   \n",
      "80d_dropout                                         710 (74.0%)   \n",
      "wl_80d_%                                           12.36 ± 4.19   \n",
      "5%_wl_achieved                                      604 (63.0%)   \n",
      "days_to_5%_wl                                     23.00 ± 14.82   \n",
      "10%_wl_achieved                                     326 (34.0%)   \n",
      "days_to_10%_wl                                    54.51 ± 28.37   \n",
      "15%_wl_achieved                                     136 (14.2%)   \n",
      "days_to_15%_wl                                    89.57 ± 38.31   \n",
      "\n",
      "                              Impulse Control Likert: p-value  \\\n",
      "Variable                                                        \n",
      "N                                                         N/A   \n",
      "age                                                     0.167   \n",
      "sex_f                                                   0.596   \n",
      "height_m                                                0.889   \n",
      "baseline_weight_kg                                      0.241   \n",
      "baseline_bmi                                            0.086   \n",
      "hunger_yn                                              <0.001   \n",
      "satiety_yn                                              0.003   \n",
      "emotional_eating_yn                                    <0.001   \n",
      "emotional_eating_value_likert                          <0.001   \n",
      "quantity_control_likert                                <0.001   \n",
      "impulse_control_likert                                 <0.001   \n",
      "total_followup_days                                     0.862   \n",
      "avg_days_between_measurements                           0.003   \n",
      "genomics_available                                      0.207   \n",
      "total_wl_%                                              0.393   \n",
      "bmi_reduction                                           0.448   \n",
      "instant_dropout                                         0.668   \n",
      "40d_dropout                                             0.781   \n",
      "wl_40d_%                                                0.834   \n",
      "60d_dropout                                             0.781   \n",
      "wl_60d_%                                                0.111   \n",
      "80d_dropout                                             0.349   \n",
      "wl_80d_%                                                0.473   \n",
      "5%_wl_achieved                                          0.438   \n",
      "days_to_5%_wl                                           0.293   \n",
      "10%_wl_achieved                                         0.253   \n",
      "days_to_10%_wl                                          0.122   \n",
      "15%_wl_achieved                                         0.499   \n",
      "days_to_15%_wl                                          0.592   \n",
      "\n",
      "                              Followup Duration: <= 10 Days  \\\n",
      "Variable                                                      \n",
      "N                                               382 (23.0%)   \n",
      "age                                           46.75 ± 10.10   \n",
      "sex_f                                           337 (88.2%)   \n",
      "height_m                                        1.65 ± 0.08   \n",
      "baseline_weight_kg                            77.69 ± 12.49   \n",
      "baseline_bmi                                   28.47 ± 3.23   \n",
      "hunger_yn                                       260 (68.1%)   \n",
      "satiety_yn                                      241 (63.1%)   \n",
      "emotional_eating_yn                             290 (75.9%)   \n",
      "emotional_eating_value_likert                   6.78 ± 2.31   \n",
      "quantity_control_likert                         5.65 ± 2.45   \n",
      "impulse_control_likert                          5.84 ± 2.29   \n",
      "total_followup_days                             3.75 ± 3.17   \n",
      "avg_days_between_measurements                   2.96 ± 2.40   \n",
      "genomics_available                               56 (14.7%)   \n",
      "total_wl_%                                      1.41 ± 1.68   \n",
      "bmi_reduction                                  -0.40 ± 0.49   \n",
      "instant_dropout                                 187 (49.0%)   \n",
      "40d_dropout                                    382 (100.0%)   \n",
      "wl_40d_%                                                N/A   \n",
      "60d_dropout                                    382 (100.0%)   \n",
      "wl_60d_%                                                N/A   \n",
      "80d_dropout                                    382 (100.0%)   \n",
      "wl_80d_%                                                N/A   \n",
      "5%_wl_achieved                                     7 (1.8%)   \n",
      "days_to_5%_wl                                   7.29 ± 1.38   \n",
      "10%_wl_achieved                                    0 (0.0%)   \n",
      "days_to_10%_wl                                          N/A   \n",
      "15%_wl_achieved                                    0 (0.0%)   \n",
      "days_to_15%_wl                                          N/A   \n",
      "\n",
      "                              Followup Duration: > 10 Days  \\\n",
      "Variable                                                     \n",
      "N                                             1282 (77.0%)   \n",
      "age                                          47.34 ± 10.18   \n",
      "sex_f                                         1031 (80.4%)   \n",
      "height_m                                       1.66 ± 0.08   \n",
      "baseline_weight_kg                           84.38 ± 11.62   \n",
      "baseline_bmi                                  30.68 ± 2.77   \n",
      "hunger_yn                                      838 (65.4%)   \n",
      "satiety_yn                                     768 (59.9%)   \n",
      "emotional_eating_yn                           1011 (78.9%)   \n",
      "emotional_eating_value_likert                  6.82 ± 2.37   \n",
      "quantity_control_likert                        5.88 ± 2.40   \n",
      "impulse_control_likert                         6.06 ± 2.36   \n",
      "total_followup_days                          76.33 ± 74.68   \n",
      "avg_days_between_measurements                  7.16 ± 5.92   \n",
      "genomics_available                             276 (21.5%)   \n",
      "total_wl_%                                     9.39 ± 5.68   \n",
      "bmi_reduction                                 -2.94 ± 1.95   \n",
      "instant_dropout                                   0 (0.0%)   \n",
      "40d_dropout                                    408 (31.8%)   \n",
      "wl_40d_%                                       8.09 ± 2.93   \n",
      "60d_dropout                                    690 (53.8%)   \n",
      "wl_60d_%                                      10.42 ± 3.81   \n",
      "80d_dropout                                    865 (67.5%)   \n",
      "wl_80d_%                                      12.24 ± 4.09   \n",
      "5%_wl_achieved                                1027 (80.1%)   \n",
      "days_to_5%_wl                                22.71 ± 14.31   \n",
      "10%_wl_achieved                                546 (42.6%)   \n",
      "days_to_10%_wl                               53.20 ± 25.53   \n",
      "15%_wl_achieved                                227 (17.7%)   \n",
      "days_to_15%_wl                               88.55 ± 36.01   \n",
      "\n",
      "                              Followup Duration: p-value  \\\n",
      "Variable                                                   \n",
      "N                                                    N/A   \n",
      "age                                                0.322   \n",
      "sex_f                                             <0.001   \n",
      "height_m                                           0.146   \n",
      "baseline_weight_kg                                <0.001   \n",
      "baseline_bmi                                      <0.001   \n",
      "hunger_yn                                          0.360   \n",
      "satiety_yn                                         0.290   \n",
      "emotional_eating_yn                                0.249   \n",
      "emotional_eating_value_likert                      0.746   \n",
      "quantity_control_likert                            0.107   \n",
      "impulse_control_likert                             0.106   \n",
      "total_followup_days                               <0.001   \n",
      "avg_days_between_measurements                     <0.001   \n",
      "genomics_available                                 0.004   \n",
      "total_wl_%                                        <0.001   \n",
      "bmi_reduction                                     <0.001   \n",
      "instant_dropout                                   <0.001   \n",
      "40d_dropout                                       <0.001   \n",
      "wl_40d_%                                             N/A   \n",
      "60d_dropout                                       <0.001   \n",
      "wl_60d_%                                             N/A   \n",
      "80d_dropout                                       <0.001   \n",
      "wl_80d_%                                             N/A   \n",
      "5%_wl_achieved                                    <0.001   \n",
      "days_to_5%_wl                                     <0.001   \n",
      "10%_wl_achieved                                   <0.001   \n",
      "days_to_10%_wl                                       N/A   \n",
      "15%_wl_achieved                                   <0.001   \n",
      "days_to_15%_wl                                       N/A   \n",
      "\n",
      "                              Measurement Frequency: < Median  \\\n",
      "Variable                                                        \n",
      "N                                                 908 (54.6%)   \n",
      "age                                             46.25 ± 10.16   \n",
      "sex_f                                             717 (79.0%)   \n",
      "height_m                                          1.66 ± 0.08   \n",
      "baseline_weight_kg                              81.91 ± 12.64   \n",
      "baseline_bmi                                     29.51 ± 3.05   \n",
      "hunger_yn                                         608 (67.0%)   \n",
      "satiety_yn                                        569 (62.7%)   \n",
      "emotional_eating_yn                               692 (76.2%)   \n",
      "emotional_eating_value_likert                     6.71 ± 2.35   \n",
      "quantity_control_likert                           5.83 ± 2.44   \n",
      "impulse_control_likert                            5.95 ± 2.37   \n",
      "total_followup_days                             37.18 ± 47.01   \n",
      "avg_days_between_measurements                     2.90 ± 1.44   \n",
      "genomics_available                                162 (17.8%)   \n",
      "total_wl_%                                        6.26 ± 5.64   \n",
      "bmi_reduction                                    -1.93 ± 1.85   \n",
      "instant_dropout                                   187 (20.6%)   \n",
      "40d_dropout                                       549 (60.5%)   \n",
      "wl_40d_%                                          8.70 ± 2.72   \n",
      "60d_dropout                                       680 (74.9%)   \n",
      "wl_60d_%                                         11.27 ± 3.23   \n",
      "80d_dropout                                       763 (84.0%)   \n",
      "wl_80d_%                                         13.22 ± 3.50   \n",
      "5%_wl_achieved                                    478 (52.6%)   \n",
      "days_to_5%_wl                                    17.63 ± 8.99   \n",
      "10%_wl_achieved                                   227 (25.0%)   \n",
      "days_to_10%_wl                                  45.38 ± 16.97   \n",
      "15%_wl_achieved                                     76 (8.4%)   \n",
      "days_to_15%_wl                                  74.61 ± 28.95   \n",
      "\n",
      "                              Measurement Frequency: >= Median  \\\n",
      "Variable                                                         \n",
      "N                                                  756 (45.4%)   \n",
      "age                                              48.35 ± 10.05   \n",
      "sex_f                                              651 (86.1%)   \n",
      "height_m                                           1.64 ± 0.08   \n",
      "baseline_weight_kg                               83.96 ± 11.44   \n",
      "baseline_bmi                                      30.97 ± 2.81   \n",
      "hunger_yn                                          490 (64.8%)   \n",
      "satiety_yn                                         440 (58.2%)   \n",
      "emotional_eating_yn                                609 (80.6%)   \n",
      "emotional_eating_value_likert                      6.94 ± 2.36   \n",
      "quantity_control_likert                            5.82 ± 2.38   \n",
      "impulse_control_likert                             6.07 ± 2.32   \n",
      "total_followup_days                              86.68 ± 86.76   \n",
      "avg_days_between_measurements                     10.14 ± 6.12   \n",
      "genomics_available                                 170 (22.5%)   \n",
      "total_wl_%                                         9.12 ± 6.19   \n",
      "bmi_reduction                                     -2.88 ± 2.10   \n",
      "instant_dropout                                       0 (0.0%)   \n",
      "40d_dropout                                        241 (31.9%)   \n",
      "wl_40d_%                                           7.66 ± 3.00   \n",
      "60d_dropout                                        392 (51.9%)   \n",
      "wl_60d_%                                           9.88 ± 4.05   \n",
      "80d_dropout                                        484 (64.0%)   \n",
      "wl_80d_%                                          11.72 ± 4.28   \n",
      "5%_wl_achieved                                     556 (73.5%)   \n",
      "days_to_5%_wl                                    26.89 ± 16.51   \n",
      "10%_wl_achieved                                    319 (42.2%)   \n",
      "days_to_10%_wl                                   58.77 ± 28.94   \n",
      "15%_wl_achieved                                    151 (20.0%)   \n",
      "days_to_15%_wl                                   95.57 ± 37.23   \n",
      "\n",
      "                              Measurement Frequency: p-value  \\\n",
      "Variable                                                       \n",
      "N                                                        N/A   \n",
      "age                                                   <0.001   \n",
      "sex_f                                                 <0.001   \n",
      "height_m                                              <0.001   \n",
      "baseline_weight_kg                                    <0.001   \n",
      "baseline_bmi                                          <0.001   \n",
      "hunger_yn                                              0.385   \n",
      "satiety_yn                                             0.071   \n",
      "emotional_eating_yn                                    0.038   \n",
      "emotional_eating_value_likert                          0.048   \n",
      "quantity_control_likert                                0.912   \n",
      "impulse_control_likert                                 0.313   \n",
      "total_followup_days                                   <0.001   \n",
      "avg_days_between_measurements                         <0.001   \n",
      "genomics_available                                     0.021   \n",
      "total_wl_%                                            <0.001   \n",
      "bmi_reduction                                         <0.001   \n",
      "instant_dropout                                       <0.001   \n",
      "40d_dropout                                           <0.001   \n",
      "wl_40d_%                                              <0.001   \n",
      "60d_dropout                                           <0.001   \n",
      "wl_60d_%                                              <0.001   \n",
      "80d_dropout                                           <0.001   \n",
      "wl_80d_%                                              <0.001   \n",
      "5%_wl_achieved                                        <0.001   \n",
      "days_to_5%_wl                                         <0.001   \n",
      "10%_wl_achieved                                       <0.001   \n",
      "days_to_10%_wl                                        <0.001   \n",
      "15%_wl_achieved                                       <0.001   \n",
      "days_to_15%_wl                                        <0.001   \n",
      "\n",
      "                              Genomics: Not Available Genomics: Available  \\\n",
      "Variable                                                                    \n",
      "N                                        1332 (80.0%)         332 (20.0%)   \n",
      "age                                     47.15 ± 10.29        47.41 ± 9.65   \n",
      "sex_f                                    1092 (82.0%)         276 (83.1%)   \n",
      "height_m                                  1.65 ± 0.08         1.65 ± 0.08   \n",
      "baseline_weight_kg                      82.83 ± 12.21       82.90 ± 11.91   \n",
      "baseline_bmi                             30.15 ± 3.02        30.25 ± 3.05   \n",
      "hunger_yn                                 880 (66.1%)         218 (65.7%)   \n",
      "satiety_yn                                809 (60.7%)         200 (60.2%)   \n",
      "emotional_eating_yn                      1042 (78.2%)         259 (78.0%)   \n",
      "emotional_eating_value_likert             6.82 ± 2.39         6.77 ± 2.22   \n",
      "quantity_control_likert                   5.76 ± 2.47         6.07 ± 2.15   \n",
      "impulse_control_likert                    5.96 ± 2.41         6.20 ± 2.10   \n",
      "total_followup_days                     55.73 ± 67.84       75.45 ± 86.37   \n",
      "avg_days_between_measurements             6.31 ± 5.31         7.50 ± 7.12   \n",
      "genomics_available                           0 (0.0%)        332 (100.0%)   \n",
      "total_wl_%                                7.24 ± 5.87         8.83 ± 6.63   \n",
      "bmi_reduction                            -2.25 ± 1.95        -2.79 ± 2.27   \n",
      "instant_dropout                           160 (12.0%)           27 (8.1%)   \n",
      "40d_dropout                               672 (50.5%)         118 (35.5%)   \n",
      "wl_40d_%                                  8.10 ± 2.94         8.04 ± 2.91   \n",
      "60d_dropout                               885 (66.4%)         187 (56.3%)   \n",
      "wl_60d_%                                 10.24 ± 3.87        10.98 ± 3.58   \n",
      "80d_dropout                              1023 (76.8%)         224 (67.5%)   \n",
      "wl_80d_%                                 12.00 ± 4.07        12.93 ± 4.06   \n",
      "5%_wl_achieved                            808 (60.7%)         226 (68.1%)   \n",
      "days_to_5%_wl                           22.14 ± 14.20       24.29 ± 14.66   \n",
      "10%_wl_achieved                           413 (31.0%)         133 (40.1%)   \n",
      "days_to_10%_wl                          53.73 ± 25.00       51.56 ± 27.14   \n",
      "15%_wl_achieved                           157 (11.8%)          70 (21.1%)   \n",
      "days_to_15%_wl                          88.65 ± 37.07       88.33 ± 33.77   \n",
      "\n",
      "                              Genomics: p-value 40d Dropout: No (0)  \\\n",
      "Variable                                                              \n",
      "N                                           N/A         874 (52.5%)   \n",
      "age                                       0.665        48.18 ± 9.90   \n",
      "sex_f                                     0.682         690 (78.9%)   \n",
      "height_m                                  0.897         1.66 ± 0.08   \n",
      "baseline_weight_kg                        0.920       86.05 ± 11.44   \n",
      "baseline_bmi                              0.611        31.21 ± 2.57   \n",
      "hunger_yn                                 0.941         578 (66.1%)   \n",
      "satiety_yn                                0.919         521 (59.6%)   \n",
      "emotional_eating_yn                       0.991         696 (79.6%)   \n",
      "emotional_eating_value_likert             0.737         6.83 ± 2.31   \n",
      "quantity_control_likert                   0.026         5.86 ± 2.40   \n",
      "impulse_control_likert                    0.077         6.05 ± 2.35   \n",
      "total_followup_days                      <0.001       97.84 ± 76.86   \n",
      "avg_days_between_measurements             0.006         7.17 ± 5.34   \n",
      "genomics_available                       <0.001         214 (24.5%)   \n",
      "total_wl_%                               <0.001        11.46 ± 5.47   \n",
      "bmi_reduction                            <0.001        -3.64 ± 1.91   \n",
      "instant_dropout                           0.057            0 (0.0%)   \n",
      "40d_dropout                              <0.001            0 (0.0%)   \n",
      "wl_40d_%                                  0.801         8.09 ± 2.93   \n",
      "60d_dropout                              <0.001         314 (35.9%)   \n",
      "wl_60d_%                                  0.034        10.69 ± 3.59   \n",
      "80d_dropout                              <0.001         475 (54.3%)   \n",
      "wl_80d_%                                  0.041        12.45 ± 3.97   \n",
      "5%_wl_achieved                            0.015         813 (93.0%)   \n",
      "days_to_5%_wl                             0.050       23.59 ± 13.20   \n",
      "10%_wl_achieved                           0.002         533 (61.0%)   \n",
      "days_to_10%_wl                            0.413       53.37 ± 25.46   \n",
      "15%_wl_achieved                          <0.001         224 (25.6%)   \n",
      "days_to_15%_wl                            0.949       88.58 ± 35.81   \n",
      "\n",
      "                              40d Dropout: Yes (1) 40d Dropout: p-value  \\\n",
      "Variable                                                                  \n",
      "N                                      790 (47.5%)                  N/A   \n",
      "age                                  46.13 ± 10.34               <0.001   \n",
      "sex_f                                  678 (85.8%)               <0.001   \n",
      "height_m                               1.65 ± 0.08                0.043   \n",
      "baseline_weight_kg                   79.30 ± 11.93               <0.001   \n",
      "baseline_bmi                          29.02 ± 3.08               <0.001   \n",
      "hunger_yn                              520 (65.8%)                0.935   \n",
      "satiety_yn                             488 (61.8%)                0.395   \n",
      "emotional_eating_yn                    605 (76.6%)                0.148   \n",
      "emotional_eating_value_likert          6.79 ± 2.41                0.694   \n",
      "quantity_control_likert                5.78 ± 2.43                0.537   \n",
      "impulse_control_likert                 5.96 ± 2.35                0.450   \n",
      "total_followup_days                  17.44 ± 32.99               <0.001   \n",
      "avg_days_between_measurements          5.68 ± 6.17               <0.001   \n",
      "genomics_available                     118 (14.9%)               <0.001   \n",
      "total_wl_%                             3.23 ± 2.96               <0.001   \n",
      "bmi_reduction                         -0.94 ± 0.89               <0.001   \n",
      "instant_dropout                        187 (23.7%)               <0.001   \n",
      "40d_dropout                           790 (100.0%)               <0.001   \n",
      "wl_40d_%                                       N/A                  N/A   \n",
      "60d_dropout                            758 (95.9%)               <0.001   \n",
      "wl_60d_%                               5.59 ± 4.43               <0.001   \n",
      "80d_dropout                            772 (97.7%)               <0.001   \n",
      "wl_80d_%                               7.57 ± 3.80               <0.001   \n",
      "5%_wl_achieved                         221 (28.0%)               <0.001   \n",
      "days_to_5%_wl                        18.99 ± 17.43               <0.001   \n",
      "10%_wl_achieved                          13 (1.6%)               <0.001   \n",
      "days_to_10%_wl                       46.38 ± 28.32                0.395   \n",
      "15%_wl_achieved                           3 (0.4%)               <0.001   \n",
      "days_to_15%_wl                       86.00 ± 59.10                0.947   \n",
      "\n",
      "                              60d Dropout: No (0) 60d Dropout: Yes (1)  \\\n",
      "Variable                                                                 \n",
      "N                                     592 (35.6%)         1072 (64.4%)   \n",
      "age                                  49.06 ± 9.70        46.18 ± 10.27   \n",
      "sex_f                                 466 (78.7%)          902 (84.1%)   \n",
      "height_m                              1.66 ± 0.08          1.65 ± 0.08   \n",
      "baseline_weight_kg                  87.01 ± 11.03        80.55 ± 12.13   \n",
      "baseline_bmi                         31.68 ± 2.40         29.34 ± 3.02   \n",
      "hunger_yn                             384 (64.9%)          714 (66.6%)   \n",
      "satiety_yn                            335 (56.6%)          674 (62.9%)   \n",
      "emotional_eating_yn                   467 (78.9%)          834 (77.8%)   \n",
      "emotional_eating_value_likert         6.72 ± 2.34          6.86 ± 2.37   \n",
      "quantity_control_likert               5.73 ± 2.42          5.87 ± 2.41   \n",
      "impulse_control_likert                5.94 ± 2.34          6.05 ± 2.35   \n",
      "total_followup_days                119.65 ± 78.38        26.54 ± 40.55   \n",
      "avg_days_between_measurements         7.43 ± 5.51          5.98 ± 5.83   \n",
      "genomics_available                    145 (24.5%)          187 (17.4%)   \n",
      "total_wl_%                           12.97 ± 5.65          4.57 ± 3.79   \n",
      "bmi_reduction                        -4.15 ± 2.01         -1.37 ± 1.18   \n",
      "instant_dropout                          0 (0.0%)          187 (17.4%)   \n",
      "40d_dropout                             32 (5.4%)          758 (70.7%)   \n",
      "wl_40d_%                              8.45 ± 2.91          7.44 ± 2.87   \n",
      "60d_dropout                              0 (0.0%)        1072 (100.0%)   \n",
      "wl_60d_%                             10.42 ± 3.81                  N/A   \n",
      "80d_dropout                           213 (36.0%)         1034 (96.5%)   \n",
      "wl_80d_%                             12.60 ± 3.94          8.66 ± 3.84   \n",
      "5%_wl_achieved                        564 (95.3%)          470 (43.8%)   \n",
      "days_to_5%_wl                       24.68 ± 14.12        20.13 ± 14.18   \n",
      "10%_wl_achieved                       446 (75.3%)           100 (9.3%)   \n",
      "days_to_10%_wl                      54.45 ± 22.87        47.66 ± 34.62   \n",
      "15%_wl_achieved                       214 (36.1%)            13 (1.2%)   \n",
      "days_to_15%_wl                      87.36 ± 34.00       108.08 ± 58.85   \n",
      "\n",
      "                              60d Dropout: p-value 80d Dropout: No (0)  \\\n",
      "Variable                                                                 \n",
      "N                                              N/A         417 (25.1%)   \n",
      "age                                         <0.001        49.06 ± 9.75   \n",
      "sex_f                                        0.007         334 (80.1%)   \n",
      "height_m                                     0.836         1.65 ± 0.08   \n",
      "baseline_weight_kg                          <0.001       87.59 ± 10.77   \n",
      "baseline_bmi                                <0.001        32.08 ± 2.29   \n",
      "hunger_yn                                    0.507         281 (67.4%)   \n",
      "satiety_yn                                   0.014         237 (56.8%)   \n",
      "emotional_eating_yn                          0.651         333 (79.9%)   \n",
      "emotional_eating_value_likert                0.246         6.82 ± 2.27   \n",
      "quantity_control_likert                      0.258         5.80 ± 2.40   \n",
      "impulse_control_likert                       0.352         6.05 ± 2.32   \n",
      "total_followup_days                         <0.001      142.28 ± 81.75   \n",
      "avg_days_between_measurements               <0.001         7.46 ± 4.72   \n",
      "genomics_available                          <0.001         108 (25.9%)   \n",
      "total_wl_%                                  <0.001        14.24 ± 5.64   \n",
      "bmi_reduction                               <0.001        -4.62 ± 2.02   \n",
      "instant_dropout                             <0.001            0 (0.0%)   \n",
      "40d_dropout                                 <0.001           18 (4.3%)   \n",
      "wl_40d_%                                    <0.001         8.49 ± 2.74   \n",
      "60d_dropout                                 <0.001           38 (9.1%)   \n",
      "wl_60d_%                                       N/A        10.90 ± 3.46   \n",
      "80d_dropout                                 <0.001            0 (0.0%)   \n",
      "wl_80d_%                                    <0.001        12.24 ± 4.09   \n",
      "5%_wl_achieved                              <0.001         408 (97.8%)   \n",
      "days_to_5%_wl                               <0.001       26.21 ± 15.88   \n",
      "10%_wl_achieved                             <0.001         346 (83.0%)   \n",
      "days_to_10%_wl                               0.064       57.60 ± 25.91   \n",
      "15%_wl_achieved                             <0.001         194 (46.5%)   \n",
      "days_to_15%_wl                               0.232       90.89 ± 35.38   \n",
      "\n",
      "                              80d Dropout: Yes (1) 80d Dropout: p-value  \\\n",
      "Variable                                                                  \n",
      "N                                     1247 (74.9%)                  N/A   \n",
      "age                                  46.58 ± 10.23               <0.001   \n",
      "sex_f                                 1034 (82.9%)                0.218   \n",
      "height_m                               1.66 ± 0.08                0.225   \n",
      "baseline_weight_kg                   81.26 ± 12.18               <0.001   \n",
      "baseline_bmi                          29.53 ± 2.98               <0.001   \n",
      "hunger_yn                              817 (65.5%)                0.524   \n",
      "satiety_yn                             772 (61.9%)                0.075   \n",
      "emotional_eating_yn                    968 (77.6%)                0.376   \n",
      "emotional_eating_value_likert          6.81 ± 2.38                0.889   \n",
      "quantity_control_likert                5.83 ± 2.42                0.808   \n",
      "impulse_control_likert                 5.99 ± 2.36                0.666   \n",
      "total_followup_days                  32.04 ± 41.24               <0.001   \n",
      "avg_days_between_measurements          6.20 ± 6.06               <0.001   \n",
      "genomics_available                     224 (18.0%)               <0.001   \n",
      "total_wl_%                             5.32 ± 4.31               <0.001   \n",
      "bmi_reduction                         -1.60 ± 1.35               <0.001   \n",
      "instant_dropout                        187 (15.0%)               <0.001   \n",
      "40d_dropout                            772 (61.9%)               <0.001   \n",
      "wl_40d_%                               7.74 ± 3.05               <0.001   \n",
      "60d_dropout                           1034 (82.9%)               <0.001   \n",
      "wl_60d_%                               9.57 ± 4.24               <0.001   \n",
      "80d_dropout                          1247 (100.0%)               <0.001   \n",
      "wl_80d_%                                       N/A                  N/A   \n",
      "5%_wl_achieved                         626 (50.2%)               <0.001   \n",
      "days_to_5%_wl                        20.27 ± 12.68               <0.001   \n",
      "10%_wl_achieved                        200 (16.0%)               <0.001   \n",
      "days_to_10%_wl                       45.59 ± 23.01               <0.001   \n",
      "15%_wl_achieved                          33 (2.6%)               <0.001   \n",
      "days_to_15%_wl                       74.79 ± 37.14                0.025   \n",
      "\n",
      "                              5% WL Achieved: No (0) 5% WL Achieved: Yes (1)  \\\n",
      "Variable                                                                       \n",
      "N                                        630 (37.9%)            1034 (62.1%)   \n",
      "age                                    47.15 ± 10.18           47.24 ± 10.16   \n",
      "sex_f                                    555 (88.1%)             813 (78.6%)   \n",
      "height_m                                 1.65 ± 0.08             1.66 ± 0.08   \n",
      "baseline_weight_kg                     79.25 ± 12.20           85.03 ± 11.59   \n",
      "baseline_bmi                            29.10 ± 3.29            30.83 ± 2.66   \n",
      "hunger_yn                                434 (68.9%)             664 (64.2%)   \n",
      "satiety_yn                               394 (62.5%)             615 (59.5%)   \n",
      "emotional_eating_yn                      489 (77.6%)             812 (78.5%)   \n",
      "emotional_eating_value_likert            6.87 ± 2.31             6.78 ± 2.39   \n",
      "quantity_control_likert                  5.67 ± 2.40             5.92 ± 2.42   \n",
      "impulse_control_likert                   5.89 ± 2.27             6.08 ± 2.39   \n",
      "total_followup_days                    15.15 ± 26.92           86.79 ± 77.67   \n",
      "avg_days_between_measurements            6.25 ± 7.00             6.69 ± 5.08   \n",
      "genomics_available                       106 (16.8%)             226 (21.9%)   \n",
      "total_wl_%                               1.89 ± 1.78            11.01 ± 5.07   \n",
      "bmi_reduction                           -0.55 ± 0.52            -3.46 ± 1.80   \n",
      "instant_dropout                          187 (29.7%)                0 (0.0%)   \n",
      "40d_dropout                              569 (90.3%)             221 (21.4%)   \n",
      "wl_40d_%                                 2.47 ± 1.43             8.51 ± 2.56   \n",
      "60d_dropout                              602 (95.6%)             470 (45.5%)   \n",
      "wl_60d_%                                 2.17 ± 2.91            10.83 ± 3.36   \n",
      "80d_dropout                              621 (98.6%)             626 (60.5%)   \n",
      "wl_80d_%                                 3.48 ± 0.85            12.43 ± 3.91   \n",
      "5%_wl_achieved                              0 (0.0%)           1034 (100.0%)   \n",
      "days_to_5%_wl                                    N/A           22.61 ± 14.32   \n",
      "10%_wl_achieved                             0 (0.0%)             546 (52.8%)   \n",
      "days_to_10%_wl                                   N/A           53.20 ± 25.53   \n",
      "15%_wl_achieved                             0 (0.0%)             227 (22.0%)   \n",
      "days_to_15%_wl                                   N/A           88.55 ± 36.01   \n",
      "\n",
      "                              5% WL Achieved: p-value 10% WL Achieved: No (0)  \\\n",
      "Variable                                                                        \n",
      "N                                                 N/A            1118 (67.2%)   \n",
      "age                                             0.863           46.82 ± 10.21   \n",
      "sex_f                                          <0.001             957 (85.6%)   \n",
      "height_m                                        0.012             1.65 ± 0.08   \n",
      "baseline_weight_kg                             <0.001           80.48 ± 11.80   \n",
      "baseline_bmi                                   <0.001            29.43 ± 3.03   \n",
      "hunger_yn                                       0.058             732 (65.5%)   \n",
      "satiety_yn                                      0.235             686 (61.4%)   \n",
      "emotional_eating_yn                             0.707             871 (77.9%)   \n",
      "emotional_eating_value_likert                   0.464             6.83 ± 2.40   \n",
      "quantity_control_likert                         0.043             5.84 ± 2.43   \n",
      "impulse_control_likert                          0.107             5.95 ± 2.36   \n",
      "total_followup_days                            <0.001           30.36 ± 44.84   \n",
      "avg_days_between_measurements                   0.230             6.37 ± 6.25   \n",
      "genomics_available                              0.015             199 (17.8%)   \n",
      "total_wl_%                                     <0.001             4.13 ± 3.08   \n",
      "bmi_reduction                                  <0.001            -1.23 ± 0.94   \n",
      "instant_dropout                                <0.001             187 (16.7%)   \n",
      "40d_dropout                                    <0.001             777 (69.5%)   \n",
      "wl_40d_%                                       <0.001             5.94 ± 2.39   \n",
      "60d_dropout                                    <0.001             972 (86.9%)   \n",
      "wl_60d_%                                       <0.001             5.98 ± 2.78   \n",
      "80d_dropout                                    <0.001            1047 (93.6%)   \n",
      "wl_80d_%                                       <0.001             6.44 ± 2.17   \n",
      "5%_wl_achieved                                 <0.001             488 (43.6%)   \n",
      "days_to_5%_wl                                     N/A           24.04 ± 16.29   \n",
      "10%_wl_achieved                                <0.001                0 (0.0%)   \n",
      "days_to_10%_wl                                    N/A                     N/A   \n",
      "15%_wl_achieved                                <0.001                0 (0.0%)   \n",
      "days_to_15%_wl                                    N/A                     N/A   \n",
      "\n",
      "                              10% WL Achieved: Yes (1)  \\\n",
      "Variable                                                 \n",
      "N                                          546 (32.8%)   \n",
      "age                                      48.00 ± 10.03   \n",
      "sex_f                                      411 (75.3%)   \n",
      "height_m                                   1.66 ± 0.08   \n",
      "baseline_weight_kg                       87.69 ± 11.41   \n",
      "baseline_bmi                              31.69 ± 2.38   \n",
      "hunger_yn                                  366 (67.0%)   \n",
      "satiety_yn                                 323 (59.2%)   \n",
      "emotional_eating_yn                        430 (78.8%)   \n",
      "emotional_eating_value_likert              6.78 ± 2.27   \n",
      "quantity_control_likert                    5.79 ± 2.39   \n",
      "impulse_control_likert                     6.12 ± 2.31   \n",
      "total_followup_days                     119.68 ± 80.42   \n",
      "avg_days_between_measurements              6.87 ± 4.73   \n",
      "genomics_available                         133 (24.4%)   \n",
      "total_wl_%                                14.57 ± 4.42   \n",
      "bmi_reduction                             -4.67 ± 1.66   \n",
      "instant_dropout                               0 (0.0%)   \n",
      "40d_dropout                                  13 (2.4%)   \n",
      "wl_40d_%                                   9.46 ± 2.38   \n",
      "60d_dropout                                100 (18.3%)   \n",
      "wl_60d_%                                  11.87 ± 2.86   \n",
      "80d_dropout                                200 (36.6%)   \n",
      "wl_80d_%                                  13.43 ± 3.29   \n",
      "5%_wl_achieved                            546 (100.0%)   \n",
      "days_to_5%_wl                            21.33 ± 12.18   \n",
      "10%_wl_achieved                           546 (100.0%)   \n",
      "days_to_10%_wl                           53.20 ± 25.53   \n",
      "15%_wl_achieved                            227 (41.6%)   \n",
      "days_to_15%_wl                           88.55 ± 36.01   \n",
      "\n",
      "                              10% WL Achieved: p-value  \\\n",
      "Variable                                                 \n",
      "N                                                  N/A   \n",
      "age                                              0.025   \n",
      "sex_f                                           <0.001   \n",
      "height_m                                         0.022   \n",
      "baseline_weight_kg                              <0.001   \n",
      "baseline_bmi                                    <0.001   \n",
      "hunger_yn                                        0.565   \n",
      "satiety_yn                                       0.418   \n",
      "emotional_eating_yn                              0.741   \n",
      "emotional_eating_value_likert                    0.685   \n",
      "quantity_control_likert                          0.719   \n",
      "impulse_control_likert                           0.174   \n",
      "total_followup_days                             <0.001   \n",
      "avg_days_between_measurements                    0.079   \n",
      "genomics_available                               0.002   \n",
      "total_wl_%                                      <0.001   \n",
      "bmi_reduction                                   <0.001   \n",
      "instant_dropout                                 <0.001   \n",
      "40d_dropout                                     <0.001   \n",
      "wl_40d_%                                        <0.001   \n",
      "60d_dropout                                     <0.001   \n",
      "wl_60d_%                                        <0.001   \n",
      "80d_dropout                                     <0.001   \n",
      "wl_80d_%                                        <0.001   \n",
      "5%_wl_achieved                                  <0.001   \n",
      "days_to_5%_wl                                    0.003   \n",
      "10%_wl_achieved                                 <0.001   \n",
      "days_to_10%_wl                                     N/A   \n",
      "15%_wl_achieved                                 <0.001   \n",
      "days_to_15%_wl                                     N/A   \n",
      "\n",
      "                              15% WL Achieved: No (0)  \\\n",
      "Variable                                                \n",
      "N                                        1437 (86.4%)   \n",
      "age                                     47.07 ± 10.28   \n",
      "sex_f                                    1202 (83.6%)   \n",
      "height_m                                  1.65 ± 0.08   \n",
      "baseline_weight_kg                      81.71 ± 11.88   \n",
      "baseline_bmi                             29.76 ± 2.94   \n",
      "hunger_yn                                 946 (65.8%)   \n",
      "satiety_yn                                878 (61.1%)   \n",
      "emotional_eating_yn                      1122 (78.1%)   \n",
      "emotional_eating_value_likert             6.81 ± 2.36   \n",
      "quantity_control_likert                   5.83 ± 2.40   \n",
      "impulse_control_likert                    5.99 ± 2.36   \n",
      "total_followup_days                     44.79 ± 56.41   \n",
      "avg_days_between_measurements             6.40 ± 5.80   \n",
      "genomics_available                        262 (18.2%)   \n",
      "total_wl_%                                5.82 ± 4.26   \n",
      "bmi_reduction                            -1.76 ± 1.33   \n",
      "instant_dropout                           187 (13.0%)   \n",
      "40d_dropout                               787 (54.8%)   \n",
      "wl_40d_%                                  7.45 ± 2.73   \n",
      "60d_dropout                              1059 (73.7%)   \n",
      "wl_60d_%                                  8.91 ± 3.28   \n",
      "80d_dropout                              1214 (84.5%)   \n",
      "wl_80d_%                                  9.90 ± 3.11   \n",
      "5%_wl_achieved                            807 (56.2%)   \n",
      "days_to_5%_wl                           22.93 ± 15.22   \n",
      "10%_wl_achieved                           319 (22.2%)   \n",
      "days_to_10%_wl                          55.48 ± 28.05   \n",
      "15%_wl_achieved                              0 (0.0%)   \n",
      "days_to_15%_wl                                    N/A   \n",
      "\n",
      "                              15% WL Achieved: Yes (1)  \\\n",
      "Variable                                                 \n",
      "N                                          227 (13.6%)   \n",
      "age                                       48.03 ± 9.35   \n",
      "sex_f                                      166 (73.1%)   \n",
      "height_m                                   1.65 ± 0.08   \n",
      "baseline_weight_kg                       90.00 ± 11.41   \n",
      "baseline_bmi                              32.79 ± 2.18   \n",
      "hunger_yn                                  152 (67.0%)   \n",
      "satiety_yn                                 131 (57.7%)   \n",
      "emotional_eating_yn                        179 (78.9%)   \n",
      "emotional_eating_value_likert              6.81 ± 2.33   \n",
      "quantity_control_likert                    5.78 ± 2.51   \n",
      "impulse_control_likert                     6.11 ± 2.27   \n",
      "total_followup_days                     153.87 ± 89.13   \n",
      "avg_days_between_measurements              7.38 ± 5.38   \n",
      "genomics_available                          70 (30.8%)   \n",
      "total_wl_%                                18.55 ± 3.84   \n",
      "bmi_reduction                             -6.13 ± 1.55   \n",
      "instant_dropout                               0 (0.0%)   \n",
      "40d_dropout                                   3 (1.3%)   \n",
      "wl_40d_%                                   9.93 ± 2.71   \n",
      "60d_dropout                                  13 (5.7%)   \n",
      "wl_60d_%                                  13.08 ± 3.19   \n",
      "80d_dropout                                 33 (14.5%)   \n",
      "wl_80d_%                                  14.93 ± 3.36   \n",
      "5%_wl_achieved                            227 (100.0%)   \n",
      "days_to_5%_wl                            21.47 ± 10.49   \n",
      "10%_wl_achieved                           227 (100.0%)   \n",
      "days_to_10%_wl                           50.01 ± 21.15   \n",
      "15%_wl_achieved                           227 (100.0%)   \n",
      "days_to_15%_wl                           88.55 ± 36.01   \n",
      "\n",
      "                              15% WL Achieved: p-value  \n",
      "Variable                                                \n",
      "N                                                  N/A  \n",
      "age                                              0.161  \n",
      "sex_f                                           <0.001  \n",
      "height_m                                         0.975  \n",
      "baseline_weight_kg                              <0.001  \n",
      "baseline_bmi                                    <0.001  \n",
      "hunger_yn                                        0.796  \n",
      "satiety_yn                                       0.369  \n",
      "emotional_eating_yn                              0.860  \n",
      "emotional_eating_value_likert                    0.972  \n",
      "quantity_control_likert                          0.755  \n",
      "impulse_control_likert                           0.449  \n",
      "total_followup_days                             <0.001  \n",
      "avg_days_between_measurements                    0.013  \n",
      "genomics_available                              <0.001  \n",
      "total_wl_%                                      <0.001  \n",
      "bmi_reduction                                   <0.001  \n",
      "instant_dropout                                 <0.001  \n",
      "40d_dropout                                     <0.001  \n",
      "wl_40d_%                                        <0.001  \n",
      "60d_dropout                                     <0.001  \n",
      "wl_60d_%                                        <0.001  \n",
      "80d_dropout                                     <0.001  \n",
      "wl_80d_%                                        <0.001  \n",
      "5%_wl_achieved                                  <0.001  \n",
      "days_to_5%_wl                                    0.096  \n",
      "10%_wl_achieved                                 <0.001  \n",
      "days_to_10%_wl                                   0.010  \n",
      "15%_wl_achieved                                 <0.001  \n",
      "days_to_15%_wl                                     N/A  \n",
      "\n",
      "Connecting to output database: .\\survival_analysis.sqlite\n",
      "Saving comparative summary to table: comparative_summary\n",
      "Table 'comparative_summary' saved successfully to .\\survival_analysis.sqlite.\n",
      "Output database connection closed.\n",
      "Input database connection closed.\n",
      "Output database connection closed.\n",
      "Script finished.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import warnings\n",
    "import sqlite3 # Assuming data is loaded from the SQLite DB\n",
    "import os # Added for path joining\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# Define the variables to include as rows in the table\n",
    "# Format: 'variable_name': {'type': 'continuous'/'categorical', 'format': 'mean_sd'/'n_perc'/'n_perc_pop'}\n",
    "# 'n_perc_pop' is special for the first 'N' row\n",
    "# 'n_perc' for categorical assumes 0/1 or False/True, calculates N(%) for 1/True\n",
    "rows_config = {\n",
    "    'N': {'type': 'categorical', 'format': 'n_perc_pop'},\n",
    "    'age': {'type': 'continuous', 'format': 'mean_sd'}, # Using corrected age column\n",
    "    'sex_f': {'type': 'categorical', 'format': 'n_perc'},\n",
    "    'height_m': {'type': 'continuous', 'format': 'mean_sd'},\n",
    "    'baseline_weight_kg': {'type': 'continuous', 'format': 'mean_sd'},\n",
    "    'baseline_bmi': {'type': 'continuous', 'format': 'mean_sd'},\n",
    "    'hunger_yn': {'type': 'categorical', 'format': 'n_perc'},\n",
    "    'satiety_yn': {'type': 'categorical', 'format': 'n_perc'},\n",
    "    'emotional_eating_yn': {'type': 'categorical', 'format': 'n_perc'},\n",
    "    'emotional_eating_value_likert': {'type': 'continuous', 'format': 'mean_sd'},\n",
    "    'quantity_control_likert': {'type': 'continuous', 'format': 'mean_sd'},\n",
    "    'impulse_control_likert': {'type': 'continuous', 'format': 'mean_sd'},\n",
    "    'total_followup_days': {'type': 'continuous', 'format': 'mean_sd'},\n",
    "    'avg_days_between_measurements': {'type': 'continuous', 'format': 'mean_sd'},\n",
    "    'genomics_available': {'type': 'categorical', 'format': 'n_perc'}, # Assuming a boolean column is created\n",
    "    'total_wl_%': {'type': 'continuous', 'format': 'mean_sd'},\n",
    "    'bmi_reduction': {'type': 'continuous', 'format': 'mean_sd'},\n",
    "    'instant_dropout': {'type': 'categorical', 'format': 'n_perc'}, # Assuming boolean: total_followup_days == 1\n",
    "    '40d_dropout': {'type': 'categorical', 'format': 'n_perc'},\n",
    "    'wl_40d_%': {'type': 'continuous', 'format': 'mean_sd'},\n",
    "    '60d_dropout': {'type': 'categorical', 'format': 'n_perc'},\n",
    "    'wl_60d_%': {'type': 'continuous', 'format': 'mean_sd'},\n",
    "    '80d_dropout': {'type': 'categorical', 'format': 'n_perc'},\n",
    "    'wl_80d_%': {'type': 'continuous', 'format': 'mean_sd'},\n",
    "    '5%_wl_achieved': {'type': 'categorical', 'format': 'n_perc'},\n",
    "    'days_to_5%_wl': {'type': 'continuous', 'format': 'mean_sd'},\n",
    "    '10%_wl_achieved': {'type': 'categorical', 'format': 'n_perc'},\n",
    "    'days_to_10%_wl': {'type': 'continuous', 'format': 'mean_sd'},\n",
    "    '15%_wl_achieved': {'type': 'categorical', 'format': 'n_perc'},\n",
    "    'days_to_15%_wl': {'type': 'continuous', 'format': 'mean_sd'},\n",
    "}\n",
    "\n",
    "# Define the stratification criteria for columns\n",
    "# Format: {'name': 'Stratification Name', 'column': 'df_column', 'type': 'median_split'/'value_split'/'binary'/'isna', 'cutoff': value_for_split, 'labels': ['Group 0 Label', 'Group 1 Label']}\n",
    "strata_config = [\n",
    "    {'name': 'Age', 'column': 'age', 'type': 'median_split', 'cutoff': None, 'labels': ['< Median', '>= Median']},\n",
    "    {'name': 'Sex', 'column': 'sex_f', 'type': 'binary', 'cutoff': None, 'labels': ['Male (0)', 'Female (1)']}, # Assumes 0/1\n",
    "    {'name': 'Height', 'column': 'height_m', 'type': 'median_split', 'cutoff': None, 'labels': ['< Median', '>= Median']},\n",
    "    {'name': 'Baseline Weight', 'column': 'baseline_weight_kg', 'type': 'median_split', 'cutoff': None, 'labels': ['< Median', '>= Median']},\n",
    "    {'name': 'Baseline BMI', 'column': 'baseline_bmi', 'type': 'value_split', 'cutoff': 30, 'labels': ['< 30', '>= 30']},\n",
    "    {'name': 'Total WL%', 'column': 'total_wl_%', 'type': 'median_split', 'cutoff': None, 'labels': ['< Median', '>= Median']},\n",
    "    {'name': 'BMI Reduction', 'column': 'bmi_reduction', 'type': 'median_split', 'cutoff': None, 'labels': ['< Median', '>= Median']},\n",
    "    {'name': 'Hunger', 'column': 'hunger_yn', 'type': 'binary', 'cutoff': None, 'labels': ['No (0)', 'Yes (1)']}, # Assumes 0/1\n",
    "    {'name': 'Satiety', 'column': 'satiety_yn', 'type': 'binary', 'cutoff': None, 'labels': ['No (0)', 'Yes (1)']}, # Assumes 0/1\n",
    "    {'name': 'Emotional Eating YN', 'column': 'emotional_eating_yn', 'type': 'binary', 'cutoff': None, 'labels': ['No (0)', 'Yes (1)']}, # Assumes 0/1\n",
    "    {'name': 'Emotional Eating Likert', 'column': 'emotional_eating_value_likert', 'type': 'median_split', 'cutoff': None, 'labels': ['< Median', '>= Median']},\n",
    "    {'name': 'Quantity Control Likert', 'column': 'quantity_control_likert', 'type': 'median_split', 'cutoff': None, 'labels': ['< Median', '>= Median']},\n",
    "    {'name': 'Impulse Control Likert', 'column': 'impulse_control_likert', 'type': 'median_split', 'cutoff': None, 'labels': ['< Median', '>= Median']},\n",
    "    {'name': 'Followup Duration', 'column': 'total_followup_days', 'type': 'value_split', 'cutoff': 10, 'labels': ['<= 10 Days', '> 10 Days']}, # Note: Cutoff is > 10, so split is <=10 vs >10\n",
    "    {'name': 'Measurement Frequency', 'column': 'avg_days_between_measurements', 'type': 'median_split', 'cutoff': None, 'labels': ['< Median', '>= Median']},\n",
    "    {'name': 'Genomics', 'column': 'genomics_sample_id', 'type': 'isna', 'cutoff': None, 'labels': ['Not Available', 'Available']}, # Special type 'isna'\n",
    "    {'name': '40d Dropout', 'column': '40d_dropout', 'type': 'binary', 'cutoff': None, 'labels': ['No (0)', 'Yes (1)']}, # Assumes 0/1\n",
    "    {'name': '60d Dropout', 'column': '60d_dropout', 'type': 'binary', 'cutoff': None, 'labels': ['No (0)', 'Yes (1)']}, # Assumes 0/1\n",
    "    {'name': '80d Dropout', 'column': '80d_dropout', 'type': 'binary', 'cutoff': None, 'labels': ['No (0)', 'Yes (1)']}, # Assumes 0/1\n",
    "    {'name': '5% WL Achieved', 'column': '5%_wl_achieved', 'type': 'binary', 'cutoff': None, 'labels': ['No (0)', 'Yes (1)']}, # Assumes 0/1\n",
    "    {'name': '10% WL Achieved', 'column': '10%_wl_achieved', 'type': 'binary', 'cutoff': None, 'labels': ['No (0)', 'Yes (1)']}, # Assumes 0/1\n",
    "    {'name': '15% WL Achieved', 'column': '15%_wl_achieved', 'type': 'binary', 'cutoff': None, 'labels': ['No (0)', 'Yes (1)']}, # Assumes 0/1\n",
    "]\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def format_mean_sd(series):\n",
    "    \"\"\"Formats mean and standard deviation.\"\"\"\n",
    "    if series.isnull().all() or len(series.dropna()) < 1:\n",
    "        return \"N/A\"\n",
    "    mean = series.mean()\n",
    "    sd = series.std()\n",
    "    return f\"{mean:.2f} ± {sd:.2f}\"\n",
    "\n",
    "def format_n_perc(series):\n",
    "    \"\"\"Formats N and percentage for categorical (0/1 or T/F) variables, focusing on the '1' or 'True' category.\"\"\"\n",
    "    if series.isnull().all():\n",
    "        return \"0 (0.0%)\"\n",
    "    n_total = len(series.dropna())\n",
    "    if n_total == 0:\n",
    "        return \"0 (0.0%)\"\n",
    "    # Ensure boolean or 0/1 interpretation\n",
    "    series_bool = series.dropna().astype(bool)\n",
    "    n_positive = series_bool.sum()\n",
    "    perc_positive = (n_positive / n_total) * 100\n",
    "    return f\"{n_positive} ({perc_positive:.1f}%)\"\n",
    "\n",
    "def format_n_perc_pop(series, total_pop_n):\n",
    "    \"\"\"Formats N and percentage relative to the total population.\"\"\"\n",
    "    n = len(series) # Count all in the group, including potential NaNs if they weren't dropped before calling\n",
    "    perc_pop = (n / total_pop_n) * 100 if total_pop_n > 0 else 0\n",
    "    return f\"{n} ({perc_pop:.1f}%)\"\n",
    "\n",
    "def calculate_p_value(series1, series2, var_type):\n",
    "    \"\"\"Calculates p-value using Welch's t-test or Chi-squared test.\"\"\"\n",
    "    series1_clean = series1.dropna()\n",
    "    series2_clean = series2.dropna()\n",
    "\n",
    "    # Check if either group is empty after dropping NaNs\n",
    "    if series1_clean.empty or series2_clean.empty:\n",
    "        # print(f\"  Debug: Empty series after dropna. S1 len: {len(series1_clean)}, S2 len: {len(series2_clean)}\") # Optional debug\n",
    "        return np.nan\n",
    "\n",
    "    try:\n",
    "        if var_type == 'continuous':\n",
    "            # Need at least 2 observations in each group for t-test\n",
    "            if len(series1_clean) < 2 or len(series2_clean) < 2:\n",
    "                # print(f\"  Debug: Insufficient data for t-test. S1 len: {len(series1_clean)}, S2 len: {len(series2_clean)}\") # Optional debug\n",
    "                return np.nan\n",
    "            # Welch's t-test (does not assume equal variance)\n",
    "            stat, p_val = stats.ttest_ind(series1_clean, series2_clean, equal_var=False, nan_policy='omit')\n",
    "            return p_val\n",
    "        elif var_type == 'categorical':\n",
    "            # --- MODIFIED CONTINGENCY TABLE CREATION ---\n",
    "            # Create a temporary DataFrame combining the data and group indicators\n",
    "            group_indicator = np.concatenate([np.zeros(len(series1_clean)), np.ones(len(series2_clean))])\n",
    "            data_values = pd.concat([series1_clean, series2_clean], ignore_index=True)\n",
    "\n",
    "            # Check if there's any data left after concatenation (shouldn't happen if checks above passed)\n",
    "            if data_values.empty:\n",
    "                 # print(\"  Debug: data_values empty in categorical p-value calc.\") # Optional debug\n",
    "                 return np.nan\n",
    "\n",
    "            # Create contingency table using crosstab on the temporary DataFrame\n",
    "            try:\n",
    "                 contingency_table = pd.crosstab(index=group_indicator, columns=data_values)\n",
    "            except Exception as e_crosstab:\n",
    "                 warnings.warn(f\"Error during pd.crosstab for categorical variable: {e_crosstab}. S1 unique: {series1_clean.unique()}, S2 unique: {series2_clean.unique()}\", UserWarning)\n",
    "                 return np.nan\n",
    "            # --- END MODIFICATION ---\n",
    "\n",
    "            # Ensure the table has the expected shape (at least 2 rows (groups) and >= 1 column (category))\n",
    "            # Chi2 requires at least 1 degree of freedom (e.g., 2x2 table).\n",
    "            # If only one category exists overall, p-value is meaningless (1.0 or NaN).\n",
    "            if contingency_table.shape[0] < 2 or contingency_table.shape[1] < 1:\n",
    "                 # print(f\"  Debug: Contingency table shape invalid: {contingency_table.shape}\") # Optional debug\n",
    "                 return np.nan # Or 1.0 if shape[1] == 1? NaN is safer.\n",
    "\n",
    "            # Handle cases where a group might have zero counts for all categories\n",
    "            if (contingency_table.sum(axis=1) == 0).any():\n",
    "                 # print(f\"  Debug: Zero counts for one group in contingency table:\\n{contingency_table}\") # Optional debug\n",
    "                 return np.nan # Chi2 will likely fail or give NaN\n",
    "\n",
    "            # Chi-squared test\n",
    "            try:\n",
    "                chi2, p_val, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "\n",
    "                # Check expected frequencies only if chi2 ran successfully\n",
    "                if dof > 0 and (expected < 5).any(): # Check only if dof > 0 (i.e., test is meaningful)\n",
    "                    warnings.warn(\n",
    "                        f\"Chi-squared test assumption violated (expected count < 5) for variable comparison. \"\n",
    "                        f\"Consider Fisher's Exact Test. Contingency Table:\\\\n{contingency_table}\\\\nExpected:\\\\n{expected}\",\n",
    "                        UserWarning\n",
    "                    )\n",
    "                # If dof is 0 (e.g., only one category after all), p-value is often NaN or 1.\n",
    "                if dof == 0:\n",
    "                    return 1.0 # Or np.nan? Let's return 1.0 indicating no difference detectable.\n",
    "\n",
    "                return p_val\n",
    "            except ValueError as e_chi2:\n",
    "                 # Catch specific errors from chi2_contingency (e.g., sum of frequencies is zero)\n",
    "                 warnings.warn(f\"Error during chi2_contingency: {e_chi2}. Contingency Table:\\\\n{contingency_table}\", UserWarning)\n",
    "                 return np.nan\n",
    "\n",
    "        else:\n",
    "            return np.nan\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Could not calculate p-value: {e}\", UserWarning)\n",
    "        return np.nan\n",
    "\n",
    "def format_p_value(p_val):\n",
    "    \"\"\"Formats p-value.\"\"\"\n",
    "    if pd.isna(p_val):\n",
    "        return \"N/A\"\n",
    "    elif p_val < 0.001:\n",
    "        return \"<0.001\"\n",
    "    else:\n",
    "        return f\"{p_val:.3f}\"\n",
    "\n",
    "# --- Main Function ---\n",
    "\n",
    "def generate_comparative_summary(df, rows_config, strata_config):\n",
    "    \"\"\"\n",
    "    Generates a comparative summary table based on defined row variables and stratification criteria.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataframe (e.g., sa_input_table).\n",
    "        rows_config (dict): Configuration for table rows.\n",
    "        strata_config (list): Configuration for table columns (stratifications).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The comparative summary table.\n",
    "    \"\"\"\n",
    "    summary_results = {}\n",
    "    total_population_n = len(df)\n",
    "    df_analysis = df.copy() # Work on a copy\n",
    "\n",
    "    # Pre-calculate stratification columns where needed\n",
    "    strata_cols_mapping = {}\n",
    "    for strat_info in strata_config:\n",
    "        strat_col_name = f\"_strata_{strat_info['column']}\"\n",
    "        original_col = strat_info['column']\n",
    "\n",
    "        # Check if original column exists\n",
    "        if original_col not in df_analysis.columns:\n",
    "            warnings.warn(f\"Stratification column '{original_col}' not found in DataFrame. Skipping stratification '{strat_info['name']}'.\", UserWarning)\n",
    "            df_analysis[strat_col_name] = pd.NA # Mark as unusable\n",
    "            labels = ['Group 0', 'Group 1'] # Placeholder labels\n",
    "        elif strat_info['type'] == 'median_split':\n",
    "            # Ensure column is numeric before calculating median\n",
    "            numeric_col = pd.to_numeric(df_analysis[original_col], errors='coerce')\n",
    "            if numeric_col.isnull().all():\n",
    "                 warnings.warn(f\"Stratification column '{original_col}' for median split is all NaN or non-numeric. Skipping stratification '{strat_info['name']}'.\", UserWarning)\n",
    "                 df_analysis[strat_col_name] = pd.NA\n",
    "                 labels = ['Group 0', 'Group 1']\n",
    "            else:\n",
    "                 median_val = numeric_col.median()\n",
    "                 df_analysis[strat_col_name] = (numeric_col >= median_val).astype(int)\n",
    "                 labels = strat_info['labels']\n",
    "        elif strat_info['type'] == 'value_split':\n",
    "            cutoff_val = strat_info['cutoff']\n",
    "            # Ensure column is numeric before comparison\n",
    "            numeric_col = pd.to_numeric(df_analysis[original_col], errors='coerce')\n",
    "            if numeric_col.isnull().all():\n",
    "                 warnings.warn(f\"Stratification column '{original_col}' for value split is all NaN or non-numeric. Skipping stratification '{strat_info['name']}'.\", UserWarning)\n",
    "                 df_analysis[strat_col_name] = pd.NA\n",
    "                 labels = ['Group 0', 'Group 1']\n",
    "            else:\n",
    "                # Handle <= vs < based on common interpretation (e.g., BMI >= 30)\n",
    "                if strat_info['name'] == 'Followup Duration': # Special case <= 10 vs > 10\n",
    "                     df_analysis[strat_col_name] = (numeric_col > cutoff_val).astype(int)\n",
    "                     labels = strat_info['labels'] # ['<= 10 Days', '> 10 Days'] -> 0, 1\n",
    "                else: # Default is < cutoff vs >= cutoff\n",
    "                    df_analysis[strat_col_name] = (numeric_col >= cutoff_val).astype(int)\n",
    "                    labels = strat_info['labels'] # ['< Cutoff', '>= Cutoff'] -> 0, 1\n",
    "        elif strat_info['type'] == 'binary':\n",
    "             # Assuming column is already 0/1 or T/F. Convert T/F to 0/1 if necessary\n",
    "             col_series = df_analysis[original_col]\n",
    "             if col_series.dtype == bool:\n",
    "                 df_analysis[strat_col_name] = col_series.astype(int)\n",
    "             elif pd.api.types.is_numeric_dtype(col_series):\n",
    "                 # Check if it's only 0s and 1s (and NaNs)\n",
    "                 unique_vals = col_series.dropna().unique()\n",
    "                 if set(unique_vals).issubset({0, 1}):\n",
    "                      df_analysis[strat_col_name] = col_series\n",
    "                 else:\n",
    "                     warnings.warn(f\"Binary column {original_col} contains values other than 0/1. Stratification might be incorrect.\", UserWarning)\n",
    "                     df_analysis[strat_col_name] = pd.NA # Mark as unusable\n",
    "             else:\n",
    "                 # Handle potential 'yes'/'no' strings if needed (add conversion logic here if required)\n",
    "                 warnings.warn(f\"Binary column {original_col} is not numeric or boolean. Stratification might be incorrect.\", UserWarning)\n",
    "                 df_analysis[strat_col_name] = pd.NA # Mark as unusable\n",
    "             labels = strat_info['labels']\n",
    "        elif strat_info['type'] == 'isna':\n",
    "            df_analysis[strat_col_name] = df_analysis[original_col].notna().astype(int)\n",
    "            labels = strat_info['labels'] # ['Not Available', 'Available'] -> 0, 1\n",
    "        else:\n",
    "             warnings.warn(f\"Unknown stratification type: {strat_info['type']} for column {original_col}\", UserWarning)\n",
    "             df_analysis[strat_col_name] = pd.NA\n",
    "             labels = ['Group 0', 'Group 1']\n",
    "\n",
    "        strata_cols_mapping[strat_info['name']] = {'strat_col': strat_col_name, 'labels': labels}\n",
    "\n",
    "\n",
    "    # --- Iterate through Stratifications (Columns) ---\n",
    "    for strat_info in strata_config:\n",
    "        strat_name = strat_info['name']\n",
    "        # Check if mapping exists (might not if original column was missing)\n",
    "        if strat_name not in strata_cols_mapping:\n",
    "            continue\n",
    "        strat_details = strata_cols_mapping[strat_name]\n",
    "        strat_col = strat_details['strat_col']\n",
    "        labels = strat_details['labels']\n",
    "\n",
    "        if pd.isna(df_analysis[strat_col]).all():\n",
    "            warnings.warn(f\"Stratification column '{strat_col}' for '{strat_name}' could not be created or is all NA. Skipping this stratification.\", UserWarning)\n",
    "            continue\n",
    "\n",
    "        # Ensure labels list has at least two elements\n",
    "        if len(labels) < 2:\n",
    "            warnings.warn(f\"Insufficient labels provided for stratification '{strat_name}'. Skipping.\", UserWarning)\n",
    "            continue\n",
    "\n",
    "        group0_label = labels[0]\n",
    "        group1_label = labels[1]\n",
    "\n",
    "        # Create group subsets, dropping rows where the stratification variable is NA\n",
    "        df_strat_clean = df_analysis.dropna(subset=[strat_col])\n",
    "        group0_df = df_strat_clean[df_strat_clean[strat_col] == 0]\n",
    "        group1_df = df_strat_clean[df_strat_clean[strat_col] == 1]\n",
    "\n",
    "        if group0_df.empty or group1_df.empty:\n",
    "             warnings.warn(f\"One or both groups are empty for stratification '{strat_name}' after dropping NAs. Skipping.\", UserWarning)\n",
    "             continue\n",
    "\n",
    "        # --- Iterate through Variables (Rows) ---\n",
    "        for row_var, config in rows_config.items():\n",
    "            var_type = config['type']\n",
    "            var_format = config['format']\n",
    "\n",
    "            if row_var not in summary_results:\n",
    "                summary_results[row_var] = {}\n",
    "\n",
    "            # Handle the special 'N' row\n",
    "            if row_var == 'N':\n",
    "                stat_g0 = format_n_perc_pop(group0_df, total_population_n)\n",
    "                stat_g1 = format_n_perc_pop(group1_df, total_population_n)\n",
    "                p_val_formatted = \"N/A\" # No p-value for N counts\n",
    "            else:\n",
    "                # Check if row_var exists in the dataframe\n",
    "                if row_var not in df_analysis.columns:\n",
    "                     warnings.warn(f\"Row variable '{row_var}' not found in DataFrame. Skipping for stratification '{strat_name}'.\", UserWarning)\n",
    "                     stat_g0 = \"Var Missing\"\n",
    "                     stat_g1 = \"Var Missing\"\n",
    "                     p_val_formatted = \"N/A\"\n",
    "                else:\n",
    "                    series_g0 = group0_df[row_var]\n",
    "                    series_g1 = group1_df[row_var]\n",
    "\n",
    "                    # Calculate stats for each group\n",
    "                    if var_format == 'mean_sd':\n",
    "                        stat_g0 = format_mean_sd(series_g0)\n",
    "                        stat_g1 = format_mean_sd(series_g1)\n",
    "                    elif var_format == 'n_perc':\n",
    "                        stat_g0 = format_n_perc(series_g0)\n",
    "                        stat_g1 = format_n_perc(series_g1)\n",
    "                    else:\n",
    "                        stat_g0 = \"Invalid Format\"\n",
    "                        stat_g1 = \"Invalid Format\"\n",
    "\n",
    "                    # Calculate p-value\n",
    "                    p_val = calculate_p_value(series_g0, series_g1, var_type)\n",
    "                    p_val_formatted = format_p_value(p_val)\n",
    "\n",
    "            # Store results\n",
    "            summary_results[row_var][f\"{strat_name}: {group0_label}\"] = stat_g0\n",
    "            summary_results[row_var][f\"{strat_name}: {group1_label}\"] = stat_g1\n",
    "            summary_results[row_var][f\"{strat_name}: p-value\"] = p_val_formatted\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    summary_df = pd.DataFrame.from_dict(summary_results, orient='index')\n",
    "    summary_df.index.name = 'Variable' # Name the index\n",
    "\n",
    "    # Reorder columns to group by stratification\n",
    "    ordered_columns = []\n",
    "    for strat_info in strata_config:\n",
    "         strat_name = strat_info['name']\n",
    "         # Check if columns were actually created for this strat_name\n",
    "         col_prefix = f\"{strat_name}: \"\n",
    "         if any(col.startswith(col_prefix) for col in summary_df.columns):\n",
    "             # Check if mapping exists (might not if original column was missing)\n",
    "             if strat_name in strata_cols_mapping:\n",
    "                 labels = strata_cols_mapping[strat_name]['labels']\n",
    "                 if len(labels) >= 2:\n",
    "                     ordered_columns.append(f\"{strat_name}: {labels[0]}\")\n",
    "                     ordered_columns.append(f\"{strat_name}: {labels[1]}\")\n",
    "                     ordered_columns.append(f\"{strat_name}: p-value\")\n",
    "\n",
    "    # Ensure only existing columns are selected\n",
    "    final_columns = [col for col in ordered_columns if col in summary_df.columns]\n",
    "    summary_df = summary_df[final_columns]\n",
    "\n",
    "    # Ensure row order matches rows_config\n",
    "    summary_df = summary_df.reindex(rows_config.keys())\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    # ** IMPORTANT: Load your actual DataFrame here **\n",
    "    # Example: Assuming you have 'survival_analysis.sqlite' and 'sa_input_table'\n",
    "\n",
    "    # --- MODIFIED: Define output DB path and table name ---\n",
    "    # Assuming paper1_directory is defined in a previous cell\n",
    "    if 'paper1_directory' not in locals() and 'paper1_directory' not in globals():\n",
    "        # Define a default or raise an error if paper1_directory is crucial\n",
    "        # Example: Use current directory if not defined\n",
    "        paper1_directory = '.'\n",
    "        print(\"Warning: 'paper1_directory' not defined. Using current directory for output.\")\n",
    "        # Or raise NameError(\"'paper1_directory' is not defined. Please define it.\")\n",
    "\n",
    "    output_db_path = os.path.join(paper1_directory, 'survival_analysis.sqlite')\n",
    "    input_table_name = 'sa_input_table'\n",
    "    output_table_name = 'comparative_summary'\n",
    "    # --- END MODIFICATION ---\n",
    "\n",
    "    input_conn = None\n",
    "    output_conn = None\n",
    "\n",
    "    try:\n",
    "        print(f\"Connecting to input database: {output_db_path}\") # Use output_db_path as it contains the input table\n",
    "        if not os.path.exists(output_db_path):\n",
    "            raise FileNotFoundError(f\"Database file not found at {output_db_path}\")\n",
    "        input_conn = sqlite3.connect(output_db_path)\n",
    "        print(f\"Loading input table: {input_table_name}\")\n",
    "        input_df = pd.read_sql_query(f\"SELECT * FROM {input_table_name}\", input_conn)\n",
    "        input_conn.close() # Close input connection after loading\n",
    "\n",
    "        if input_df.empty:\n",
    "             print(f\"Warning: Input table '{input_table_name}' is empty. Cannot generate summary.\")\n",
    "        else:\n",
    "            # --- Data Preprocessing (Crucial Steps - Adapt as needed) ---\n",
    "            print(\"Starting data preprocessing...\")\n",
    "            # 1. Convert boolean-like columns if they are not already bool or 0/1\n",
    "            bool_cols = ['sex_f', 'hunger_yn', 'satiety_yn', 'emotional_eating_yn',\n",
    "                         '40d_dropout', '60d_dropout', '80d_dropout',\n",
    "                         '5%_wl_achieved', '10%_wl_achieved', '15%_wl_achieved']\n",
    "            for col in bool_cols:\n",
    "                if col in input_df.columns:\n",
    "                     if input_df[col].dtype == object: # If it's string 'yes'/'no' or similar\n",
    "                          # Example conversion (adjust based on your actual string values)\n",
    "                          input_df[col] = input_df[col].map({'yes': 1, 'no': 0, 'Yes': 1, 'No': 0, 'True': 1, 'False': 0, True: 1, False: 0, 1:1, 0:0}).astype(float) # Use float to allow NAs\n",
    "                     elif pd.api.types.is_bool_dtype(input_df[col].dtype):\n",
    "                          input_df[col] = input_df[col].astype(float) # Convert bool to float (0.0/1.0) to allow NAs if any exist\n",
    "                     elif pd.api.types.is_numeric_dtype(input_df[col].dtype):\n",
    "                          # Ensure it's float if it might contain NAs, otherwise check if 0/1\n",
    "                          unique_vals = input_df[col].dropna().unique()\n",
    "                          if not set(unique_vals).issubset({0, 1}):\n",
    "                               warnings.warn(f\"Column {col} intended as binary contains values other than 0/1.\", UserWarning)\n",
    "                          input_df[col] = input_df[col].astype(float) # Ensure float for consistency\n",
    "\n",
    "\n",
    "            # 2. Create 'genomics_available' column (based on 'genomics_sample_id')\n",
    "            if 'genomics_sample_id' in input_df.columns:\n",
    "                input_df['genomics_available'] = input_df['genomics_sample_id'].notna() # True if ID exists, False if NaN/None\n",
    "                input_df['genomics_available'] = input_df['genomics_available'].astype(float) # Convert to 0.0 / 1.0\n",
    "            else:\n",
    "                warnings.warn(\"'genomics_sample_id' column not found. Genomics stratification will be skipped.\", UserWarning)\n",
    "                # Remove genomics from strata_config if column doesn't exist\n",
    "                strata_config = [s for s in strata_config if s['column'] != 'genomics_sample_id']\n",
    "\n",
    "\n",
    "            # 3. Create 'instant_dropout' column\n",
    "            if 'total_followup_days' in input_df.columns:\n",
    "                 input_df['instant_dropout'] = (input_df['total_followup_days'] == 1) # Or <= 1 depending on definition\n",
    "                 input_df['instant_dropout'] = input_df['instant_dropout'].astype(float)\n",
    "            else:\n",
    "                 warnings.warn(\"'total_followup_days' column not found. 'instant_dropout' row cannot be calculated.\", UserWarning)\n",
    "                 # Remove instant_dropout from rows_config if column doesn't exist\n",
    "                 if 'instant_dropout' in rows_config: del rows_config['instant_dropout']\n",
    "\n",
    "\n",
    "            # 4. Ensure numeric types for continuous columns (handle potential errors)\n",
    "            num_cols = [r for r, c in rows_config.items() if c['type'] == 'continuous' and r != 'N']\n",
    "            for col in num_cols:\n",
    "                 if col in input_df.columns:\n",
    "                      input_df[col] = pd.to_numeric(input_df[col], errors='coerce')\n",
    "\n",
    "            print(\"Preprocessing finished.\")\n",
    "            # --- Generate the Summary Table ---\n",
    "            print(\"Generating comparative summary table...\")\n",
    "            comparative_summary_df = generate_comparative_summary(input_df, rows_config, strata_config)\n",
    "\n",
    "            # --- Display or Save the Table ---\n",
    "            print(\"Comparative Summary Table generated.\")\n",
    "            # Display the full table (might be very wide)\n",
    "            pd.set_option('display.max_rows', None)\n",
    "            pd.set_option('display.max_columns', None)\n",
    "            print(comparative_summary_df)\n",
    "\n",
    "            # --- MODIFIED: Save to SQLite database ---\n",
    "            if not comparative_summary_df.empty:\n",
    "                print(f\"\\nConnecting to output database: {output_db_path}\")\n",
    "                output_conn = sqlite3.connect(output_db_path)\n",
    "                try:\n",
    "                    print(f\"Saving comparative summary to table: {output_table_name}\")\n",
    "                    # Save DataFrame to SQL table. Use index=True to save the 'Variable' index as a column.\n",
    "                    comparative_summary_df.to_sql(output_table_name, output_conn, if_exists='replace', index=True)\n",
    "                    output_conn.commit()\n",
    "                    print(f\"Table '{output_table_name}' saved successfully to {output_db_path}.\")\n",
    "                except sqlite3.Error as e:\n",
    "                    print(f\"SQLite error saving table '{output_table_name}': {e}\")\n",
    "                    if output_conn: output_conn.rollback()\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred saving table '{output_table_name}': {e}\")\n",
    "                    if output_conn: output_conn.rollback()\n",
    "                finally:\n",
    "                    if output_conn:\n",
    "                        output_conn.close()\n",
    "                        print(\"Output database connection closed.\")\n",
    "            else:\n",
    "                print(\"Comparative summary DataFrame is empty. Nothing to save.\")\n",
    "            # --- END MODIFICATION ---\n",
    "\n",
    "            # Optionally save to CSV or Excel\n",
    "            # comparative_summary_df.to_csv(\"comparative_summary_table.csv\")\n",
    "            # comparative_summary_df.to_excel(\"comparative_summary_table.xlsx\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"SQLite error during connection or loading: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        # Ensure input connection is closed if it was opened and not closed yet\n",
    "        if input_conn:\n",
    "            try:\n",
    "                input_conn.close()\n",
    "                print(\"Input database connection closed.\")\n",
    "            except: # Handle cases where connection might already be closed or invalid\n",
    "                pass\n",
    "        # Ensure output connection is closed if it was opened and not closed yet\n",
    "        if output_conn:\n",
    "            try:\n",
    "                output_conn.close()\n",
    "                print(\"Output database connection closed.\")\n",
    "            except:\n",
    "                pass\n",
    "    print(\"Script finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanation of the first draft pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define scenarios for the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code of the first draft pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing script...\n",
      "Using pre-defined 'paper1_directory': C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\n",
      "========== Starting Linear Regression Pipeline ==========\n",
      "\n",
      "--- Step 1: Loading Data ---\n",
      "Loading data from C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\survival_analysis.sqlite, table sa_input_table...\n",
      "Loaded 1664 rows and 56 columns.\n",
      "Data loaded successfully: 1664 rows, 56 columns.\n",
      "\n",
      "--- Step 2: Preprocessing Data ---\n",
      "Preprocessing data for Linear models...\n",
      "Preprocessing complete.\n",
      "Data preprocessed successfully. Resulting shape: 1664 rows, 56 columns.\n",
      "\n",
      "--- Step 3: Processing Scenarios ---\n",
      "\n",
      "--- Processing Scenario: total_wl_hunger_unadjusted ---\n",
      "  Starting processing for scenario: total_wl_hunger_unadjusted\n",
      "    Outcome variable: 'total_wl_%'\n",
      "    Preparing data subset for the model...\n",
      "      Selected columns: ['total_wl_%', 'hunger_yn']\n",
      "      Removed 0 rows with missing values (out of 1664). Final dataset size for model: 1664 rows.\n",
      "    Running linear regression model for scenario 'total_wl_hunger_unadjusted'...\n",
      "  Running Linear model (OLS) for scenario: total_wl_hunger_unadjusted\n",
      "    Outcome: total_wl_%\n",
      "    Predictors: ['hunger_yn']\n",
      "    Data shape: (1664, 2)\n",
      "    Model fitting complete.\n",
      "    Model for scenario 'total_wl_hunger_unadjusted' ran successfully.\n",
      "  --- Scenario total_wl_hunger_unadjusted completed successfully. Results collected. ---\n",
      "\n",
      "--- Processing Scenario: 60d_wl_emot_val_adjusted ---\n",
      "  Starting processing for scenario: 60d_wl_emot_val_adjusted\n",
      "    Outcome variable: 'wl_60d_%'\n",
      "    Preparing data subset for the model...\n",
      "      Selected columns: ['wl_60d_%', 'emotional_eating_value_likert', 'age', 'sex_f', 'baseline_bmi']\n",
      "      Removed 1072 rows with missing values (out of 1664). Final dataset size for model: 592 rows.\n",
      "    Running linear regression model for scenario '60d_wl_emot_val_adjusted'...\n",
      "  Running Linear model (OLS) for scenario: 60d_wl_emot_val_adjusted\n",
      "    Outcome: wl_60d_%\n",
      "    Predictors: ['emotional_eating_value_likert', 'age', 'sex_f', 'baseline_bmi']\n",
      "    Data shape: (592, 5)\n",
      "    Model fitting complete.\n",
      "    Model for scenario '60d_wl_emot_val_adjusted' ran successfully.\n",
      "  --- Scenario 60d_wl_emot_val_adjusted completed successfully. Results collected. ---\n",
      "\n",
      "--- Processing Scenario: 60d_wl_quant_ctrl_adjusted ---\n",
      "  Starting processing for scenario: 60d_wl_quant_ctrl_adjusted\n",
      "    Outcome variable: 'wl_60d_%'\n",
      "    Preparing data subset for the model...\n",
      "      Selected columns: ['wl_60d_%', 'quantity_control_likert', 'age', 'sex_f', 'baseline_bmi']\n",
      "      Removed 1072 rows with missing values (out of 1664). Final dataset size for model: 592 rows.\n",
      "    Running linear regression model for scenario '60d_wl_quant_ctrl_adjusted'...\n",
      "  Running Linear model (OLS) for scenario: 60d_wl_quant_ctrl_adjusted\n",
      "    Outcome: wl_60d_%\n",
      "    Predictors: ['quantity_control_likert', 'age', 'sex_f', 'baseline_bmi']\n",
      "    Data shape: (592, 5)\n",
      "    Model fitting complete.\n",
      "    Model for scenario '60d_wl_quant_ctrl_adjusted' ran successfully.\n",
      "  --- Scenario 60d_wl_quant_ctrl_adjusted completed successfully. Results collected. ---\n",
      "\n",
      "--- Step 4: Aggregating and Saving Results ---\n",
      "Aggregated results from 3 successful scenarios.\n",
      "Attempting to connect to database: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\survival_analysis.sqlite\n",
      "Saving aggregated results to table: lin_regression_results\n",
      "Results saved successfully to the database.\n",
      "Database connection closed.\n",
      "\n",
      "========== Linear Regression Pipeline Finished ==========\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# Linear Regression Pipeline Module\n",
    "####\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import warnings\n",
    "import traceback # Keep traceback for detailed error reporting\n",
    "\n",
    "# Suppress potential warnings (optional, adjust as needed)\n",
    "# warnings.simplefilter('ignore', SomeWarningCategory)\n",
    "\n",
    "# Ensure paper1_directory is defined in a previous cell\n",
    "if 'paper1_directory' not in locals() and 'paper1_directory' not in globals():\n",
    "    # Try to get it from the environment if running as a script, or raise error\n",
    "    paper1_directory = os.getenv('PAPER1_DIRECTORY')\n",
    "    if paper1_directory is None:\n",
    "        raise NameError(\"'paper1_directory' variable not defined. Please ensure it is defined or set as an environment variable 'PAPER1_DIRECTORY'.\")\n",
    "    else:\n",
    "        print(f\"Using paper1_directory from environment variable: {paper1_directory}\")\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "\n",
    "# Adjust these paths as needed\n",
    "DB_PATH = os.path.join(paper1_directory, 'survival_analysis.sqlite') # Path to the SQLite database\n",
    "INPUT_TABLE_NAME = \"sa_input_table\" # Your wide input table\n",
    "OUTPUT_TABLE_NAME = \"lin_regression_results\" # Table to store summary results\n",
    "\n",
    "# --- Define Scenarios (Example - Adapt as needed) ---\n",
    "# The structure is similar to logistic regression, but 'outcome_type' implicitly means predicting wl_%\n",
    "# 'target_perc' is not used here.\n",
    "# 'time_window' determines which wl_% column to use ('total', 40, 60, etc.)\n",
    "LINREG_SCENARIOS = [\n",
    "    # Example: Predict total weight loss % using hunger (unadjusted)\n",
    "    {\n",
    "        'name': 'total_wl_hunger_unadjusted',\n",
    "        'time_window': 'total', # Use 'total_wl_%'\n",
    "        'predictors': ['hunger_yn'],\n",
    "        'covariates': []\n",
    "    },\n",
    "    # Example: Predict 60-day weight loss % using emotional eating value (adjusted)\n",
    "    {\n",
    "        'name': '60d_wl_emot_val_adjusted',\n",
    "        'time_window': 60, # Use 'wl_60d_%'\n",
    "        'predictors': ['emotional_eating_value_likert'],\n",
    "        'covariates': ['age', 'sex_f', 'baseline_bmi']\n",
    "    },\n",
    "    # Example: Predict 60-day weight loss % using quantity control (adjusted)\n",
    "    {\n",
    "        'name': '60d_wl_quant_ctrl_adjusted',\n",
    "        'time_window': 60, # Use 'wl_60d_%'\n",
    "        'predictors': ['quantity_control_likert'],\n",
    "        'covariates': ['age', 'sex_f', 'baseline_bmi']\n",
    "    },\n",
    "    # Add more scenarios following this pattern\n",
    "]\n",
    "\n",
    "# --- HELPERS ---\n",
    "\n",
    "def load_data(db_path, table_name):\n",
    "    \"\"\"Loads data from the specified SQLite database table.\"\"\"\n",
    "    print(f\"Loading data from {db_path}, table {table_name}...\")\n",
    "    if not os.path.exists(db_path):\n",
    "        raise FileNotFoundError(f\"Database file not found at {db_path}\")\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n",
    "        print(f\"Loaded {len(df)} rows and {len(df.columns)} columns.\")\n",
    "        if df.empty:\n",
    "             print(f\"Warning: Loaded DataFrame from {table_name} is empty.\")\n",
    "        return df\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"SQLite error loading data: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred loading data: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "def preprocess_data(df, scenarios):\n",
    "    \"\"\"Handles basic data preparation for linear models.\"\"\"\n",
    "    print(\"Preprocessing data for Linear models...\")\n",
    "    if df.empty:\n",
    "         print(\"  Input DataFrame is empty. Skipping preprocessing.\")\n",
    "         return df\n",
    "\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # Identify all unique predictor/covariate columns needed across scenarios\n",
    "    all_model_cols = set()\n",
    "    bool_predictors = set() # Specifically track yes/no predictors\n",
    "    for scenario in scenarios:\n",
    "        all_model_cols.update(scenario['predictors'])\n",
    "        all_model_cols.update(scenario['covariates'])\n",
    "        # Identify boolean predictors based on common naming or explicit list\n",
    "        for pred in scenario['predictors']:\n",
    "            if pred.endswith('_yn'): # Assuming '_yn' suffix for boolean predictors\n",
    "                 bool_predictors.add(pred)\n",
    "\n",
    "    # Ensure boolean predictors are 0/1 integers\n",
    "    for col in bool_predictors:\n",
    "        if col in df_processed.columns:\n",
    "            if not pd.api.types.is_numeric_dtype(df_processed[col]) or not df_processed[col].dropna().isin([0, 1]).all():\n",
    "                print(f\"  Attempting to convert boolean predictor '{col}' to 0/1 integer.\")\n",
    "                # Handle potential 'yes'/'no' strings or True/False bools\n",
    "                if df_processed[col].dtype == 'object':\n",
    "                    df_processed[col] = df_processed[col].str.lower().map({'yes': 1, 'no': 0, 'y': 1, 'n': 0})\n",
    "                elif df_processed[col].dtype == 'bool':\n",
    "                     df_processed[col] = df_processed[col].astype(int)\n",
    "\n",
    "                # Convert to numeric, coercing errors\n",
    "                df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')\n",
    "                if not df_processed[col].dropna().isin([0, 1]).all():\n",
    "                     print(f\"    Warning: Column '{col}' could not be reliably converted to 0/1.\")\n",
    "            # Convert float 0.0/1.0 to integer\n",
    "            elif pd.api.types.is_float_dtype(df_processed[col]) and df_processed[col].dropna().isin([0.0, 1.0]).all():\n",
    "                 df_processed[col] = df_processed[col].astype('Int64') # Use nullable integer\n",
    "        else:\n",
    "             print(f\"  Warning: Boolean predictor column '{col}' specified in scenarios but not found.\")\n",
    "\n",
    "    # Ensure other predictors/covariates AND potential outcome columns are numeric\n",
    "    numeric_cols_to_check = all_model_cols - bool_predictors\n",
    "    # Add potential weight loss outcome columns dynamically based on scenarios\n",
    "    for scenario in scenarios:\n",
    "        tw = scenario['time_window']\n",
    "        wl_col = f\"wl_{tw}d_%\" if isinstance(tw, int) else \"total_wl_%\"\n",
    "        numeric_cols_to_check.add(wl_col)\n",
    "\n",
    "    for col in numeric_cols_to_check:\n",
    "        if col in df_processed.columns:\n",
    "            if not pd.api.types.is_numeric_dtype(df_processed[col]):\n",
    "                print(f\"  Attempting to convert column '{col}' to numeric.\")\n",
    "                original_nan_count = df_processed[col].isnull().sum()\n",
    "                df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')\n",
    "                if df_processed[col].isnull().sum() > original_nan_count:\n",
    "                    print(f\"    Warning: Conversion of column '{col}' introduced NaNs.\")\n",
    "        # Don't warn if column doesn't exist, it might only be needed for specific scenarios checked later\n",
    "\n",
    "    print(\"Preprocessing complete.\")\n",
    "    return df_processed\n",
    "\n",
    "\n",
    "# --- LINEAR REGRESSION MODULE ---\n",
    "\n",
    "def run_linear_model(df_subset, outcome_col, predictor_cols, scenario_name):\n",
    "    \"\"\"\n",
    "    Fits an OLS Linear Regression model using statsmodels and extracts results.\n",
    "\n",
    "    Args:\n",
    "        df_subset (pd.DataFrame): DataFrame containing only necessary columns and no NaNs for the model.\n",
    "        outcome_col (str): Name of the continuous outcome column (e.g., 'wl_60d_%').\n",
    "        predictor_cols (list): List of predictor and covariate column names.\n",
    "        scenario_name (str): Name of the scenario for labeling outputs.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing model summary results (Coefficient, CI, p-value per variable),\n",
    "                      or None if the model fails or data is insufficient.\n",
    "    \"\"\"\n",
    "    print(f\"  Running Linear model (OLS) for scenario: {scenario_name}\")\n",
    "    print(f\"    Outcome: {outcome_col}\")\n",
    "    print(f\"    Predictors: {predictor_cols}\")\n",
    "    print(f\"    Data shape: {df_subset.shape}\")\n",
    "\n",
    "    # Check for sufficient data\n",
    "    min_obs_per_predictor = 10\n",
    "    required_obs = min_obs_per_predictor * len(predictor_cols) if predictor_cols else 20\n",
    "    # Need at least k+1 observations for k predictors + intercept\n",
    "    required_obs = max(required_obs, len(predictor_cols) + 2)\n",
    "    if df_subset.shape[0] < required_obs:\n",
    "        print(f\"    Warning: Insufficient data ({df_subset.shape[0]} obs) for {len(predictor_cols)} predictors. Need at least {required_obs}. Skipping model.\")\n",
    "        return None\n",
    "    if not predictor_cols:\n",
    "        print(\"    Warning: No predictors specified. Skipping model.\")\n",
    "        return None\n",
    "    # Check if outcome column has variance\n",
    "    if outcome_col not in df_subset or df_subset[outcome_col].var() == 0:\n",
    "        print(f\"    Warning: Outcome column '{outcome_col}' has zero variance or is missing. Skipping model.\")\n",
    "        return None\n",
    "\n",
    "    results_list = []\n",
    "\n",
    "    try:\n",
    "        # Prepare data for statsmodels (add constant for intercept)\n",
    "        X = df_subset[predictor_cols]\n",
    "        y = df_subset[outcome_col]\n",
    "        X = sm.add_constant(X, has_constant='add') # Add intercept\n",
    "\n",
    "        # Fit the model\n",
    "        ols_model = sm.OLS(y, X)\n",
    "        result = ols_model.fit()\n",
    "\n",
    "        # Extract Results\n",
    "        params = result.params\n",
    "        conf = result.conf_int()\n",
    "        conf.columns = ['CI Lower 95%', 'CI Upper 95%']\n",
    "        p_values = result.pvalues\n",
    "        t_values = result.tvalues # Get t-statistics\n",
    "\n",
    "        # Combine results into a DataFrame\n",
    "        summary_df = pd.DataFrame({\n",
    "            'Variable': params.index,\n",
    "            'Coefficient': params.values,\n",
    "            'Std_Err': result.bse.values, # Standard Error\n",
    "            'T_Statistic': t_values.values,\n",
    "            'P_Value': p_values.values,\n",
    "            'CI Lower 95%': conf['CI Lower 95%'],\n",
    "            'CI Upper 95%': conf['CI Upper 95%'],\n",
    "        })\n",
    "\n",
    "        # Add model-level stats\n",
    "        model_stats = {\n",
    "            'scenario_name': scenario_name,\n",
    "            'outcome_variable': outcome_col,\n",
    "            'n_observations': int(result.nobs),\n",
    "            'r_squared': result.rsquared,\n",
    "            'adj_r_squared': result.rsquared_adj,\n",
    "            'f_statistic': result.fvalue,\n",
    "            'f_p_value': result.f_pvalue,\n",
    "            'aic': result.aic,\n",
    "            'bic': result.bic,\n",
    "            'condition_number': result.condition_number, # Indicator for multicollinearity\n",
    "            # 'converged': True # OLS typically doesn't have convergence issues like iterative methods\n",
    "        }\n",
    "\n",
    "        # Add model stats to each row of the summary_df for easy aggregation\n",
    "        for col, val in model_stats.items():\n",
    "            summary_df[col] = val\n",
    "\n",
    "        print(\"    Model fitting complete.\")\n",
    "        return summary_df\n",
    "\n",
    "    except np.linalg.LinAlgError as e_linalg:\n",
    "         print(f\"    ERROR: Linear algebra error (likely perfect multicollinearity) for scenario {scenario_name}. Skipping. Error: {e_linalg}\")\n",
    "         return None\n",
    "    except ValueError as e_val:\n",
    "         # This might happen if data isn't purely numeric after preprocessing somehow\n",
    "         print(f\"    ERROR: Value error during model fitting (check data types/values) for scenario {scenario_name}. Skipping. Error: {e_val}\")\n",
    "         return None\n",
    "    except Exception as e_fit:\n",
    "        print(f\"    ERROR: Failed to fit Linear model for scenario {scenario_name}. Error: {e_fit}\")\n",
    "        print(traceback.format_exc()) # Print traceback for debugging\n",
    "        return None\n",
    "\n",
    "# --- Scenario Processing Function ---\n",
    "\n",
    "def process_scenario(scenario, df_analysis):\n",
    "    \"\"\"\n",
    "    Processes a single linear regression scenario.\n",
    "\n",
    "    Args:\n",
    "        scenario (dict): Dictionary containing scenario configuration.\n",
    "        df_analysis (pd.DataFrame): The preprocessed DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame or None: DataFrame with model results for the scenario,\n",
    "                              or None if processing fails or is skipped.\n",
    "    \"\"\"\n",
    "    scenario_name = scenario['name']\n",
    "    print(f\"  Starting processing for scenario: {scenario_name}\")\n",
    "\n",
    "    time_window = scenario['time_window']\n",
    "    predictors = scenario['predictors']\n",
    "    covariates = scenario['covariates']\n",
    "    all_predictors = predictors + covariates\n",
    "\n",
    "    # --- Define Outcome Variable Dynamically ---\n",
    "    # Determine the source weight loss column based on time_window\n",
    "    if time_window == 'total':\n",
    "        outcome_col_name = 'total_wl_%'\n",
    "    elif isinstance(time_window, int):\n",
    "        outcome_col_name = f'wl_{time_window}d_%'\n",
    "    else:\n",
    "        print(f\"    ERROR: Invalid 'time_window' ({time_window}) in scenario '{scenario_name}'. Use 'total' or integer days. Skipping scenario.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"    Outcome variable: '{outcome_col_name}'\")\n",
    "\n",
    "    # --- Prepare Data for Model ---\n",
    "    print(\"    Preparing data subset for the model...\")\n",
    "    # Check if all necessary columns exist\n",
    "    required_cols = [outcome_col_name] + all_predictors\n",
    "    missing_cols = [col for col in required_cols if col not in df_analysis.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"    ERROR: Missing required columns for scenario '{scenario_name}': {missing_cols}. Skipping scenario.\")\n",
    "        return None\n",
    "\n",
    "    # Select relevant columns and drop rows with missing values in predictors/covariates OR outcome\n",
    "    initial_rows = len(df_analysis)\n",
    "    # Use a copy to avoid modifying df_analysis if needed elsewhere, though not strictly necessary here\n",
    "    temp_df = df_analysis.copy()\n",
    "    df_model_subset = temp_df[required_cols].dropna()\n",
    "    final_rows = len(df_model_subset)\n",
    "    print(f\"      Selected columns: {required_cols}\")\n",
    "    print(f\"      Removed {initial_rows - final_rows} rows with missing values (out of {initial_rows}). Final dataset size for model: {final_rows} rows.\")\n",
    "\n",
    "    if final_rows == 0:\n",
    "        print(f\"    ERROR: No valid data remaining after handling missing values for scenario '{scenario_name}'. Skipping scenario.\")\n",
    "        return None\n",
    "    if df_model_subset[outcome_col_name].var() == 0:\n",
    "        print(f\"    ERROR: Outcome variable '{outcome_col_name}' has zero variance after filtering. Cannot run linear regression. Skipping scenario.\")\n",
    "        return None\n",
    "\n",
    "    # --- Run the Linear Model ---\n",
    "    print(f\"    Running linear regression model for scenario '{scenario_name}'...\")\n",
    "    model_output_df = None\n",
    "    try:\n",
    "        model_output_df = run_linear_model(\n",
    "            df_model_subset,\n",
    "            outcome_col_name,\n",
    "            all_predictors,\n",
    "            scenario_name\n",
    "        )\n",
    "    except Exception as e_model:\n",
    "        print(f\"    ERROR: An exception occurred during model execution for scenario '{scenario_name}': {e_model}\")\n",
    "        print(traceback.format_exc()) # Print traceback for debugging\n",
    "        return None # Indicate failure\n",
    "\n",
    "    if model_output_df is not None and not model_output_df.empty:\n",
    "        print(f\"    Model for scenario '{scenario_name}' ran successfully.\")\n",
    "        return model_output_df\n",
    "    else:\n",
    "         print(f\"    Model for scenario '{scenario_name}' did not produce valid results (returned None or empty DataFrame).\")\n",
    "         return None\n",
    "\n",
    "# --- ORCHESTRATION ---\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate the Linear Regression pipeline.\"\"\"\n",
    "    print(\"========== Starting Linear Regression Pipeline ==========\")\n",
    "    all_results_summary = [] # To store summary stats from each model\n",
    "\n",
    "    try:\n",
    "        # 1. Load Data\n",
    "        print(\"\\n--- Step 1: Loading Data ---\")\n",
    "        df_raw = load_data(DB_PATH, INPUT_TABLE_NAME)\n",
    "        if df_raw is None: # Assume load_data returns None on error\n",
    "             print(\"ERROR: Failed to load data. Stopping pipeline.\")\n",
    "             return # Exit main function\n",
    "        if df_raw.empty:\n",
    "             print(\"ERROR: Input data table is empty. Stopping pipeline.\")\n",
    "             return # Exit main function\n",
    "        print(f\"Data loaded successfully: {df_raw.shape[0]} rows, {df_raw.shape[1]} columns.\")\n",
    "\n",
    "        # 2. Preprocess Data\n",
    "        print(\"\\n--- Step 2: Preprocessing Data ---\")\n",
    "        # Pass the specific scenarios for this pipeline\n",
    "        df_analysis = preprocess_data(df_raw, LINREG_SCENARIOS)\n",
    "        if df_analysis is None: # Assume preprocess_data returns None on error\n",
    "            print(\"ERROR: Failed during data preprocessing. Stopping pipeline.\")\n",
    "            return # Exit main function\n",
    "        if df_analysis.empty:\n",
    "             print(\"ERROR: Data preprocessing resulted in an empty DataFrame. Stopping pipeline.\")\n",
    "             return # Exit main function\n",
    "        print(f\"Data preprocessed successfully. Resulting shape: {df_analysis.shape[0]} rows, {df_analysis.shape[1]} columns.\")\n",
    "\n",
    "        # 3. Loop through scenarios and run models\n",
    "        print(\"\\n--- Step 3: Processing Scenarios ---\")\n",
    "        for scenario in LINREG_SCENARIOS:\n",
    "            print(f\"\\n--- Processing Scenario: {scenario.get('name', 'Unnamed Scenario')} ---\") # Use .get for safety\n",
    "\n",
    "            # Call the dedicated function to process this scenario\n",
    "            scenario_result = process_scenario(scenario, df_analysis)\n",
    "\n",
    "            # Collect results if model ran successfully\n",
    "            if scenario_result is not None and not scenario_result.empty:\n",
    "                all_results_summary.append(scenario_result)\n",
    "                print(f\"  --- Scenario {scenario.get('name', 'Unnamed Scenario')} completed successfully. Results collected. ---\")\n",
    "            else:\n",
    "                 print(f\"  --- Scenario {scenario.get('name', 'Unnamed Scenario')} did not produce valid results or was skipped. ---\")\n",
    "\n",
    "        # 4. Save Aggregated Results\n",
    "        print(\"\\n--- Step 4: Aggregating and Saving Results ---\")\n",
    "        if all_results_summary:\n",
    "            # Concatenate all results DataFrames\n",
    "            results_df = pd.concat(all_results_summary, ignore_index=True)\n",
    "            print(f\"Aggregated results from {len(all_results_summary)} successful scenarios.\")\n",
    "\n",
    "            conn_out = None\n",
    "            try:\n",
    "                print(f\"Attempting to connect to database: {DB_PATH}\")\n",
    "                conn_out = sqlite3.connect(DB_PATH)\n",
    "                print(f\"Saving aggregated results to table: {OUTPUT_TABLE_NAME}\")\n",
    "                # Use the specific output table name for linear regression\n",
    "                results_df.to_sql(OUTPUT_TABLE_NAME, conn_out, if_exists='replace', index=False)\n",
    "                conn_out.commit()\n",
    "                print(\"Results saved successfully to the database.\")\n",
    "            except sqlite3.Error as e_sql:\n",
    "                print(f\"ERROR: SQLite error occurred while saving results: {e_sql}\")\n",
    "                if conn_out:\n",
    "                    try:\n",
    "                        conn_out.rollback()\n",
    "                        print(\"Database transaction rolled back.\")\n",
    "                    except sqlite3.Error as e_rb:\n",
    "                        print(f\"ERROR: Could not rollback transaction: {e_rb}\")\n",
    "            except Exception as e_save:\n",
    "                print(f\"ERROR: An unexpected error occurred saving results: {e_save}\")\n",
    "                if conn_out:\n",
    "                    try:\n",
    "                        conn_out.rollback()\n",
    "                        print(\"Database transaction rolled back.\")\n",
    "                    except sqlite3.Error as e_rb:\n",
    "                         print(f\"ERROR: Could not rollback transaction: {e_rb}\")\n",
    "                print(traceback.format_exc())\n",
    "            finally:\n",
    "                if conn_out:\n",
    "                    conn_out.close()\n",
    "                    print(\"Database connection closed.\")\n",
    "        else:\n",
    "            print(\"No valid model results were generated across all scenarios. Nothing to save.\")\n",
    "\n",
    "    except FileNotFoundError as e_fnf:\n",
    "        print(f\"ERROR: File not found during pipeline execution: {e_fnf}\")\n",
    "    except sqlite3.Error as e_sql_main:\n",
    "        print(f\"ERROR: Database error during setup or initial loading: {e_sql_main}\")\n",
    "    except NameError as e_name:\n",
    "         print(f\"ERROR: A required variable or function might be missing or misspelled: {e_name}\")\n",
    "    except KeyError as e_key:\n",
    "         print(f\"ERROR: Missing expected key in scenario configuration or data: {e_key}\")\n",
    "    except Exception as e_main:\n",
    "        print(f\"ERROR: An unexpected error occurred in the main pipeline orchestration: {e_main}\")\n",
    "        print(\"--- Traceback ---\")\n",
    "        print(traceback.format_exc())\n",
    "        print(\"--- End Traceback ---\")\n",
    "    finally:\n",
    "        print(\"\\n========== Linear Regression Pipeline Finished ==========\")\n",
    "\n",
    "\n",
    "# --- EXECUTION ---\n",
    "# This block should be outside the main() function definition\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "     print(\"Executing script...\")\n",
    "     # Check if running as script vs notebook cell where paper1_directory might be predefined\n",
    "     if 'paper1_directory' not in locals() and 'paper1_directory' not in globals():\n",
    "          print(\"'paper1_directory' not found in local/global scope. Checking environment variable...\")\n",
    "          # Attempt to retrieve from environment variable\n",
    "          paper1_directory = os.getenv('PAPER1_DIRECTORY')\n",
    "          if paper1_directory is None:\n",
    "               print(\"ERROR: 'paper1_directory' variable not defined and 'PAPER1_DIRECTORY' environment variable not set. Cannot run main().\")\n",
    "               # Depending on your setup, you might want to exit here:\n",
    "               # import sys\n",
    "               # sys.exit(1)\n",
    "          else:\n",
    "               print(f\"Using 'paper1_directory' from environment variable: {paper1_directory}\")\n",
    "               main() # Call main only if directory is found\n",
    "     else:\n",
    "          # 'paper1_directory' is already defined (e.g., in a notebook cell)\n",
    "          print(f\"Using pre-defined 'paper1_directory': {paper1_directory}\")\n",
    "          main() # Call main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanation of the first draft pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression Pipeline Summary\n",
    "\n",
    "This Python code defines and executes a pipeline for running multiple logistic regression analyses based on a predefined set of scenarios.\n",
    "\n",
    "**Structure:**\n",
    "\n",
    "1.  **Configuration:**\n",
    "    *   Imports necessary libraries (`pandas`, `sqlite3`, `os`, `statsmodels`, `numpy`, `warnings`).\n",
    "    *   Suppresses `ConvergenceWarning` from `statsmodels`.\n",
    "    *   Checks for and sets up the `paper1_directory` variable.\n",
    "    *   Defines constants for the database path (`DB_PATH`), input table name (`INPUT_TABLE_NAME`), and output table name (`OUTPUT_TABLE_NAME`).\n",
    "    *   Relies on the `LOGREG_SCENARIOS` list (defined in a previous cell) which contains dictionaries, each specifying the parameters for a single logistic regression model (outcome, predictors, covariates, time window, etc.).\n",
    "\n",
    "2.  **Helper Functions:**\n",
    "    *   `load_data(db_path, table_name)`: Connects to the specified SQLite database and loads the input table into a pandas DataFrame. Includes error handling for file not found and database errors.\n",
    "    *   `preprocess_data(df, scenarios)`: Performs initial data cleaning on the loaded DataFrame. It identifies all necessary predictor and covariate columns from the scenarios, attempts to convert boolean-like columns (e.g., ending in `_yn`) to 0/1 integers, and converts other specified columns to numeric types, handling potential errors.\n",
    "\n",
    "3.  **Core Modeling Function:**\n",
    "    *   `run_logistic_model(df_subset, outcome_col, predictor_cols, scenario_name)`:\n",
    "        *   Takes a prepared DataFrame subset, the outcome variable name, and predictor names.\n",
    "        *   Checks for sufficient data and variance in the outcome variable.\n",
    "        *   Uses `statsmodels.api.Logit` to fit the logistic regression model (adding a constant for the intercept).\n",
    "        *   Extracts key results: Coefficients, Odds Ratios (by exponentiating coefficients), 95% Confidence Intervals for ORs, p-values, standard errors, and model-level statistics (N, Log-Likelihood, Pseudo R-squared, AIC, BIC, convergence status).\n",
    "        *   Returns these results formatted as a pandas DataFrame.\n",
    "        *   Includes robust error handling for model fitting issues (e.g., `LinAlgError` for multicollinearity, `ValueError`).\n",
    "\n",
    "4.  **Scenario Processing Function:**\n",
    "    *   `process_scenario(scenario, df_analysis)`:\n",
    "        *   Takes a single scenario dictionary and the preprocessed DataFrame.\n",
    "        *   Dynamically creates the binary outcome variable (e.g., `60d_10p_hunger_unadjusted_outcome`) based on the scenario's `outcome_type`, `target_perc`, and `time_window` specifications (handling weight loss targets and dropout definitions).\n",
    "        *   Selects the required columns (outcome, predictors, covariates) for the specific scenario.\n",
    "        *   Handles missing values (`dropna()`) for the selected subset.\n",
    "        *   Calls `run_logistic_model` with the prepared data for this scenario.\n",
    "        *   Returns the results DataFrame from `run_logistic_model` or `None` if any step fails.\n",
    "\n",
    "5.  **Orchestration Function:**\n",
    "    *   `main()`:\n",
    "        *   Coordinates the entire pipeline flow.\n",
    "        *   Calls `load_data` to get the raw data.\n",
    "        *   Calls `preprocess_data` to clean the data.\n",
    "        *   Iterates through each `scenario` defined in `LOGREG_SCENARIOS`.\n",
    "        *   For each scenario, calls `process_scenario`.\n",
    "        *   Collects the results DataFrames from successfully processed scenarios.\n",
    "        *   If any results were generated, concatenates them into a single DataFrame.\n",
    "        *   Connects to the SQLite database and saves the aggregated results DataFrame to the specified `OUTPUT_TABLE_NAME`, replacing it if it exists.\n",
    "        *   Includes overall error handling for the pipeline steps.\n",
    "\n",
    "6.  **Execution Block:**\n",
    "    *   The `if __name__ == \"__main__\":` block ensures that the `main()` function is called when the code is executed as a script. It also includes logic to handle the definition of `paper1_directory` if it wasn't set previously (e.g., when running in a notebook).\n",
    "\n",
    "**Workflow:**\n",
    "\n",
    "The script executes the `main` function, which orchestrates these steps: Load -> Preprocess -> Loop through Scenarios (Define Outcome -> Prepare Subset -> Run Model) -> Aggregate Results -> Save Results. Each scenario defined in `LOGREG_SCENARIOS` results in a separate logistic regression model being fitted and its summary statistics being saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define scenarios for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the scenarios to run for Logistic Regression\n",
    "# outcome_type: 'target_wl' or 'dropout'\n",
    "# target_perc: % WL target (used if outcome_type='target_wl')\n",
    "# time_window: Time window in days ('total', 40, 60, 80, etc.) or 'instant' for dropout\n",
    "# predictors: List of main predictors (eating behaviors)\n",
    "# covariates: List of adjustment covariates\n",
    "\n",
    "LOGREG_SCENARIOS = [\n",
    "    # 60 days, 10%, eating behavior, UNADJUSTED models\n",
    "    {\n",
    "        'name': '60d_10p_hunger_unadjusted',\n",
    "        'outcome_type': 'target_wl',\n",
    "        'target_perc': 10,\n",
    "        'time_window': 60,\n",
    "        'predictors': ['hunger_yn'],\n",
    "        'covariates': []\n",
    "    },\n",
    "    {\n",
    "        'name': '60d_10p_satiety_unadjusted',\n",
    "        'outcome_type': 'target_wl',\n",
    "        'target_perc': 10,\n",
    "        'time_window': 60,\n",
    "        'predictors': ['satiety_yn'],\n",
    "        'covariates': []\n",
    "    },\n",
    "    {\n",
    "        'name': '60d_10p_emotional_eating_unadjusted',\n",
    "        'outcome_type': 'target_wl',\n",
    "        'target_perc': 10,\n",
    "        'time_window': 60,\n",
    "        'predictors': ['emotional_eating_yn'],\n",
    "        'covariates': []\n",
    "    },\n",
    "    {\n",
    "        'name': '60d_10p_emotional_eating_value_unadjusted',\n",
    "        'outcome_type': 'target_wl',\n",
    "        'target_perc': 10,\n",
    "        'time_window': 60,\n",
    "        'predictors': ['emotional_eating_value_likert'],\n",
    "        'covariates': []\n",
    "    },\n",
    "    {\n",
    "        'name': '60d_10p_quantity_control_unadjusted',\n",
    "        'outcome_type': 'target_wl',\n",
    "        'target_perc': 10,\n",
    "        'time_window': 60,\n",
    "        'predictors': ['quantity_control_likert'],\n",
    "        'covariates': []\n",
    "    },\n",
    "    {\n",
    "        'name': '60d_10p_impulse_control_unadjusted',\n",
    "        'outcome_type': 'target_wl',\n",
    "        'target_perc': 10,\n",
    "        'time_window': 60,\n",
    "        'predictors': ['impulse_control_likert'],\n",
    "        'covariates': []\n",
    "    },\n",
    "\n",
    "    # 60 days, 10%, eating behavior, ADJUSTED models\n",
    "    {\n",
    "        'name': '60d_10p_hunger_adjusted',\n",
    "        'outcome_type': 'target_wl',\n",
    "        'target_perc': 10,\n",
    "        'time_window': 60,\n",
    "        'predictors': ['hunger_yn'],\n",
    "        'covariates': ['age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi']\n",
    "    },\n",
    "    {\n",
    "        'name': '60d_10p_satiety_adjusted',\n",
    "        'outcome_type': 'target_wl',\n",
    "        'target_perc': 10,\n",
    "        'time_window': 60,\n",
    "        'predictors': ['satiety_yn'],\n",
    "        'covariates': ['age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi']\n",
    "    },\n",
    "    {\n",
    "        'name': '60d_10p_emotional_eating_adjusted',\n",
    "        'outcome_type': 'target_wl',\n",
    "        'target_perc': 10,\n",
    "        'time_window': 60,\n",
    "        'predictors': ['emotional_eating_yn'],\n",
    "        'covariates': ['age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi']\n",
    "    },\n",
    "    {\n",
    "        'name': '60d_10p_emotional_eating_value_adjusted',\n",
    "        'outcome_type': 'target_wl',\n",
    "        'target_perc': 10,\n",
    "        'time_window': 60,\n",
    "        'predictors': ['emotional_eating_value_likert'],\n",
    "        'covariates': ['age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi']\n",
    "    },\n",
    "    {\n",
    "        'name': '60d_10p_quantity_control_adjusted',\n",
    "        'outcome_type': 'target_wl',\n",
    "        'target_perc': 10,\n",
    "        'time_window': 60,\n",
    "        'predictors': ['quantity_control_likert'],\n",
    "        'covariates': ['age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi']\n",
    "    },\n",
    "    {\n",
    "        'name': '60d_10p_impulse_control_adjusted',\n",
    "        'outcome_type': 'target_wl',\n",
    "        'target_perc': 10,\n",
    "        'time_window': 60,\n",
    "        'predictors': ['impulse_control_likert'],\n",
    "        'covariates': ['age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi']\n",
    "    },\n",
    "\n",
    "    # 60 days, dropout, eating behavior, UNADJUSTED models\n",
    "    {\n",
    "        'name': '60d_dropout_hunger_unadjusted',\n",
    "        'outcome_type': 'dropout',\n",
    "        'target_perc': None,\n",
    "        'time_window': 60,\n",
    "        'predictors': ['hunger_yn'],\n",
    "        'covariates': []\n",
    "    },\n",
    "    {\n",
    "        'name': '60d_dropout_satiety_unadjusted',\n",
    "        'outcome_type': 'dropout',\n",
    "        'target_perc': None,\n",
    "        'time_window': 60,\n",
    "        'predictors': ['satiety_yn'],\n",
    "        'covariates': []\n",
    "    },\n",
    "    {\n",
    "        'name': '60d_dropout_emotional_eating_unadjusted',\n",
    "        'outcome_type': 'dropout',\n",
    "        'target_perc': None,\n",
    "        'time_window': 60,\n",
    "        'predictors': ['emotional_eating_yn'],\n",
    "        'covariates': []\n",
    "    },\n",
    "    {\n",
    "        'name': '60d_dropout_emotional_eating_value_unadjusted',\n",
    "        'outcome_type': 'dropout',\n",
    "        'target_perc': None,\n",
    "        'time_window': 60,\n",
    "        'predictors': ['emotional_eating_value_likert'],\n",
    "        'covariates': []\n",
    "    },\n",
    "    {\n",
    "        'name': '60d_dropout_quantity_control_unadjusted',\n",
    "        'outcome_type': 'dropout',\n",
    "        'target_perc': None,\n",
    "        'time_window': 60,\n",
    "        'predictors': ['quantity_control_likert'],\n",
    "        'covariates': []\n",
    "    },\n",
    "    {\n",
    "        'name': '60d_dropout_impulse_control_unadjusted',\n",
    "        'outcome_type': 'dropout',\n",
    "        'target_perc': None,\n",
    "        'time_window': 60,\n",
    "        'predictors': ['impulse_control_likert'],\n",
    "        'covariates': []\n",
    "    },\n",
    "\n",
    "\n",
    "    # Predict instant dropout using quantity control (unadjusted)\n",
    "    {\n",
    "        'name': 'instant_dropout_quant_ctrl_unadjusted',\n",
    "        'outcome_type': 'dropout',\n",
    "        'target_perc': None,\n",
    "        'time_window': 'instant', # Special case using 'nr_visits'\n",
    "        'predictors': ['quantity_control_likert'],\n",
    "        'covariates': []\n",
    "    },\n",
    "\n",
    "    # Instant, dropout, eating behavior, UNADJUSTED models\n",
    "    {\n",
    "        'name': 'instant_dropout_hunger_unadjusted',\n",
    "        'outcome_type': 'dropout',\n",
    "        'target_perc': None,\n",
    "        'time_window': 'instant',\n",
    "        'predictors': ['hunger_yn'],\n",
    "        'covariates': []\n",
    "    },\n",
    "    {\n",
    "        'name': 'instant_dropout_satiety_unadjusted',\n",
    "        'outcome_type': 'dropout',\n",
    "        'target_perc': None,\n",
    "        'time_window': 'instant',\n",
    "        'predictors': ['satiety_yn'],\n",
    "        'covariates': []\n",
    "    },\n",
    "    {\n",
    "        'name': 'instant_dropout_emotional_eating_unadjusted',\n",
    "        'outcome_type': 'dropout',\n",
    "        'target_perc': None,\n",
    "        'time_window': 'instant',\n",
    "        'predictors': ['emotional_eating_yn'],\n",
    "        'covariates': []\n",
    "    },\n",
    "    {\n",
    "        'name': 'instant_dropout_emotional_eating_value_unadjusted',\n",
    "        'outcome_type': 'dropout',\n",
    "        'target_perc': None,\n",
    "        'time_window': 'instant',\n",
    "        'predictors': ['emotional_eating_value_likert'],\n",
    "        'covariates': []\n",
    "    },\n",
    "    {\n",
    "        'name': 'instant_dropout_quantity_control_unadjusted',\n",
    "        'outcome_type': 'dropout',\n",
    "        'target_perc': None,\n",
    "        'time_window': 'instant',\n",
    "        'predictors': ['quantity_control_likert'],\n",
    "        'covariates': []\n",
    "    },\n",
    "    {\n",
    "        'name': 'instant_dropout_impulse_control_unadjusted',\n",
    "        'outcome_type': 'dropout',\n",
    "        'target_perc': None,\n",
    "        'time_window': 'instant',\n",
    "        'predictors': ['impulse_control_likert'],\n",
    "        'covariates': []\n",
    "    },\n",
    "\n",
    "\n",
    "    # # OTHER EXAMPLES AND SETUPS\n",
    " \n",
    "    # # X% achievement during total followup lengths\n",
    "    # # eg: Predict 10% WL achievement overall (total followup) using emotional eating value (adjusted)\n",
    "    # {\n",
    "    #     'name': 'total_10p_wl_emot_val_adjusted',\n",
    "    #     'outcome_type': 'target_wl',\n",
    "    #     'target_perc': 10,\n",
    "    #     'time_window': 'total', # Use 'total_wl_%' column\n",
    "    #     'predictors': ['emotional_eating_value_likert'],\n",
    "    #     'covariates': ['age', 'sex_f', 'baseline_bmi']\n",
    "    # },\n",
    "\n",
    "    # # NICER WAY OF LISTING SCENARIOS\n",
    "    # # --- Add your specific scenarios here ---\n",
    "    # # Example: All 6 variables for 60d/10% WL, adjusted\n",
    "    # {\n",
    "    #     'name': '60d_10p_wl_satiety_adjusted',\n",
    "    #     'outcome_type': 'target_wl', 'target_perc': 10, 'time_window': 60,\n",
    "    #     'predictors': ['satiety_yn'], 'covariates': ['age', 'sex_f', 'baseline_bmi']\n",
    "    # },\n",
    "    # {\n",
    "    #     'name': '60d_10p_wl_emot_yn_adjusted',\n",
    "    #     'outcome_type': 'target_wl', 'target_perc': 10, 'time_window': 60,\n",
    "    #     'predictors': ['emotional_eating_yn'], 'covariates': ['age', 'sex_f', 'baseline_bmi']\n",
    "    # },\n",
    "    # {\n",
    "    #     'name': '60d_10p_wl_emot_val_adjusted',\n",
    "    #     'outcome_type': 'target_wl', 'target_perc': 10, 'time_window': 60,\n",
    "    #     'predictors': ['emotional_eating_value_likert'], 'covariates': ['age', 'sex_f', 'baseline_bmi']\n",
    "    # },\n",
    "    # {\n",
    "    #     'name': '60d_10p_wl_quant_ctrl_adjusted',\n",
    "    #     'outcome_type': 'target_wl', 'target_perc': 10, 'time_window': 60,\n",
    "    #     'predictors': ['quantity_control_likert'], 'covariates': ['age', 'sex_f', 'baseline_bmi']\n",
    "    # },\n",
    "    # {\n",
    "    #     'name': '60d_10p_wl_impulse_ctrl_adjusted',\n",
    "    #     'outcome_type': 'target_wl', 'target_perc': 10, 'time_window': 60,\n",
    "    #     'predictors': ['impulse_control_likert'], 'covariates': ['age', 'sex_f', 'baseline_bmi']\n",
    "    # },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code of the first draft pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing script...\n",
      "Using pre-defined 'paper1_directory': C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\n",
      "========== Starting Logistic Regression Pipeline ==========\n",
      "\n",
      "--- Step 1: Loading Data ---\n",
      "Loading data from C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\survival_analysis.sqlite, table sa_input_table...\n",
      "Loaded 1664 rows and 56 columns.\n",
      "Data loaded successfully: 1664 rows, 56 columns.\n",
      "\n",
      "--- Step 2: Preprocessing Data ---\n",
      "Preprocessing data for Logistic models...\n",
      "Preprocessing complete.\n",
      "Data preprocessed successfully. Resulting shape: 1664 rows, 56 columns.\n",
      "\n",
      "--- Step 3: Processing Scenarios ---\n",
      "\n",
      "--- Processing Scenario: 60d_10p_hunger_unadjusted ---\n",
      "  Starting processing for scenario: 60d_10p_hunger_unadjusted\n",
      "    Attempting to define outcome variable '60d_10p_hunger_unadjusted_outcome'...\n",
      "      Defining outcome based on wl_60d_% >= 10%\n",
      "    Outcome '60d_10p_hunger_unadjusted_outcome' created successfully.\n",
      "    Preparing data subset for the model...\n",
      "      Selected columns: ['60d_10p_hunger_unadjusted_outcome', 'hunger_yn']\n",
      "      Removed 0 rows with missing values (out of 1664). Final dataset size for model: 1664 rows.\n",
      "    Running logistic regression model for scenario '60d_10p_hunger_unadjusted'...\n",
      "  Running Logistic model for scenario: 60d_10p_hunger_unadjusted\n",
      "    Outcome: 60d_10p_hunger_unadjusted_outcome\n",
      "    Predictors: ['hunger_yn']\n",
      "    Data shape: (1664, 2)\n",
      "    Model fitting complete.\n",
      "    Model for scenario '60d_10p_hunger_unadjusted' ran successfully.\n",
      "  --- Scenario 60d_10p_hunger_unadjusted completed successfully. Results collected. ---\n",
      "\n",
      "--- Processing Scenario: 60d_10p_satiety_unadjusted ---\n",
      "  Starting processing for scenario: 60d_10p_satiety_unadjusted\n",
      "    Attempting to define outcome variable '60d_10p_satiety_unadjusted_outcome'...\n",
      "      Defining outcome based on wl_60d_% >= 10%\n",
      "    Outcome '60d_10p_satiety_unadjusted_outcome' created successfully.\n",
      "    Preparing data subset for the model...\n",
      "      Selected columns: ['60d_10p_satiety_unadjusted_outcome', 'satiety_yn']\n",
      "      Removed 0 rows with missing values (out of 1664). Final dataset size for model: 1664 rows.\n",
      "    Running logistic regression model for scenario '60d_10p_satiety_unadjusted'...\n",
      "  Running Logistic model for scenario: 60d_10p_satiety_unadjusted\n",
      "    Outcome: 60d_10p_satiety_unadjusted_outcome\n",
      "    Predictors: ['satiety_yn']\n",
      "    Data shape: (1664, 2)\n",
      "    Model fitting complete.\n",
      "    Model for scenario '60d_10p_satiety_unadjusted' ran successfully.\n",
      "  --- Scenario 60d_10p_satiety_unadjusted completed successfully. Results collected. ---\n",
      "\n",
      "--- Processing Scenario: 60d_10p_emotional_eating_unadjusted ---\n",
      "  Starting processing for scenario: 60d_10p_emotional_eating_unadjusted\n",
      "    Attempting to define outcome variable '60d_10p_emotional_eating_unadjusted_outcome'...\n",
      "      Defining outcome based on wl_60d_% >= 10%\n",
      "    Outcome '60d_10p_emotional_eating_unadjusted_outcome' created successfully.\n",
      "    Preparing data subset for the model...\n",
      "      Selected columns: ['60d_10p_emotional_eating_unadjusted_outcome', 'emotional_eating_yn']\n",
      "      Removed 0 rows with missing values (out of 1664). Final dataset size for model: 1664 rows.\n",
      "    Running logistic regression model for scenario '60d_10p_emotional_eating_unadjusted'...\n",
      "  Running Logistic model for scenario: 60d_10p_emotional_eating_unadjusted\n",
      "    Outcome: 60d_10p_emotional_eating_unadjusted_outcome\n",
      "    Predictors: ['emotional_eating_yn']\n",
      "    Data shape: (1664, 2)\n",
      "    Model fitting complete.\n",
      "    Model for scenario '60d_10p_emotional_eating_unadjusted' ran successfully.\n",
      "  --- Scenario 60d_10p_emotional_eating_unadjusted completed successfully. Results collected. ---\n",
      "\n",
      "--- Processing Scenario: 60d_10p_emotional_eating_value_unadjusted ---\n",
      "  Starting processing for scenario: 60d_10p_emotional_eating_value_unadjusted\n",
      "    Attempting to define outcome variable '60d_10p_emotional_eating_value_unadjusted_outcome'...\n",
      "      Defining outcome based on wl_60d_% >= 10%\n",
      "    Outcome '60d_10p_emotional_eating_value_unadjusted_outcome' created successfully.\n",
      "    Preparing data subset for the model...\n",
      "      Selected columns: ['60d_10p_emotional_eating_value_unadjusted_outcome', 'emotional_eating_value_likert']\n",
      "      Removed 0 rows with missing values (out of 1664). Final dataset size for model: 1664 rows.\n",
      "    Running logistic regression model for scenario '60d_10p_emotional_eating_value_unadjusted'...\n",
      "  Running Logistic model for scenario: 60d_10p_emotional_eating_value_unadjusted\n",
      "    Outcome: 60d_10p_emotional_eating_value_unadjusted_outcome\n",
      "    Predictors: ['emotional_eating_value_likert']\n",
      "    Data shape: (1664, 2)\n",
      "    Model fitting complete.\n",
      "    Model for scenario '60d_10p_emotional_eating_value_unadjusted' ran successfully.\n",
      "  --- Scenario 60d_10p_emotional_eating_value_unadjusted completed successfully. Results collected. ---\n",
      "\n",
      "--- Processing Scenario: 60d_10p_quantity_control_unadjusted ---\n",
      "  Starting processing for scenario: 60d_10p_quantity_control_unadjusted\n",
      "    Attempting to define outcome variable '60d_10p_quantity_control_unadjusted_outcome'...\n",
      "      Defining outcome based on wl_60d_% >= 10%\n",
      "    Outcome '60d_10p_quantity_control_unadjusted_outcome' created successfully.\n",
      "    Preparing data subset for the model...\n",
      "      Selected columns: ['60d_10p_quantity_control_unadjusted_outcome', 'quantity_control_likert']\n",
      "      Removed 0 rows with missing values (out of 1664). Final dataset size for model: 1664 rows.\n",
      "    Running logistic regression model for scenario '60d_10p_quantity_control_unadjusted'...\n",
      "  Running Logistic model for scenario: 60d_10p_quantity_control_unadjusted\n",
      "    Outcome: 60d_10p_quantity_control_unadjusted_outcome\n",
      "    Predictors: ['quantity_control_likert']\n",
      "    Data shape: (1664, 2)\n",
      "    Model fitting complete.\n",
      "    Model for scenario '60d_10p_quantity_control_unadjusted' ran successfully.\n",
      "  --- Scenario 60d_10p_quantity_control_unadjusted completed successfully. Results collected. ---\n",
      "\n",
      "--- Processing Scenario: 60d_10p_impulse_control_unadjusted ---\n",
      "  Starting processing for scenario: 60d_10p_impulse_control_unadjusted\n",
      "    Attempting to define outcome variable '60d_10p_impulse_control_unadjusted_outcome'...\n",
      "      Defining outcome based on wl_60d_% >= 10%\n",
      "    Outcome '60d_10p_impulse_control_unadjusted_outcome' created successfully.\n",
      "    Preparing data subset for the model...\n",
      "      Selected columns: ['60d_10p_impulse_control_unadjusted_outcome', 'impulse_control_likert']\n",
      "      Removed 0 rows with missing values (out of 1664). Final dataset size for model: 1664 rows.\n",
      "    Running logistic regression model for scenario '60d_10p_impulse_control_unadjusted'...\n",
      "  Running Logistic model for scenario: 60d_10p_impulse_control_unadjusted\n",
      "    Outcome: 60d_10p_impulse_control_unadjusted_outcome\n",
      "    Predictors: ['impulse_control_likert']\n",
      "    Data shape: (1664, 2)\n",
      "    Model fitting complete.\n",
      "    Model for scenario '60d_10p_impulse_control_unadjusted' ran successfully.\n",
      "  --- Scenario 60d_10p_impulse_control_unadjusted completed successfully. Results collected. ---\n",
      "\n",
      "--- Processing Scenario: 60d_10p_hunger_adjusted ---\n",
      "  Starting processing for scenario: 60d_10p_hunger_adjusted\n",
      "    Attempting to define outcome variable '60d_10p_hunger_adjusted_outcome'...\n",
      "      Defining outcome based on wl_60d_% >= 10%\n",
      "    Outcome '60d_10p_hunger_adjusted_outcome' created successfully.\n",
      "    Preparing data subset for the model...\n",
      "      Selected columns: ['60d_10p_hunger_adjusted_outcome', 'hunger_yn', 'age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi']\n",
      "      Removed 0 rows with missing values (out of 1664). Final dataset size for model: 1664 rows.\n",
      "    Running logistic regression model for scenario '60d_10p_hunger_adjusted'...\n",
      "  Running Logistic model for scenario: 60d_10p_hunger_adjusted\n",
      "    Outcome: 60d_10p_hunger_adjusted_outcome\n",
      "    Predictors: ['hunger_yn', 'age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi']\n",
      "    Data shape: (1664, 6)\n",
      "    Model fitting complete.\n",
      "    Model for scenario '60d_10p_hunger_adjusted' ran successfully.\n",
      "  --- Scenario 60d_10p_hunger_adjusted completed successfully. Results collected. ---\n",
      "\n",
      "--- Processing Scenario: 60d_10p_satiety_adjusted ---\n",
      "  Starting processing for scenario: 60d_10p_satiety_adjusted\n",
      "    Attempting to define outcome variable '60d_10p_satiety_adjusted_outcome'...\n",
      "      Defining outcome based on wl_60d_% >= 10%\n",
      "    Outcome '60d_10p_satiety_adjusted_outcome' created successfully.\n",
      "    Preparing data subset for the model...\n",
      "      Selected columns: ['60d_10p_satiety_adjusted_outcome', 'satiety_yn', 'age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi']\n",
      "      Removed 0 rows with missing values (out of 1664). Final dataset size for model: 1664 rows.\n",
      "    Running logistic regression model for scenario '60d_10p_satiety_adjusted'...\n",
      "  Running Logistic model for scenario: 60d_10p_satiety_adjusted\n",
      "    Outcome: 60d_10p_satiety_adjusted_outcome\n",
      "    Predictors: ['satiety_yn', 'age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi']\n",
      "    Data shape: (1664, 6)\n",
      "    Model fitting complete.\n",
      "    Model for scenario '60d_10p_satiety_adjusted' ran successfully.\n",
      "  --- Scenario 60d_10p_satiety_adjusted completed successfully. Results collected. ---\n",
      "\n",
      "--- Processing Scenario: 60d_10p_emotional_eating_adjusted ---\n",
      "  Starting processing for scenario: 60d_10p_emotional_eating_adjusted\n",
      "    Attempting to define outcome variable '60d_10p_emotional_eating_adjusted_outcome'...\n",
      "      Defining outcome based on wl_60d_% >= 10%\n",
      "    Outcome '60d_10p_emotional_eating_adjusted_outcome' created successfully.\n",
      "    Preparing data subset for the model...\n",
      "      Selected columns: ['60d_10p_emotional_eating_adjusted_outcome', 'emotional_eating_yn', 'age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi']\n",
      "      Removed 0 rows with missing values (out of 1664). Final dataset size for model: 1664 rows.\n",
      "    Running logistic regression model for scenario '60d_10p_emotional_eating_adjusted'...\n",
      "  Running Logistic model for scenario: 60d_10p_emotional_eating_adjusted\n",
      "    Outcome: 60d_10p_emotional_eating_adjusted_outcome\n",
      "    Predictors: ['emotional_eating_yn', 'age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi']\n",
      "    Data shape: (1664, 6)\n",
      "    Model fitting complete.\n",
      "    Model for scenario '60d_10p_emotional_eating_adjusted' ran successfully.\n",
      "  --- Scenario 60d_10p_emotional_eating_adjusted completed successfully. Results collected. ---\n",
      "\n",
      "--- Processing Scenario: 60d_10p_emotional_eating_value_adjusted ---\n",
      "  Starting processing for scenario: 60d_10p_emotional_eating_value_adjusted\n",
      "    Attempting to define outcome variable '60d_10p_emotional_eating_value_adjusted_outcome'...\n",
      "      Defining outcome based on wl_60d_% >= 10%\n",
      "    Outcome '60d_10p_emotional_eating_value_adjusted_outcome' created successfully.\n",
      "    Preparing data subset for the model...\n",
      "      Selected columns: ['60d_10p_emotional_eating_value_adjusted_outcome', 'emotional_eating_value_likert', 'age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi']\n",
      "      Removed 0 rows with missing values (out of 1664). Final dataset size for model: 1664 rows.\n",
      "    Running logistic regression model for scenario '60d_10p_emotional_eating_value_adjusted'...\n",
      "  Running Logistic model for scenario: 60d_10p_emotional_eating_value_adjusted\n",
      "    Outcome: 60d_10p_emotional_eating_value_adjusted_outcome\n",
      "    Predictors: ['emotional_eating_value_likert', 'age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi']\n",
      "    Data shape: (1664, 6)\n",
      "    Model fitting complete.\n",
      "    Model for scenario '60d_10p_emotional_eating_value_adjusted' ran successfully.\n",
      "  --- Scenario 60d_10p_emotional_eating_value_adjusted completed successfully. Results collected. ---\n",
      "\n",
      "--- Processing Scenario: 60d_10p_quantity_control_adjusted ---\n",
      "  Starting processing for scenario: 60d_10p_quantity_control_adjusted\n",
      "    Attempting to define outcome variable '60d_10p_quantity_control_adjusted_outcome'...\n",
      "      Defining outcome based on wl_60d_% >= 10%\n",
      "    Outcome '60d_10p_quantity_control_adjusted_outcome' created successfully.\n",
      "    Preparing data subset for the model...\n",
      "      Selected columns: ['60d_10p_quantity_control_adjusted_outcome', 'quantity_control_likert', 'age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi']\n",
      "      Removed 0 rows with missing values (out of 1664). Final dataset size for model: 1664 rows.\n",
      "    Running logistic regression model for scenario '60d_10p_quantity_control_adjusted'...\n",
      "  Running Logistic model for scenario: 60d_10p_quantity_control_adjusted\n",
      "    Outcome: 60d_10p_quantity_control_adjusted_outcome\n",
      "    Predictors: ['quantity_control_likert', 'age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi']\n",
      "    Data shape: (1664, 6)\n",
      "    Model fitting complete.\n",
      "    Model for scenario '60d_10p_quantity_control_adjusted' ran successfully.\n",
      "  --- Scenario 60d_10p_quantity_control_adjusted completed successfully. Results collected. ---\n",
      "\n",
      "--- Processing Scenario: 60d_10p_impulse_control_adjusted ---\n",
      "  Starting processing for scenario: 60d_10p_impulse_control_adjusted\n",
      "    Attempting to define outcome variable '60d_10p_impulse_control_adjusted_outcome'...\n",
      "      Defining outcome based on wl_60d_% >= 10%\n",
      "    Outcome '60d_10p_impulse_control_adjusted_outcome' created successfully.\n",
      "    Preparing data subset for the model...\n",
      "      Selected columns: ['60d_10p_impulse_control_adjusted_outcome', 'impulse_control_likert', 'age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi']\n",
      "      Removed 0 rows with missing values (out of 1664). Final dataset size for model: 1664 rows.\n",
      "    Running logistic regression model for scenario '60d_10p_impulse_control_adjusted'...\n",
      "  Running Logistic model for scenario: 60d_10p_impulse_control_adjusted\n",
      "    Outcome: 60d_10p_impulse_control_adjusted_outcome\n",
      "    Predictors: ['impulse_control_likert', 'age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi']\n",
      "    Data shape: (1664, 6)\n",
      "    Model fitting complete.\n",
      "    Model for scenario '60d_10p_impulse_control_adjusted' ran successfully.\n",
      "  --- Scenario 60d_10p_impulse_control_adjusted completed successfully. Results collected. ---\n",
      "\n",
      "--- Processing Scenario: 60d_dropout_hunger_unadjusted ---\n",
      "  Starting processing for scenario: 60d_dropout_hunger_unadjusted\n",
      "    Attempting to define outcome variable '60d_dropout_hunger_unadjusted_outcome'...\n",
      "      Defining outcome based on column '60d_dropout' (NaN treated as 0)\n",
      "    Outcome '60d_dropout_hunger_unadjusted_outcome' created successfully.\n",
      "    Preparing data subset for the model...\n",
      "      Selected columns: ['60d_dropout_hunger_unadjusted_outcome', 'hunger_yn']\n",
      "      Removed 0 rows with missing values (out of 1664). Final dataset size for model: 1664 rows.\n",
      "    Running logistic regression model for scenario '60d_dropout_hunger_unadjusted'...\n",
      "  Running Logistic model for scenario: 60d_dropout_hunger_unadjusted\n",
      "    Outcome: 60d_dropout_hunger_unadjusted_outcome\n",
      "    Predictors: ['hunger_yn']\n",
      "    Data shape: (1664, 2)\n",
      "    Model fitting complete.\n",
      "    Model for scenario '60d_dropout_hunger_unadjusted' ran successfully.\n",
      "  --- Scenario 60d_dropout_hunger_unadjusted completed successfully. Results collected. ---\n",
      "\n",
      "--- Processing Scenario: 60d_dropout_satiety_unadjusted ---\n",
      "  Starting processing for scenario: 60d_dropout_satiety_unadjusted\n",
      "    Attempting to define outcome variable '60d_dropout_satiety_unadjusted_outcome'...\n",
      "      Defining outcome based on column '60d_dropout' (NaN treated as 0)\n",
      "    Outcome '60d_dropout_satiety_unadjusted_outcome' created successfully.\n",
      "    Preparing data subset for the model...\n",
      "      Selected columns: ['60d_dropout_satiety_unadjusted_outcome', 'satiety_yn']\n",
      "      Removed 0 rows with missing values (out of 1664). Final dataset size for model: 1664 rows.\n",
      "    Running logistic regression model for scenario '60d_dropout_satiety_unadjusted'...\n",
      "  Running Logistic model for scenario: 60d_dropout_satiety_unadjusted\n",
      "    Outcome: 60d_dropout_satiety_unadjusted_outcome\n",
      "    Predictors: ['satiety_yn']\n",
      "    Data shape: (1664, 2)\n",
      "    Model fitting complete.\n",
      "    Model for scenario '60d_dropout_satiety_unadjusted' ran successfully.\n",
      "  --- Scenario 60d_dropout_satiety_unadjusted completed successfully. Results collected. ---\n",
      "\n",
      "--- Processing Scenario: 60d_dropout_emotional_eating_unadjusted ---\n",
      "  Starting processing for scenario: 60d_dropout_emotional_eating_unadjusted\n",
      "    Attempting to define outcome variable '60d_dropout_emotional_eating_unadjusted_outcome'...\n",
      "      Defining outcome based on column '60d_dropout' (NaN treated as 0)\n",
      "    Outcome '60d_dropout_emotional_eating_unadjusted_outcome' created successfully.\n",
      "    Preparing data subset for the model...\n",
      "      Selected columns: ['60d_dropout_emotional_eating_unadjusted_outcome', 'emotional_eating_yn']\n",
      "      Removed 0 rows with missing values (out of 1664). Final dataset size for model: 1664 rows.\n",
      "    Running logistic regression model for scenario '60d_dropout_emotional_eating_unadjusted'...\n",
      "  Running Logistic model for scenario: 60d_dropout_emotional_eating_unadjusted\n",
      "    Outcome: 60d_dropout_emotional_eating_unadjusted_outcome\n",
      "    Predictors: ['emotional_eating_yn']\n",
      "    Data shape: (1664, 2)\n",
      "    Model fitting complete.\n",
      "    Model for scenario '60d_dropout_emotional_eating_unadjusted' ran successfully.\n",
      "  --- Scenario 60d_dropout_emotional_eating_unadjusted completed successfully. Results collected. ---\n",
      "\n",
      "--- Processing Scenario: 60d_dropout_emotional_eating_value_unadjusted ---\n",
      "  Starting processing for scenario: 60d_dropout_emotional_eating_value_unadjusted\n",
      "    Attempting to define outcome variable '60d_dropout_emotional_eating_value_unadjusted_outcome'...\n",
      "      Defining outcome based on column '60d_dropout' (NaN treated as 0)\n",
      "    Outcome '60d_dropout_emotional_eating_value_unadjusted_outcome' created successfully.\n",
      "    Preparing data subset for the model...\n",
      "      Selected columns: ['60d_dropout_emotional_eating_value_unadjusted_outcome', 'emotional_eating_value_likert']\n",
      "      Removed 0 rows with missing values (out of 1664). Final dataset size for model: 1664 rows.\n",
      "    Running logistic regression model for scenario '60d_dropout_emotional_eating_value_unadjusted'...\n",
      "  Running Logistic model for scenario: 60d_dropout_emotional_eating_value_unadjusted\n",
      "    Outcome: 60d_dropout_emotional_eating_value_unadjusted_outcome\n",
      "    Predictors: ['emotional_eating_value_likert']\n",
      "    Data shape: (1664, 2)\n",
      "    Model fitting complete.\n",
      "    Model for scenario '60d_dropout_emotional_eating_value_unadjusted' ran successfully.\n",
      "  --- Scenario 60d_dropout_emotional_eating_value_unadjusted completed successfully. Results collected. ---\n",
      "\n",
      "--- Processing Scenario: 60d_dropout_quantity_control_unadjusted ---\n",
      "  Starting processing for scenario: 60d_dropout_quantity_control_unadjusted\n",
      "    Attempting to define outcome variable '60d_dropout_quantity_control_unadjusted_outcome'...\n",
      "      Defining outcome based on column '60d_dropout' (NaN treated as 0)\n",
      "    Outcome '60d_dropout_quantity_control_unadjusted_outcome' created successfully.\n",
      "    Preparing data subset for the model...\n",
      "      Selected columns: ['60d_dropout_quantity_control_unadjusted_outcome', 'quantity_control_likert']\n",
      "      Removed 0 rows with missing values (out of 1664). Final dataset size for model: 1664 rows.\n",
      "    Running logistic regression model for scenario '60d_dropout_quantity_control_unadjusted'...\n",
      "  Running Logistic model for scenario: 60d_dropout_quantity_control_unadjusted\n",
      "    Outcome: 60d_dropout_quantity_control_unadjusted_outcome\n",
      "    Predictors: ['quantity_control_likert']\n",
      "    Data shape: (1664, 2)\n",
      "    Model fitting complete.\n",
      "    Model for scenario '60d_dropout_quantity_control_unadjusted' ran successfully.\n",
      "  --- Scenario 60d_dropout_quantity_control_unadjusted completed successfully. Results collected. ---\n",
      "\n",
      "--- Processing Scenario: 60d_dropout_impulse_control_unadjusted ---\n",
      "  Starting processing for scenario: 60d_dropout_impulse_control_unadjusted\n",
      "    Attempting to define outcome variable '60d_dropout_impulse_control_unadjusted_outcome'...\n",
      "      Defining outcome based on column '60d_dropout' (NaN treated as 0)\n",
      "    Outcome '60d_dropout_impulse_control_unadjusted_outcome' created successfully.\n",
      "    Preparing data subset for the model...\n",
      "      Selected columns: ['60d_dropout_impulse_control_unadjusted_outcome', 'impulse_control_likert']\n",
      "      Removed 0 rows with missing values (out of 1664). Final dataset size for model: 1664 rows.\n",
      "    Running logistic regression model for scenario '60d_dropout_impulse_control_unadjusted'...\n",
      "  Running Logistic model for scenario: 60d_dropout_impulse_control_unadjusted\n",
      "    Outcome: 60d_dropout_impulse_control_unadjusted_outcome\n",
      "    Predictors: ['impulse_control_likert']\n",
      "    Data shape: (1664, 2)\n",
      "    Model fitting complete.\n",
      "    Model for scenario '60d_dropout_impulse_control_unadjusted' ran successfully.\n",
      "  --- Scenario 60d_dropout_impulse_control_unadjusted completed successfully. Results collected. ---\n",
      "\n",
      "--- Processing Scenario: instant_dropout_quant_ctrl_unadjusted ---\n",
      "  Starting processing for scenario: instant_dropout_quant_ctrl_unadjusted\n",
      "    Attempting to define outcome variable 'instant_dropout_quant_ctrl_unadjusted_outcome'...\n",
      "      Defining outcome based on nr_visits == 1\n",
      "    Outcome 'instant_dropout_quant_ctrl_unadjusted_outcome' created successfully.\n",
      "    Preparing data subset for the model...\n",
      "      Selected columns: ['instant_dropout_quant_ctrl_unadjusted_outcome', 'quantity_control_likert']\n",
      "      Removed 0 rows with missing values (out of 1664). Final dataset size for model: 1664 rows.\n",
      "    Running logistic regression model for scenario 'instant_dropout_quant_ctrl_unadjusted'...\n",
      "  Running Logistic model for scenario: instant_dropout_quant_ctrl_unadjusted\n",
      "    Outcome: instant_dropout_quant_ctrl_unadjusted_outcome\n",
      "    Predictors: ['quantity_control_likert']\n",
      "    Data shape: (1664, 2)\n",
      "    Model fitting complete.\n",
      "    Model for scenario 'instant_dropout_quant_ctrl_unadjusted' ran successfully.\n",
      "  --- Scenario instant_dropout_quant_ctrl_unadjusted completed successfully. Results collected. ---\n",
      "\n",
      "--- Processing Scenario: instant_dropout_hunger_unadjusted ---\n",
      "  Starting processing for scenario: instant_dropout_hunger_unadjusted\n",
      "    Attempting to define outcome variable 'instant_dropout_hunger_unadjusted_outcome'...\n",
      "      Defining outcome based on nr_visits == 1\n",
      "    Outcome 'instant_dropout_hunger_unadjusted_outcome' created successfully.\n",
      "    Preparing data subset for the model...\n",
      "      Selected columns: ['instant_dropout_hunger_unadjusted_outcome', 'hunger_yn']\n",
      "      Removed 0 rows with missing values (out of 1664). Final dataset size for model: 1664 rows.\n",
      "    Running logistic regression model for scenario 'instant_dropout_hunger_unadjusted'...\n",
      "  Running Logistic model for scenario: instant_dropout_hunger_unadjusted\n",
      "    Outcome: instant_dropout_hunger_unadjusted_outcome\n",
      "    Predictors: ['hunger_yn']\n",
      "    Data shape: (1664, 2)\n",
      "    Model fitting complete.\n",
      "    Model for scenario 'instant_dropout_hunger_unadjusted' ran successfully.\n",
      "  --- Scenario instant_dropout_hunger_unadjusted completed successfully. Results collected. ---\n",
      "\n",
      "--- Processing Scenario: instant_dropout_satiety_unadjusted ---\n",
      "  Starting processing for scenario: instant_dropout_satiety_unadjusted\n",
      "    Attempting to define outcome variable 'instant_dropout_satiety_unadjusted_outcome'...\n",
      "      Defining outcome based on nr_visits == 1\n",
      "    Outcome 'instant_dropout_satiety_unadjusted_outcome' created successfully.\n",
      "    Preparing data subset for the model...\n",
      "      Selected columns: ['instant_dropout_satiety_unadjusted_outcome', 'satiety_yn']\n",
      "      Removed 0 rows with missing values (out of 1664). Final dataset size for model: 1664 rows.\n",
      "    Running logistic regression model for scenario 'instant_dropout_satiety_unadjusted'...\n",
      "  Running Logistic model for scenario: instant_dropout_satiety_unadjusted\n",
      "    Outcome: instant_dropout_satiety_unadjusted_outcome\n",
      "    Predictors: ['satiety_yn']\n",
      "    Data shape: (1664, 2)\n",
      "    Model fitting complete.\n",
      "    Model for scenario 'instant_dropout_satiety_unadjusted' ran successfully.\n",
      "  --- Scenario instant_dropout_satiety_unadjusted completed successfully. Results collected. ---\n",
      "\n",
      "--- Processing Scenario: instant_dropout_emotional_eating_unadjusted ---\n",
      "  Starting processing for scenario: instant_dropout_emotional_eating_unadjusted\n",
      "    Attempting to define outcome variable 'instant_dropout_emotional_eating_unadjusted_outcome'...\n",
      "      Defining outcome based on nr_visits == 1\n",
      "    Outcome 'instant_dropout_emotional_eating_unadjusted_outcome' created successfully.\n",
      "    Preparing data subset for the model...\n",
      "      Selected columns: ['instant_dropout_emotional_eating_unadjusted_outcome', 'emotional_eating_yn']\n",
      "      Removed 0 rows with missing values (out of 1664). Final dataset size for model: 1664 rows.\n",
      "    Running logistic regression model for scenario 'instant_dropout_emotional_eating_unadjusted'...\n",
      "  Running Logistic model for scenario: instant_dropout_emotional_eating_unadjusted\n",
      "    Outcome: instant_dropout_emotional_eating_unadjusted_outcome\n",
      "    Predictors: ['emotional_eating_yn']\n",
      "    Data shape: (1664, 2)\n",
      "    Model fitting complete.\n",
      "    Model for scenario 'instant_dropout_emotional_eating_unadjusted' ran successfully.\n",
      "  --- Scenario instant_dropout_emotional_eating_unadjusted completed successfully. Results collected. ---\n",
      "\n",
      "--- Processing Scenario: instant_dropout_emotional_eating_value_unadjusted ---\n",
      "  Starting processing for scenario: instant_dropout_emotional_eating_value_unadjusted\n",
      "    Attempting to define outcome variable 'instant_dropout_emotional_eating_value_unadjusted_outcome'...\n",
      "      Defining outcome based on nr_visits == 1\n",
      "    Outcome 'instant_dropout_emotional_eating_value_unadjusted_outcome' created successfully.\n",
      "    Preparing data subset for the model...\n",
      "      Selected columns: ['instant_dropout_emotional_eating_value_unadjusted_outcome', 'emotional_eating_value_likert']\n",
      "      Removed 0 rows with missing values (out of 1664). Final dataset size for model: 1664 rows.\n",
      "    Running logistic regression model for scenario 'instant_dropout_emotional_eating_value_unadjusted'...\n",
      "  Running Logistic model for scenario: instant_dropout_emotional_eating_value_unadjusted\n",
      "    Outcome: instant_dropout_emotional_eating_value_unadjusted_outcome\n",
      "    Predictors: ['emotional_eating_value_likert']\n",
      "    Data shape: (1664, 2)\n",
      "    Model fitting complete.\n",
      "    Model for scenario 'instant_dropout_emotional_eating_value_unadjusted' ran successfully.\n",
      "  --- Scenario instant_dropout_emotional_eating_value_unadjusted completed successfully. Results collected. ---\n",
      "\n",
      "--- Processing Scenario: instant_dropout_quantity_control_unadjusted ---\n",
      "  Starting processing for scenario: instant_dropout_quantity_control_unadjusted\n",
      "    Attempting to define outcome variable 'instant_dropout_quantity_control_unadjusted_outcome'...\n",
      "      Defining outcome based on nr_visits == 1\n",
      "    Outcome 'instant_dropout_quantity_control_unadjusted_outcome' created successfully.\n",
      "    Preparing data subset for the model...\n",
      "      Selected columns: ['instant_dropout_quantity_control_unadjusted_outcome', 'quantity_control_likert']\n",
      "      Removed 0 rows with missing values (out of 1664). Final dataset size for model: 1664 rows.\n",
      "    Running logistic regression model for scenario 'instant_dropout_quantity_control_unadjusted'...\n",
      "  Running Logistic model for scenario: instant_dropout_quantity_control_unadjusted\n",
      "    Outcome: instant_dropout_quantity_control_unadjusted_outcome\n",
      "    Predictors: ['quantity_control_likert']\n",
      "    Data shape: (1664, 2)\n",
      "    Model fitting complete.\n",
      "    Model for scenario 'instant_dropout_quantity_control_unadjusted' ran successfully.\n",
      "  --- Scenario instant_dropout_quantity_control_unadjusted completed successfully. Results collected. ---\n",
      "\n",
      "--- Processing Scenario: instant_dropout_impulse_control_unadjusted ---\n",
      "  Starting processing for scenario: instant_dropout_impulse_control_unadjusted\n",
      "    Attempting to define outcome variable 'instant_dropout_impulse_control_unadjusted_outcome'...\n",
      "      Defining outcome based on nr_visits == 1\n",
      "    Outcome 'instant_dropout_impulse_control_unadjusted_outcome' created successfully.\n",
      "    Preparing data subset for the model...\n",
      "      Selected columns: ['instant_dropout_impulse_control_unadjusted_outcome', 'impulse_control_likert']\n",
      "      Removed 0 rows with missing values (out of 1664). Final dataset size for model: 1664 rows.\n",
      "    Running logistic regression model for scenario 'instant_dropout_impulse_control_unadjusted'...\n",
      "  Running Logistic model for scenario: instant_dropout_impulse_control_unadjusted\n",
      "    Outcome: instant_dropout_impulse_control_unadjusted_outcome\n",
      "    Predictors: ['impulse_control_likert']\n",
      "    Data shape: (1664, 2)\n",
      "    Model fitting complete.\n",
      "    Model for scenario 'instant_dropout_impulse_control_unadjusted' ran successfully.\n",
      "  --- Scenario instant_dropout_impulse_control_unadjusted completed successfully. Results collected. ---\n",
      "\n",
      "--- Step 4: Aggregating and Saving Results ---\n",
      "Aggregated results from 25 successful scenarios.\n",
      "Attempting to connect to database: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\survival_analysis.sqlite\n",
      "Saving aggregated results to table: log_regression_results\n",
      "Results saved successfully to the database.\n",
      "Database connection closed.\n",
      "\n",
      "========== Logistic Regression Pipeline Finished ==========\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# Logistic Regression Pipeline Module\n",
    "####\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "\n",
    "# Suppress ConvergenceWarning from statsmodels\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "\n",
    "# Ensure paper1_directory is defined in a previous cell\n",
    "if 'paper1_directory' not in locals() and 'paper1_directory' not in globals():\n",
    "    # Try to get it from the environment if running as a script, or raise error\n",
    "    paper1_directory = os.getenv('PAPER1_DIRECTORY')\n",
    "    if paper1_directory is None:\n",
    "        raise NameError(\"'paper1_directory' variable not defined. Please ensure it is defined or set as an environment variable 'PAPER1_DIRECTORY'.\")\n",
    "    else:\n",
    "        print(f\"Using paper1_directory from environment variable: {paper1_directory}\")\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "\n",
    "# Adjust these paths as needed\n",
    "DB_PATH = os.path.join(paper1_directory, 'survival_analysis.sqlite') # Path to the SQLite database\n",
    "INPUT_TABLE_NAME = \"sa_input_table\" # Your wide input table\n",
    "OUTPUT_TABLE_NAME = \"log_regression_results\" # Table to store summary results\n",
    "\n",
    "# The analysis scenarios are defined in the previous cell\n",
    "\n",
    "# --- HELPERS ---\n",
    "\n",
    "def load_data(db_path, table_name):\n",
    "    \"\"\"Loads data from the specified SQLite database table.\"\"\"\n",
    "    print(f\"Loading data from {db_path}, table {table_name}...\")\n",
    "    if not os.path.exists(db_path):\n",
    "        raise FileNotFoundError(f\"Database file not found at {db_path}\")\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n",
    "        print(f\"Loaded {len(df)} rows and {len(df.columns)} columns.\")\n",
    "        if df.empty:\n",
    "             print(f\"Warning: Loaded DataFrame from {table_name} is empty.\")\n",
    "        return df\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"SQLite error loading data: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred loading data: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "def preprocess_data(df, scenarios):\n",
    "    \"\"\"Handles basic data preparation for logistic models.\"\"\"\n",
    "    print(\"Preprocessing data for Logistic models...\")\n",
    "    if df.empty:\n",
    "         print(\"  Input DataFrame is empty. Skipping preprocessing.\")\n",
    "         return df\n",
    "\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # Identify all unique predictor/covariate columns needed across scenarios\n",
    "    all_model_cols = set()\n",
    "    bool_predictors = set() # Specifically track yes/no predictors\n",
    "    for scenario in scenarios:\n",
    "        all_model_cols.update(scenario['predictors'])\n",
    "        all_model_cols.update(scenario['covariates'])\n",
    "        # Identify boolean predictors based on common naming or explicit list\n",
    "        for pred in scenario['predictors']:\n",
    "            if pred.endswith('_yn'): # Assuming '_yn' suffix for boolean predictors\n",
    "                 bool_predictors.add(pred)\n",
    "\n",
    "    # Ensure boolean predictors are 0/1 integers\n",
    "    for col in bool_predictors:\n",
    "        if col in df_processed.columns:\n",
    "            if not pd.api.types.is_numeric_dtype(df_processed[col]) or not df_processed[col].dropna().isin([0, 1]).all():\n",
    "                print(f\"  Attempting to convert boolean predictor '{col}' to 0/1 integer.\")\n",
    "                # Handle potential 'yes'/'no' strings or True/False bools\n",
    "                if df_processed[col].dtype == 'object':\n",
    "                    df_processed[col] = df_processed[col].str.lower().map({'yes': 1, 'no': 0, 'y': 1, 'n': 0})\n",
    "                elif df_processed[col].dtype == 'bool':\n",
    "                     df_processed[col] = df_processed[col].astype(int)\n",
    "\n",
    "                # Convert to numeric, coercing errors, then fill NA with a placeholder if needed (e.g., 0 or median)\n",
    "                # For simplicity here, we'll rely on dropna later, but imputation could be added.\n",
    "                df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')\n",
    "                if not df_processed[col].dropna().isin([0, 1]).all():\n",
    "                     print(f\"    Warning: Column '{col}' could not be reliably converted to 0/1.\")\n",
    "            # Convert float 0.0/1.0 to integer\n",
    "            elif pd.api.types.is_float_dtype(df_processed[col]) and df_processed[col].dropna().isin([0.0, 1.0]).all():\n",
    "                 df_processed[col] = df_processed[col].astype('Int64') # Use nullable integer\n",
    "        else:\n",
    "             print(f\"  Warning: Boolean predictor column '{col}' specified in scenarios but not found.\")\n",
    "\n",
    "    # Ensure other predictors/covariates are numeric\n",
    "    numeric_cols_to_check = all_model_cols - bool_predictors\n",
    "    # Add specific outcome-related columns that need to be numeric\n",
    "    numeric_cols_to_check.add('nr_visits')\n",
    "    # Add potential weight loss columns dynamically based on scenarios\n",
    "    for scenario in scenarios:\n",
    "        if scenario['outcome_type'] == 'target_wl':\n",
    "            tw = scenario['time_window']\n",
    "            wl_col = f\"wl_{tw}d_%\" if isinstance(tw, int) else \"total_wl_%\"\n",
    "            numeric_cols_to_check.add(wl_col)\n",
    "        elif scenario['outcome_type'] == 'dropout' and isinstance(scenario['time_window'], int):\n",
    "             dropout_col = f\"{scenario['time_window']}d_dropout\"\n",
    "             numeric_cols_to_check.add(dropout_col) # Ensure dropout flags are numeric\n",
    "\n",
    "    for col in numeric_cols_to_check:\n",
    "        if col in df_processed.columns:\n",
    "            if not pd.api.types.is_numeric_dtype(df_processed[col]):\n",
    "                print(f\"  Attempting to convert column '{col}' to numeric.\")\n",
    "                original_nan_count = df_processed[col].isnull().sum()\n",
    "                df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')\n",
    "                if df_processed[col].isnull().sum() > original_nan_count:\n",
    "                    print(f\"    Warning: Conversion of column '{col}' introduced NaNs.\")\n",
    "        # Don't warn if column doesn't exist, it might only be needed for specific scenarios checked later\n",
    "\n",
    "    print(\"Preprocessing complete.\")\n",
    "    return df_processed\n",
    "\n",
    "\n",
    "# --- LOGISTIC REGRESSION MODULE ---\n",
    "\n",
    "def run_logistic_model(df_subset, outcome_col, predictor_cols, scenario_name):\n",
    "    \"\"\"\n",
    "    Fits a Logistic Regression model using statsmodels and extracts results.\n",
    "\n",
    "    Args:\n",
    "        df_subset (pd.DataFrame): DataFrame containing only necessary columns and no NaNs for the model.\n",
    "        outcome_col (str): Name of the binary outcome column (0/1).\n",
    "        predictor_cols (list): List of predictor and covariate column names.\n",
    "        scenario_name (str): Name of the scenario for labeling outputs.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing model summary results (OR, CI, p-value per variable),\n",
    "                      or None if the model fails or data is insufficient.\n",
    "    \"\"\"\n",
    "    print(f\"  Running Logistic model for scenario: {scenario_name}\")\n",
    "    print(f\"    Outcome: {outcome_col}\")\n",
    "    print(f\"    Predictors: {predictor_cols}\")\n",
    "    print(f\"    Data shape: {df_subset.shape}\")\n",
    "\n",
    "    # Check for sufficient data\n",
    "    min_obs_per_predictor = 10\n",
    "    required_obs = min_obs_per_predictor * len(predictor_cols) if predictor_cols else 20\n",
    "    if df_subset.shape[0] < max(20, required_obs):\n",
    "        print(f\"    Warning: Insufficient data ({df_subset.shape[0]} obs) for {len(predictor_cols)} predictors. Need at least {max(20, required_obs)}. Skipping model.\")\n",
    "        return None\n",
    "    if not predictor_cols:\n",
    "        print(\"    Warning: No predictors specified. Skipping model.\")\n",
    "        return None\n",
    "    # Check if outcome column has variance\n",
    "    if outcome_col not in df_subset or df_subset[outcome_col].nunique() < 2:\n",
    "        print(f\"    Warning: Outcome column '{outcome_col}' has no variation (all 0s or all 1s) or is missing. Skipping model.\")\n",
    "        return None\n",
    "\n",
    "    results_list = []\n",
    "\n",
    "    try:\n",
    "        # Prepare data for statsmodels (add constant for intercept)\n",
    "        X = df_subset[predictor_cols]\n",
    "        y = df_subset[outcome_col]\n",
    "        X = sm.add_constant(X, has_constant='add') # Add intercept\n",
    "\n",
    "        # Fit the model\n",
    "        logit_model = sm.Logit(y, X)\n",
    "        result = logit_model.fit(disp=0) # disp=0 suppresses optimization output\n",
    "\n",
    "        # Extract Results\n",
    "        params = result.params\n",
    "        conf = result.conf_int()\n",
    "        conf['Odds Ratio'] = params\n",
    "        conf.columns = ['CI Lower 95%', 'CI Upper 95%', 'Coefficient']\n",
    "        conf = np.exp(conf) # Exponentiate to get Odds Ratios and their CIs\n",
    "\n",
    "        p_values = result.pvalues\n",
    "        # Combine results into a DataFrame\n",
    "        summary_df = pd.DataFrame({\n",
    "            'Variable': params.index,\n",
    "            'Coefficient': params.values,\n",
    "            'Odds Ratio': conf['Coefficient'],\n",
    "            'OR CI Lower 95%': conf['CI Lower 95%'],\n",
    "            'OR CI Upper 95%': conf['CI Upper 95%'],\n",
    "            'P-Value': p_values.values,\n",
    "            'Std_Err': result.bse.values # Standard Error\n",
    "        })\n",
    "\n",
    "        # Add model-level stats\n",
    "        model_stats = {\n",
    "            'scenario_name': scenario_name,\n",
    "            'outcome_variable': outcome_col,\n",
    "            'n_observations': int(result.nobs),\n",
    "            'log_likelihood': result.llf,\n",
    "            'pseudo_r_squared': result.prsquared,\n",
    "            'aic': result.aic,\n",
    "            'bic': result.bic,\n",
    "            'converged': result.mle_retvals['converged']\n",
    "        }\n",
    "\n",
    "        # Add model stats to each row of the summary_df for easy aggregation\n",
    "        for col, val in model_stats.items():\n",
    "            summary_df[col] = val\n",
    "\n",
    "        print(\"    Model fitting complete.\")\n",
    "        return summary_df\n",
    "\n",
    "    except np.linalg.LinAlgError as e_linalg:\n",
    "         print(f\"    ERROR: Linear algebra error (likely perfect multicollinearity) for scenario {scenario_name}. Skipping. Error: {e_linalg}\")\n",
    "         return None\n",
    "    except ValueError as e_val:\n",
    "         print(f\"    ERROR: Value error during model fitting (check data types/values) for scenario {scenario_name}. Skipping. Error: {e_val}\")\n",
    "         return None\n",
    "    except Exception as e_fit:\n",
    "        print(f\"    ERROR: Failed to fit Logistic model for scenario {scenario_name}. Error: {e_fit}\")\n",
    "        # import traceback # Uncomment for detailed traceback\n",
    "        # print(traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "# --- Scenario Processing Function ---\n",
    "\n",
    "def process_scenario(scenario, df_analysis):\n",
    "    \"\"\"\n",
    "    Processes a single logistic regression scenario.\n",
    "\n",
    "    Args:\n",
    "        scenario (dict): Dictionary containing scenario configuration.\n",
    "        df_analysis (pd.DataFrame): The preprocessed DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame or None: DataFrame with model results for the scenario,\n",
    "                              or None if processing fails or is skipped.\n",
    "    \"\"\"\n",
    "    scenario_name = scenario['name']\n",
    "    print(f\"  Starting processing for scenario: {scenario_name}\")\n",
    "\n",
    "    outcome_type = scenario['outcome_type']\n",
    "    target_perc = scenario.get('target_perc') # Use .get for safer access\n",
    "    time_window = scenario['time_window']\n",
    "    predictors = scenario['predictors']\n",
    "    covariates = scenario['covariates']\n",
    "    all_predictors = predictors + covariates\n",
    "\n",
    "    # --- Define Outcome Variable Dynamically ---\n",
    "    outcome_col_name = f\"{scenario_name}_outcome\"\n",
    "    # Work on a copy to avoid modifying df_analysis across scenarios\n",
    "    temp_df = df_analysis.copy()\n",
    "    outcome_defined = False\n",
    "\n",
    "    try:\n",
    "        print(f\"    Attempting to define outcome variable '{outcome_col_name}'...\")\n",
    "        if outcome_type == 'target_wl':\n",
    "            if target_perc is None:\n",
    "                print(f\"    ERROR: 'target_perc' must be specified for outcome_type 'target_wl' in scenario '{scenario_name}'. Skipping scenario.\")\n",
    "                return None\n",
    "\n",
    "            # Determine the source weight loss column\n",
    "            if time_window == 'total':\n",
    "                source_wl_col = 'total_wl_%'\n",
    "            elif isinstance(time_window, int):\n",
    "                source_wl_col = f'wl_{time_window}d_%'\n",
    "            else:\n",
    "                print(f\"    ERROR: Invalid 'time_window' ({time_window}) for outcome_type 'target_wl' in scenario '{scenario_name}'. Skipping scenario.\")\n",
    "                return None\n",
    "\n",
    "            if source_wl_col not in temp_df.columns:\n",
    "                print(f\"    ERROR: Source weight loss column '{source_wl_col}' not found in data for scenario '{scenario_name}'. Skipping scenario.\")\n",
    "                return None\n",
    "\n",
    "            # Create binary outcome\n",
    "            print(f\"      Defining outcome based on {source_wl_col} >= {target_perc}%\")\n",
    "            temp_df[source_wl_col] = pd.to_numeric(temp_df[source_wl_col], errors='coerce')\n",
    "            if temp_df[source_wl_col].isnull().all():\n",
    "                 print(f\"      WARNING: Source column '{source_wl_col}' contains only NaN values after numeric conversion.\")\n",
    "            temp_df[outcome_col_name] = (temp_df[source_wl_col] >= target_perc).astype(float) # Use float first\n",
    "            temp_df[outcome_col_name] = temp_df[outcome_col_name].fillna(0).astype(int) # Fill NaN outcome with 0\n",
    "            print(f\"    Outcome '{outcome_col_name}' created successfully.\")\n",
    "            outcome_defined = True\n",
    "\n",
    "        elif outcome_type == 'dropout':\n",
    "            if time_window == 'instant':\n",
    "                source_dropout_col = 'nr_visits'\n",
    "                if source_dropout_col not in temp_df.columns:\n",
    "                     print(f\"    ERROR: Column '{source_dropout_col}' needed for instant dropout not found for scenario '{scenario_name}'. Skipping scenario.\")\n",
    "                     return None\n",
    "                print(f\"      Defining outcome based on {source_dropout_col} == 1\")\n",
    "                temp_df[source_dropout_col] = pd.to_numeric(temp_df[source_dropout_col], errors='coerce')\n",
    "                if temp_df[source_dropout_col].isnull().all():\n",
    "                     print(f\"      WARNING: Source column '{source_dropout_col}' contains only NaN values after numeric conversion.\")\n",
    "                # Handle potential NaNs in nr_visits (treat as non-dropout)\n",
    "                temp_df[outcome_col_name] = (temp_df[source_dropout_col] == 1).astype(float).fillna(0).astype(int)\n",
    "                print(f\"    Outcome '{outcome_col_name}' created successfully.\")\n",
    "                outcome_defined = True\n",
    "            elif isinstance(time_window, int):\n",
    "                source_dropout_col = f'{time_window}d_dropout'\n",
    "                if source_dropout_col not in temp_df.columns:\n",
    "                    print(f\"    ERROR: Dropout column '{source_dropout_col}' not found for scenario '{scenario_name}'. Skipping scenario.\")\n",
    "                    return None\n",
    "                print(f\"      Defining outcome based on column '{source_dropout_col}' (NaN treated as 0)\")\n",
    "                temp_df[source_dropout_col] = pd.to_numeric(temp_df[source_dropout_col], errors='coerce')\n",
    "                if temp_df[source_dropout_col].isnull().all():\n",
    "                     print(f\"      WARNING: Source column '{source_dropout_col}' contains only NaN values after numeric conversion.\")\n",
    "                temp_df[outcome_col_name] = temp_df[source_dropout_col].fillna(0).astype(int)\n",
    "                # Verify it's 0/1\n",
    "                if not temp_df[outcome_col_name].isin([0, 1]).all():\n",
    "                     print(f\"      WARNING: Dropout column '{source_dropout_col}' contains values other than 0, 1 after cleaning. Check data. Proceeding cautiously.\")\n",
    "                print(f\"    Outcome '{outcome_col_name}' created successfully.\")\n",
    "                outcome_defined = True\n",
    "            else:\n",
    "                print(f\"    ERROR: Invalid 'time_window' ({time_window}) for outcome_type 'dropout' in scenario '{scenario_name}'. Use 'instant' or integer days. Skipping scenario.\")\n",
    "                return None\n",
    "        else:\n",
    "            print(f\"    ERROR: Invalid outcome_type '{outcome_type}' in scenario '{scenario_name}'. Skipping scenario.\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e_outcome:\n",
    "         print(f\"    ERROR: Unexpected error creating outcome variable for scenario {scenario_name}: {e_outcome}. Skipping scenario.\")\n",
    "         print(traceback.format_exc()) # Print traceback for debugging\n",
    "         return None\n",
    "\n",
    "    if not outcome_defined:\n",
    "         print(f\"    Outcome variable could not be defined for scenario {scenario_name}. Skipping scenario.\")\n",
    "         return None # Should not happen if logic above is correct, but safety check\n",
    "\n",
    "    # --- Prepare Data for Model ---\n",
    "    print(\"    Preparing data subset for the model...\")\n",
    "    # Check if all necessary columns exist\n",
    "    required_cols = [outcome_col_name] + all_predictors\n",
    "    missing_cols = [col for col in required_cols if col not in temp_df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"    ERROR: Missing required columns for scenario '{scenario_name}': {missing_cols}. Skipping scenario.\")\n",
    "        return None\n",
    "\n",
    "    # Select relevant columns and drop rows with missing values in predictors/covariates or outcome\n",
    "    initial_rows = len(temp_df)\n",
    "    df_model_subset = temp_df[required_cols].dropna()\n",
    "    final_rows = len(df_model_subset)\n",
    "    print(f\"      Selected columns: {required_cols}\")\n",
    "    print(f\"      Removed {initial_rows - final_rows} rows with missing values (out of {initial_rows}). Final dataset size for model: {final_rows} rows.\")\n",
    "\n",
    "    if final_rows == 0:\n",
    "        print(f\"    ERROR: No valid data remaining after handling missing values for scenario '{scenario_name}'. Skipping scenario.\")\n",
    "        return None\n",
    "    if df_model_subset[outcome_col_name].nunique() < 2:\n",
    "        print(f\"    ERROR: Outcome variable '{outcome_col_name}' has only one unique value ({df_model_subset[outcome_col_name].unique()}) after filtering. Cannot run logistic regression. Skipping scenario.\")\n",
    "        return None\n",
    "\n",
    "    # --- Run the Logistic Model ---\n",
    "    print(f\"    Running logistic regression model for scenario '{scenario_name}'...\")\n",
    "    model_output_df = None\n",
    "    try:\n",
    "        model_output_df = run_logistic_model(\n",
    "            df_model_subset,\n",
    "            outcome_col_name,\n",
    "            all_predictors,\n",
    "            scenario_name\n",
    "        )\n",
    "    except Exception as e_model:\n",
    "        print(f\"    ERROR: An exception occurred during model execution for scenario '{scenario_name}': {e_model}\")\n",
    "        print(traceback.format_exc()) # Print traceback for debugging\n",
    "        return None # Indicate failure\n",
    "\n",
    "    if model_output_df is not None and not model_output_df.empty:\n",
    "        print(f\"    Model for scenario '{scenario_name}' ran successfully.\")\n",
    "        return model_output_df\n",
    "    else:\n",
    "         print(f\"    Model for scenario '{scenario_name}' did not produce valid results (returned None or empty DataFrame).\")\n",
    "         return None\n",
    "\n",
    "# --- ORCHESTRATION ---\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate the Logistic Regression pipeline.\"\"\"\n",
    "    print(\"========== Starting Logistic Regression Pipeline ==========\")\n",
    "    all_results_summary = [] # To store summary stats from each model\n",
    "\n",
    "    try:\n",
    "        # 1. Load Data\n",
    "        print(\"\\n--- Step 1: Loading Data ---\")\n",
    "        df_raw = load_data(DB_PATH, INPUT_TABLE_NAME)\n",
    "        if df_raw is None: # Assume load_data returns None on error\n",
    "             print(\"ERROR: Failed to load data. Stopping pipeline.\")\n",
    "             return # Exit main function\n",
    "        if df_raw.empty:\n",
    "             print(\"ERROR: Input data table is empty. Stopping pipeline.\")\n",
    "             return # Exit main function\n",
    "        print(f\"Data loaded successfully: {df_raw.shape[0]} rows, {df_raw.shape[1]} columns.\")\n",
    "\n",
    "        # 2. Preprocess Data\n",
    "        print(\"\\n--- Step 2: Preprocessing Data ---\")\n",
    "        df_analysis = preprocess_data(df_raw, LOGREG_SCENARIOS)\n",
    "        if df_analysis is None: # Assume preprocess_data returns None on error\n",
    "            print(\"ERROR: Failed during data preprocessing. Stopping pipeline.\")\n",
    "            return # Exit main function\n",
    "        if df_analysis.empty:\n",
    "             print(\"ERROR: Data preprocessing resulted in an empty DataFrame. Stopping pipeline.\")\n",
    "             return # Exit main function\n",
    "        print(f\"Data preprocessed successfully. Resulting shape: {df_analysis.shape[0]} rows, {df_analysis.shape[1]} columns.\")\n",
    "\n",
    "        # 3. Loop through scenarios and run models\n",
    "        print(\"\\n--- Step 3: Processing Scenarios ---\")\n",
    "        for scenario in LOGREG_SCENARIOS:\n",
    "            print(f\"\\n--- Processing Scenario: {scenario.get('name', 'Unnamed Scenario')} ---\") # Use .get for safety\n",
    "\n",
    "            # Call the dedicated function to process this scenario\n",
    "            scenario_result = process_scenario(scenario, df_analysis)\n",
    "\n",
    "            # Collect results if model ran successfully\n",
    "            if scenario_result is not None and not scenario_result.empty:\n",
    "                all_results_summary.append(scenario_result)\n",
    "                print(f\"  --- Scenario {scenario.get('name', 'Unnamed Scenario')} completed successfully. Results collected. ---\")\n",
    "            else:\n",
    "                 print(f\"  --- Scenario {scenario.get('name', 'Unnamed Scenario')} did not produce valid results or was skipped. ---\")\n",
    "\n",
    "        # 4. Save Aggregated Results\n",
    "        print(\"\\n--- Step 4: Aggregating and Saving Results ---\")\n",
    "        if all_results_summary:\n",
    "            # Concatenate all results DataFrames\n",
    "            results_df = pd.concat(all_results_summary, ignore_index=True)\n",
    "            print(f\"Aggregated results from {len(all_results_summary)} successful scenarios.\")\n",
    "\n",
    "            conn_out = None\n",
    "            try:\n",
    "                print(f\"Attempting to connect to database: {DB_PATH}\")\n",
    "                conn_out = sqlite3.connect(DB_PATH)\n",
    "                print(f\"Saving aggregated results to table: {OUTPUT_TABLE_NAME}\")\n",
    "                results_df.to_sql(OUTPUT_TABLE_NAME, conn_out, if_exists='replace', index=False)\n",
    "                conn_out.commit()\n",
    "                print(\"Results saved successfully to the database.\")\n",
    "            except sqlite3.Error as e_sql:\n",
    "                print(f\"ERROR: SQLite error occurred while saving results: {e_sql}\")\n",
    "                if conn_out:\n",
    "                    try:\n",
    "                        conn_out.rollback()\n",
    "                        print(\"Database transaction rolled back.\")\n",
    "                    except sqlite3.Error as e_rb:\n",
    "                        print(f\"ERROR: Could not rollback transaction: {e_rb}\")\n",
    "            except Exception as e_save:\n",
    "                print(f\"ERROR: An unexpected error occurred saving results: {e_save}\")\n",
    "                if conn_out:\n",
    "                    try:\n",
    "                        conn_out.rollback()\n",
    "                        print(\"Database transaction rolled back.\")\n",
    "                    except sqlite3.Error as e_rb:\n",
    "                         print(f\"ERROR: Could not rollback transaction: {e_rb}\")\n",
    "                print(traceback.format_exc())\n",
    "            finally:\n",
    "                if conn_out:\n",
    "                    conn_out.close()\n",
    "                    print(\"Database connection closed.\")\n",
    "        else:\n",
    "            print(\"No valid model results were generated across all scenarios. Nothing to save.\")\n",
    "\n",
    "    except FileNotFoundError as e_fnf:\n",
    "        print(f\"ERROR: File not found during pipeline execution: {e_fnf}\")\n",
    "    except sqlite3.Error as e_sql_main:\n",
    "        print(f\"ERROR: Database error during setup or initial loading: {e_sql_main}\")\n",
    "    except NameError as e_name:\n",
    "         print(f\"ERROR: A required variable or function might be missing or misspelled: {e_name}\")\n",
    "    except KeyError as e_key:\n",
    "         print(f\"ERROR: Missing expected key in scenario configuration or data: {e_key}\")\n",
    "    except Exception as e_main:\n",
    "        print(f\"ERROR: An unexpected error occurred in the main pipeline orchestration: {e_main}\")\n",
    "        print(\"--- Traceback ---\")\n",
    "        print(traceback.format_exc())\n",
    "        print(\"--- End Traceback ---\")\n",
    "    finally:\n",
    "        print(\"\\n========== Logistic Regression Pipeline Finished ==========\")\n",
    "\n",
    "\n",
    "# --- EXECUTION ---\n",
    "# This block should be outside the main() function definition\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "     print(\"Executing script...\")\n",
    "     # Check if running as script vs notebook cell where paper1_directory might be predefined\n",
    "     if 'paper1_directory' not in locals() and 'paper1_directory' not in globals():\n",
    "          print(\"'paper1_directory' not found in local/global scope. Checking environment variable...\")\n",
    "          # Attempt to retrieve from environment variable\n",
    "          paper1_directory = os.getenv('PAPER1_DIRECTORY')\n",
    "          if paper1_directory is None:\n",
    "               print(\"ERROR: 'paper1_directory' variable not defined and 'PAPER1_DIRECTORY' environment variable not set. Cannot run main().\")\n",
    "               # Depending on your setup, you might want to exit here:\n",
    "               # import sys\n",
    "               # sys.exit(1)\n",
    "          else:\n",
    "               print(f\"Using 'paper1_directory' from environment variable: {paper1_directory}\")\n",
    "               # Potentially set DB_PATH or other paths based on paper1_directory here if needed\n",
    "               # Example: DB_PATH = os.path.join(paper1_directory, 'database', 'my_db.sqlite')\n",
    "               main() # Call main only if directory is found\n",
    "     else:\n",
    "          # 'paper1_directory' is already defined (e.g., in a notebook cell)\n",
    "          print(f\"Using pre-defined 'paper1_directory': {paper1_directory}\")\n",
    "          # Potentially set DB_PATH or other paths based on paper1_directory here if needed\n",
    "          main() # Call main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kaplan-Meier plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanation of the first draft pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "plot_kaplan_meier Function:\n",
    "Takes the DataFrame, column names (duration_col, event_col, strata_col), an optional mapping for legend labels (strata_map), a title prefix, and the output directory.\n",
    "Creates the output directory if needed.\n",
    "Crucially, it first selects only the necessary columns and drops rows where any of them are missing (.dropna()). This ensures KM is run on complete data for that specific plot.\n",
    "It checks if there's enough data and at least two groups to stratify.\n",
    "It iterates through each unique value in the strata_col.\n",
    "For each group, it fits the KaplanMeierFitter and plots the survival function on the same axes (ax).\n",
    "It uses the strata_map to create more readable legend labels if provided.\n",
    "It performs a logrank_test between the first two groups (for simplicity; extend if needed for >2 groups) to get a p-value comparing the curves.\n",
    "It sets the title (including the p-value), labels, and saves the plot to a PNG file in the specified directory.\n",
    "It closes the plot figure (plt.close(fig)) to prevent plots from consuming memory or displaying inline if not desired.\n",
    "Example Usage:\n",
    "Preprocessing: It first calculates the median for the Likert scale columns and creates new binary _median_split columns (0 for below median, 1 for median or above). This is done before the loop.\n",
    "Configuration: Defines the scenarios (targets) and the stratification_vars (including the new median split columns and their label mappings).\n",
    "Looping: It iterates through each scenario and then through each strata_col.\n",
    "Column Checks: It checks if the required duration, event, and strata columns exist in the DataFrame before attempting to plot, printing warnings if not.\n",
    "Function Call: It calls plot_kaplan_meier for each combination.\n",
    "This setup allows you to easily generate all 18 plots (3 scenarios x 6 stratifications) by running the example usage block after defining your df_analysis DataFrame. Remember to adjust column names (days_to_X%_wl, X%_wl_achieved, etc.) if they differ in your actual DataFrame.\n",
    "\n",
    "Changes Made:\n",
    "\n",
    "Clear Config Section: All user-adjustable parameters (paths, scenarios, stratification variables, preprocessing details) are now grouped at the top.\n",
    "SCENARIOS List: Defines each analysis outcome (5% WL, 10% WL, etc.) with its specific duration and event columns and a readable name.\n",
    "STRATIFICATION_CONFIG Dictionary: Defines the columns to stratify by and provides mappings for their legend labels. Median split columns are added dynamically during preprocessing.\n",
    "LIKERT_COLS_FOR_MEDIAN_SPLIT & MEDIAN_SPLIT_MAPPING: Centralizes the logic for creating median split variables.\n",
    "Dynamic Median Split Addition: The preprocessing step now automatically adds the newly created _median_split columns and their mapping to the STRATIFICATION_CONFIG.\n",
    "Refactored Main Loop: The loops now iterate through the SCENARIOS list and the STRATIFICATION_CONFIG dictionary, extracting necessary information.\n",
    "Error Handling: Added checks to ensure required columns (duration, event, strata) exist in the DataFrame before attempting to plot, preventing crashes.\n",
    "if __name__ == \"__main__\": block: Encapsulates the execution logic, making the script potentially importable without running the analysis automatically.\n",
    "Data Loading Example: Included a basic example of loading data from the SQLite database defined in the config.\n",
    "Filename Sanitization: Added basic sanitization for filenames derived from scenario names and strata columns.\n",
    "This structure makes it much easier to:\n",
    "\n",
    "Change the output directory.\n",
    "Add or remove weight loss targets or dropout scenarios.\n",
    "Add or remove variables to stratify by.\n",
    "Adjust the mapping for legend labels.\n",
    "See all the key parameters in one place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code of the first draft pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to database: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\survival_analysis.sqlite\n",
      "Loading table: sa_input_table\n",
      "Data loaded successfully. Shape: (1664, 56)\n",
      "\n",
      "--- Starting Preprocessing ---\n",
      "Created median split for 'emotional_eating_value_likert' as 'emotional_eating_value_likert_median_split' (Median: 7.0)\n",
      "Created median split for 'quantity_control_likert' as 'quantity_control_likert_median_split' (Median: 6.0)\n",
      "Created median split for 'impulse_control_likert' as 'impulse_control_likert_median_split' (Median: 6.0)\n",
      "--- Preprocessing Finished ---\n",
      "\n",
      "--- Generating KM Plots (Output Dir: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots) ---\n",
      "\n",
      "-- Scenario: 5% WL --\n",
      "Saved plot: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots\\km_5perc_wl_by_hunger_yn.png\n",
      "Saved plot: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots\\km_5perc_wl_by_satiety_yn.png\n",
      "Saved plot: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots\\km_5perc_wl_by_emotional_eating_yn.png\n",
      "Saved plot: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots\\km_5perc_wl_by_emotional_eating_value_likert_median_split.png\n",
      "Saved plot: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots\\km_5perc_wl_by_quantity_control_likert_median_split.png\n",
      "Saved plot: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots\\km_5perc_wl_by_impulse_control_likert_median_split.png\n",
      "\n",
      "-- Scenario: 10% WL --\n",
      "Saved plot: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots\\km_10perc_wl_by_hunger_yn.png\n",
      "Saved plot: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots\\km_10perc_wl_by_satiety_yn.png\n",
      "Saved plot: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots\\km_10perc_wl_by_emotional_eating_yn.png\n",
      "Saved plot: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots\\km_10perc_wl_by_emotional_eating_value_likert_median_split.png\n",
      "Saved plot: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots\\km_10perc_wl_by_quantity_control_likert_median_split.png\n",
      "Saved plot: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots\\km_10perc_wl_by_impulse_control_likert_median_split.png\n",
      "\n",
      "-- Scenario: 15% WL --\n",
      "Saved plot: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots\\km_15perc_wl_by_hunger_yn.png\n",
      "Saved plot: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots\\km_15perc_wl_by_satiety_yn.png\n",
      "Saved plot: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots\\km_15perc_wl_by_emotional_eating_yn.png\n",
      "Saved plot: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots\\km_15perc_wl_by_emotional_eating_value_likert_median_split.png\n",
      "Saved plot: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots\\km_15perc_wl_by_quantity_control_likert_median_split.png\n",
      "Saved plot: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots\\km_15perc_wl_by_impulse_control_likert_median_split.png\n",
      "\n",
      "-- Scenario: 40d Dropout --\n",
      "Saved plot: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots\\km_40d_dropout_by_hunger_yn.png\n",
      "Saved plot: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots\\km_40d_dropout_by_satiety_yn.png\n",
      "Saved plot: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots\\km_40d_dropout_by_emotional_eating_yn.png\n",
      "Saved plot: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots\\km_40d_dropout_by_emotional_eating_value_likert_median_split.png\n",
      "Saved plot: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots\\km_40d_dropout_by_quantity_control_likert_median_split.png\n",
      "Saved plot: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots\\km_40d_dropout_by_impulse_control_likert_median_split.png\n",
      "\n",
      "-- Scenario: 60d Dropout --\n",
      "Saved plot: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots\\km_60d_dropout_by_hunger_yn.png\n",
      "Saved plot: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots\\km_60d_dropout_by_satiety_yn.png\n",
      "Saved plot: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots\\km_60d_dropout_by_emotional_eating_yn.png\n",
      "Saved plot: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots\\km_60d_dropout_by_emotional_eating_value_likert_median_split.png\n",
      "Saved plot: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots\\km_60d_dropout_by_quantity_control_likert_median_split.png\n",
      "Saved plot: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots\\km_60d_dropout_by_impulse_control_likert_median_split.png\n",
      "\n",
      "-- Scenario: 80d Dropout --\n",
      "Saved plot: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots\\km_80d_dropout_by_hunger_yn.png\n",
      "Saved plot: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots\\km_80d_dropout_by_satiety_yn.png\n",
      "Saved plot: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots\\km_80d_dropout_by_emotional_eating_yn.png\n",
      "Saved plot: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots\\km_80d_dropout_by_emotional_eating_value_likert_median_split.png\n",
      "Saved plot: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots\\km_80d_dropout_by_quantity_control_likert_median_split.png\n",
      "Saved plot: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\km_plots\\km_80d_dropout_by_impulse_control_likert_median_split.png\n",
      "\n",
      "--- KM Plot generation finished. ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines.statistics import logrank_test\n",
    "import os\n",
    "import sqlite3 # Assuming data is loaded from SQLite\n",
    "\n",
    "# --- Configuration Section ---\n",
    "\n",
    "# --- Input/Output ---\n",
    "# Assuming data is loaded from the same DB as before\n",
    "DB_PATH = os.path.join(paper1_directory, 'survival_analysis.sqlite') # Path to the SQLite database\n",
    "INPUT_TABLE_NAME = \"sa_input_table\" # The table containing all necessary columns\n",
    "OUTPUT_PLOT_DIR = os.path.join(paper1_directory, 'km_plots')\n",
    "\n",
    "# --- Analysis Scenarios ---\n",
    "# Define the different survival outcomes to analyze\n",
    "# Each dict needs:\n",
    "#   'name': A short name for the scenario (used in titles/filenames)\n",
    "#   'duration_col': The column name for time-to-event/censoring\n",
    "#   'event_col': The column name indicating if the event occurred (1) or not (0)\n",
    "SCENARIOS = [\n",
    "    {\n",
    "        'name': '5% WL',\n",
    "        'duration_col': 'days_to_5%_wl',\n",
    "        'event_col': '5%_wl_achieved'\n",
    "    },\n",
    "    {\n",
    "        'name': '10% WL',\n",
    "        'duration_col': 'days_to_10%_wl',\n",
    "        'event_col': '10%_wl_achieved'\n",
    "    },\n",
    "    {\n",
    "        'name': '15% WL',\n",
    "        'duration_col': 'days_to_15%_wl',\n",
    "        'event_col': '15%_wl_achieved'\n",
    "    },\n",
    "    {\n",
    "        'name': '40d Dropout',\n",
    "        'duration_col': 'total_followup_days', # Or a specific days_to_60d_measurement if available\n",
    "        'event_col': '40d_dropout'\n",
    "    },\n",
    "    {\n",
    "        'name': '60d Dropout',\n",
    "        'duration_col': 'total_followup_days', # Or a specific days_to_60d_measurement if available\n",
    "        'event_col': '60d_dropout'\n",
    "    },\n",
    "    {\n",
    "        'name': '80d Dropout',\n",
    "        'duration_col': 'total_followup_days', # Or a specific days_to_60d_measurement if available\n",
    "        'event_col': '80d_dropout'\n",
    "    },\n",
    "]\n",
    "\n",
    "# --- Stratification Variables ---\n",
    "# Define the variables to split the population by for the KM plots\n",
    "# Each key is the column name in the DataFrame.\n",
    "# The value is a dictionary mapping the column's values to readable labels for the legend.\n",
    "# Use None or {} for the mapping if the column values are already readable or no mapping is needed.\n",
    "# Columns requiring median split preprocessing are listed separately below.\n",
    "STRATIFICATION_CONFIG = {\n",
    "    'hunger_yn': {0: 'No', 1: 'Yes'},\n",
    "    'satiety_yn': {0: 'No', 1: 'Yes'},\n",
    "    'emotional_eating_yn': {0: 'No', 1: 'Yes'},\n",
    "    # Median split columns will be added here after preprocessing\n",
    "}\n",
    "\n",
    "# --- Preprocessing Configuration ---\n",
    "LIKERT_COLS_FOR_MEDIAN_SPLIT = [\n",
    "    'emotional_eating_value_likert',\n",
    "    'quantity_control_likert',\n",
    "    'impulse_control_likert'\n",
    "]\n",
    "MEDIAN_SPLIT_MAPPING = {0: '< Median', 1: '>= Median'} # Common mapping for all median splits\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def plot_kaplan_meier(df, duration_col, event_col, strata_col, strata_map, title_prefix, output_dir):\n",
    "    \"\"\"\n",
    "    Generates and saves a Kaplan-Meier plot stratified by a given column.\n",
    "    (Function definition remains the same as provided previously)\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing survival data.\n",
    "        duration_col (str): Name of the column with duration data.\n",
    "        event_col (str): Name of the column with event status (1=event, 0=censored).\n",
    "        strata_col (str): Name of the column to stratify by.\n",
    "        strata_map (dict): Optional dictionary to map strata values to readable labels for the legend.\n",
    "                           Example: {0: 'No', 1: 'Yes'} or {0: '< Median', 1: '>= Median'}\n",
    "        title_prefix (str): Prefix for the plot title (e.g., \"Time to 10% WL\").\n",
    "        output_dir (str): Directory to save the plot image.\n",
    "    \"\"\"\n",
    "    kmf = KaplanMeierFitter()\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    log_rank_results = None\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # --- Data Preparation for this specific plot ---\n",
    "    plot_df = df[[duration_col, event_col, strata_col]].dropna()\n",
    "\n",
    "    if plot_df.empty or plot_df[strata_col].nunique() < 2:\n",
    "        print(f\"Warning: Insufficient data or groups for stratification '{strata_col}'. Skipping plot '{title_prefix} by {strata_col}'.\")\n",
    "        plt.close(fig) # Close the empty figure\n",
    "        return\n",
    "\n",
    "    # --- Fit and Plot for each group ---\n",
    "    all_groups_results = []\n",
    "    unique_strata = sorted(plot_df[strata_col].unique()) # Ensure consistent order\n",
    "\n",
    "    for name in unique_strata:\n",
    "        grouped_df = plot_df[plot_df[strata_col] == name]\n",
    "        if grouped_df.empty:\n",
    "            continue # Should not happen with dropna and nunique check, but safety first\n",
    "\n",
    "        label = strata_map.get(name, name) if strata_map else name # Use mapping if provided\n",
    "        kmf.fit(grouped_df[duration_col], event_observed=grouped_df[event_col], label=f\"{strata_col}: {label}\")\n",
    "        kmf.plot_survival_function(ax=ax)\n",
    "        # Store data for log-rank test\n",
    "        all_groups_results.append({\n",
    "            'durations': grouped_df[duration_col],\n",
    "            'events': grouped_df[event_col],\n",
    "            'label': label # Store label for potential error messages\n",
    "        })\n",
    "\n",
    "    # --- Perform Log-Rank Test ---\n",
    "    if len(all_groups_results) >= 2:\n",
    "        try:\n",
    "            # Simple pairwise log-rank for the first two groups found\n",
    "            # For >2 groups, consider pairwise or other methods if needed\n",
    "            log_rank_results = logrank_test(\n",
    "                durations_A=all_groups_results[0]['durations'],\n",
    "                durations_B=all_groups_results[1]['durations'],\n",
    "                event_observed_A=all_groups_results[0]['events'],\n",
    "                event_observed_B=all_groups_results[1]['events']\n",
    "            )\n",
    "            p_value = log_rank_results.p_value\n",
    "            p_value_text = f\"Log-Rank p={p_value:.3f}\"\n",
    "            if len(all_groups_results) > 2:\n",
    "                 p_value_text += f\" ({all_groups_results[0]['label']} vs {all_groups_results[1]['label']})\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not perform log-rank test for {strata_col}. Error: {e}\")\n",
    "            p_value_text = \"Log-Rank: Error\"\n",
    "    else:\n",
    "        p_value_text = \"Log-Rank: N/A (<=1 group)\"\n",
    "\n",
    "\n",
    "    # --- Finalize Plot ---\n",
    "    plt.title(f\"{title_prefix} by {strata_col}\\n{p_value_text}\")\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Survival Probability (Not Achieving Target)\")\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    # Generate a nicer legend title from the column name\n",
    "    legend_title = strata_col.replace('_', ' ').replace(' yn', '').replace(' median split', '').title()\n",
    "    plt.legend(title=legend_title)\n",
    "\n",
    "    # --- Save Plot ---\n",
    "    # Sanitize scenario name and strata column for filename\n",
    "    safe_scenario_name = scenario['name'].replace('%', 'perc').replace(' ', '_').lower()\n",
    "    safe_strata_col = strata_col.replace('%', 'perc').replace(' ', '_').lower()\n",
    "    filename = f\"km_{safe_scenario_name}_by_{safe_strata_col}.png\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    try:\n",
    "        plt.savefig(filepath, bbox_inches='tight')\n",
    "        print(f\"Saved plot: {filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving plot {filepath}: {e}\")\n",
    "    plt.close(fig) # Close the figure to free memory\n",
    "\n",
    "\n",
    "# --- Main Execution Logic ---\n",
    "\n",
    "if __name__ == \"__main__\": # Ensures this runs only when script is executed directly\n",
    "\n",
    "    # --- 1. Load Data ---\n",
    "    print(f\"Connecting to database: {DB_PATH}\")\n",
    "    try:\n",
    "        con = sqlite3.connect(DB_PATH)\n",
    "        print(f\"Loading table: {INPUT_TABLE_NAME}\")\n",
    "        df_analysis = pd.read_sql_query(f\"SELECT * FROM {INPUT_TABLE_NAME}\", con)\n",
    "        con.close()\n",
    "        print(f\"Data loaded successfully. Shape: {df_analysis.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        exit() # Or handle error appropriately\n",
    "\n",
    "    # --- 2. Preprocessing: Create Median Split Columns ---\n",
    "    print(\"\\n--- Starting Preprocessing ---\")\n",
    "    df_analysis_processed = df_analysis.copy() # Work on a copy\n",
    "\n",
    "    for col in LIKERT_COLS_FOR_MEDIAN_SPLIT:\n",
    "        median_split_col_name = f'{col}_median_split'\n",
    "        if col in df_analysis_processed.columns:\n",
    "            # Ensure column is numeric before calculating median\n",
    "            numeric_col = pd.to_numeric(df_analysis_processed[col], errors='coerce')\n",
    "            median_val = numeric_col.median()\n",
    "            if pd.notna(median_val):\n",
    "                # Create the 0/1 split column\n",
    "                df_analysis_processed[median_split_col_name] = (numeric_col >= median_val).astype(int)\n",
    "                # Add this new column to the stratification config\n",
    "                STRATIFICATION_CONFIG[median_split_col_name] = MEDIAN_SPLIT_MAPPING\n",
    "                print(f\"Created median split for '{col}' as '{median_split_col_name}' (Median: {median_val})\")\n",
    "            else:\n",
    "                print(f\"Warning: Could not calculate median for '{col}' (all NA or non-numeric?). Skipping split.\")\n",
    "                # Optionally create an all-NA column if needed downstream\n",
    "                # df_analysis_processed[median_split_col_name] = np.nan\n",
    "        else:\n",
    "            print(f\"Warning: Column '{col}' not found for median split.\")\n",
    "            # Optionally create an all-NA column if needed downstream\n",
    "            # df_analysis_processed[median_split_col_name] = np.nan\n",
    "\n",
    "    print(\"--- Preprocessing Finished ---\")\n",
    "\n",
    "\n",
    "    # --- 3. Generate Plots based on Configuration ---\n",
    "    print(f\"\\n--- Generating KM Plots (Output Dir: {OUTPUT_PLOT_DIR}) ---\")\n",
    "    for scenario in SCENARIOS:\n",
    "        scenario_name = scenario['name']\n",
    "        duration_col = scenario['duration_col']\n",
    "        event_col = scenario['event_col']\n",
    "\n",
    "        # Check if necessary columns exist for the scenario\n",
    "        if duration_col not in df_analysis_processed.columns or event_col not in df_analysis_processed.columns:\n",
    "            print(f\"\\nWarning: Skipping scenario '{scenario_name}' - Columns '{duration_col}' or '{event_col}' not found in DataFrame.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n-- Scenario: {scenario_name} --\")\n",
    "\n",
    "        for strata_col, strata_map in STRATIFICATION_CONFIG.items():\n",
    "            if strata_col not in df_analysis_processed.columns:\n",
    "                 print(f\"Warning: Stratification column '{strata_col}' not found for scenario '{scenario_name}'. Skipping plot.\")\n",
    "                 continue\n",
    "\n",
    "            # Call the plotting function using configured parameters\n",
    "            plot_kaplan_meier(\n",
    "                df=df_analysis_processed,\n",
    "                duration_col=duration_col,\n",
    "                event_col=event_col,\n",
    "                strata_col=strata_col,\n",
    "                strata_map=strata_map,\n",
    "                title_prefix=f\"Time to {scenario_name}\",\n",
    "                output_dir=OUTPUT_PLOT_DIR\n",
    "            )\n",
    "\n",
    "    print(\"\\n--- KM Plot generation finished. ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary of the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COX\n",
    "\n",
    "\n",
    "Let's refine the pipeline structure specifically for running and evaluating Cox models based on your sa_input_table.\n",
    "\n",
    "Refined Cox Regression Pipeline Structure:\n",
    "\n",
    "CONFIG:\n",
    "\n",
    "DB_PATH: Path to your survival_analysis.sqlite database.\n",
    "INPUT_TABLE_NAME: \"sa_input_table\"\n",
    "OUTPUT_TABLE_NAME: \"cox_results\" (or similar, for storing model summaries)\n",
    "ASSUMPTION_PLOT_DIR: Folder path to save PH assumption plots.\n",
    "SCENARIOS: A list of dictionaries, where each dictionary defines a specific analysis run. This makes it easy to loop through.\n",
    "HELPERS:\n",
    "\n",
    "load_data(db_path, table_name): Loads the sa_input_table.\n",
    "preprocess_data(df): Handles final prep needed specifically for Cox:\n",
    "Ensure categorical predictors/covariates like sex, hunger, satiety, emotional_eating are numerically encoded (e.g., 0/1). This is where you'd implement the conversion if not done upstream.\n",
    "Handle any remaining missing values in the columns needed for the models (e.g., using listwise deletion for simplicity initially, df.dropna(subset=columns_for_model)).\n",
    "Maybe create median split flags here if you want to run models stratified by them (though using continuous is often preferred in Cox).\n",
    "COX REGRESSION MODULE:\n",
    "\n",
    "run_cox_model(df, duration_col, event_col, predictor_cols, scenario_name, plot_dir):\n",
    "Input: DataFrame subset with necessary columns, duration column name, event column name, list of predictor/covariate column names, scenario name (for plot saving), plot directory.\n",
    "Actions:\n",
    "Instantiate CoxPHFitter from lifelines.\n",
    "Fit the model: cph.fit(df, duration_col=duration_col, event_col=event_col, formula=\" + \".join(predictor_cols)) (using formula is often cleaner).\n",
    "Check Proportional Hazards Assumption:\n",
    "Call cph.check_assumptions(df, show_plots=False, p_value_threshold=0.05). This returns results and can optionally plot.\n",
    "Save assumption check summary (p-values).\n",
    "Optionally, generate and save Schoenfeld residual plots for key variables: cph.plot_covariate_groups(...) or manually plot residuals over time. Save plots to plot_dir using scenario_name.\n",
    "Extract results: cph.summary (Hazard Ratios, CIs, p-values), concordance index (cph.concordance_index_), assumption check results.\n",
    "Output: A dictionary containing the model summary (as DataFrame/dict), concordance index, and assumption check results.\n",
    "ORCHESTRATION & EXECUTION:\n",
    "\n",
    "Load data using load_data.\n",
    "Preprocess data using preprocess_data.\n",
    "Initialize an empty list all_results.\n",
    "Loop through each scenario in SCENARIOS:\n",
    "Print which scenario is running.\n",
    "Determine duration_col and event_col based on scenario['event_type']:\n",
    "If event_type == 'target':\n",
    "duration_col = f\"days_to_{scenario['target_perc']}_perc_wl\"\n",
    "event_col = f\"{scenario['target_perc']}_perc_wl_achieved\" (ensure this is 0/1 or bool)\n",
    "If event_type == 'dropout':\n",
    "duration_col = 'total_followup_days' (or specific window like days_to_60d_measurement if appropriate)\n",
    "event_col = f\"{scenario['time_window']}d_dropout\" (ensure this is 0/1 or bool)\n",
    "Define predictor_cols = scenario['predictors'] + scenario['covariates'].\n",
    "Select relevant columns and handle missing data for this specific model: df_model = df_analysis[ [duration_col, event_col] + predictor_cols ].dropna()\n",
    "Check if df_model is empty after dropping NAs. If so, skip.\n",
    "Call run_cox_model with the appropriate arguments.\n",
    "Store the returned results dictionary, adding the scenario_name to it. Append to all_results.\n",
    "Save Results:\n",
    "Convert all_results (list of dicts, where each dict contains model summary etc.) into a structured format. Saving the cph.summary DataFrame directly might be tricky if combining results. A better approach might be to extract key info (variable, HR, CI_lower, CI_upper, p-value, concordance, assumption_p_value) for each scenario into a new summary DataFrame.\n",
    "Connect to the SQLite DB and save this summary DataFrame to the OUTPUT_TABLE_NAME (e.g., cox_results), perhaps using df.to_sql(..., if_exists='append').\n",
    "Key Considerations:\n",
    "\n",
    "Assumption Checking: Yes, the PH assumption needs to be checked for each model you fit, as it depends on the specific covariates included. lifelines makes this relatively easy. If the assumption is violated for a key predictor:\n",
    "Report it as a limitation.\n",
    "Consider stratification (if the violation is due to a categorical variable).\n",
    "Consider time-varying covariates/coefficients (more advanced).\n",
    "Consider alternative models (like accelerated failure time models).\n",
    "Data Types: Ensure your duration columns are numeric (days) and event columns are boolean or 0/1. Categorical predictors must be numerically encoded (dummy variables or 0/1).\n",
    "Interpretation: Focus on the Hazard Ratios (HRs) and their confidence intervals. An HR > 1 means increased hazard (faster event rate), HR < 1 means decreased hazard (slower event rate) for a one-unit increase in the predictor, holding others constant.\n",
    "Saving Output: Saving key summary stats (HR, CI, p-value per variable per model) to a single SQL table is efficient for later aggregation and comparison. Saving plots separately is necessary.\n",
    "This structure provides a modular way to run your exploratory Cox models, check assumptions systematically, and store the results. Remember to handle the upstream data type conversions (like sex to numeric) either in the sa_input_table creation or in the preprocess_data helper function here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanation of the 1st draft pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "Configuration: Sets up paths, table names, and the SCENARIOS list which drives the analysis.\n",
    "Helpers:\n",
    "load_data: Standard function to load a table from SQLite.\n",
    "preprocess_data: Crucial step. This function needs to be adapted to your specific data. It currently includes examples for converting 'sex', 'hunger', etc., to 0/1. You must review and modify this section to correctly handle all your categorical predictors and ensure event/duration columns are numeric.\n",
    "Cox Regression Module:\n",
    "run_cox_model: Takes a prepared DataFrame subset (only relevant columns, no NaNs), fits the CoxPHFitter, checks assumptions using check_assumptions, extracts key results (HR, CI, p-values, concordance), and saves assumption plots if violations occur. It uses a try...except block to handle potential convergence errors during fitting.\n",
    "Orchestration & Execution (main function):\n",
    "Loads the raw data.\n",
    "Calls preprocess_data.\n",
    "Loops through each scenario dictionary in SCENARIOS.\n",
    "Determines the correct duration_col and event_col based on event_type.\n",
    "Selects the necessary columns for the current model.\n",
    "Uses .dropna() to perform listwise deletion for that specific model.\n",
    "Calls run_cox_model.\n",
    "If the model runs successfully, it extracts key summary statistics (HR, CI, p-value for each variable, plus overall concordance, N, etc.) and appends them to the all_results_summary list.\n",
    "Finally, it converts all_results_summary into a pandas DataFrame and saves it to a new table (cox_regression_results) in your SQLite database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code of the 1st draft pipeline - REVISE PREPROCESS and FIX LIFELINES INSTALLATION ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Starting Cox Regression Pipeline ==========\n",
      "Loading data from C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\survival_analysis.sqlite, table sa_input_table...\n",
      "Loaded 1664 rows and 56 columns.\n",
      "Preprocessing data for Cox models...\n",
      "Preprocessing complete.\n",
      "\n",
      "--- Processing Scenario: 60d_10p_hunger_unadjusted ---\n",
      "  Running Cox model for scenario: 60d_10p_hunger_unadjusted\n",
      "    Duration: 60d_10p_hunger_unadjusted_duration, Event: 10%_wl_achieved\n",
      "    Predictors: ['hunger_yn']\n",
      "    Data shape: (1664, 3)\n",
      "    Checking proportional hazards assumption...\n",
      "Proportional hazard assumption looks okay.\n",
      "    Model fitting and assumption check complete.\n",
      "\n",
      "--- Processing Scenario: 60d_10p_satiety_unadjusted ---\n",
      "  Running Cox model for scenario: 60d_10p_satiety_unadjusted\n",
      "    Duration: 60d_10p_satiety_unadjusted_duration, Event: 10%_wl_achieved\n",
      "    Predictors: ['satiety_yn']\n",
      "    Data shape: (1664, 3)\n",
      "    Checking proportional hazards assumption...\n",
      "Proportional hazard assumption looks okay.\n",
      "    Model fitting and assumption check complete.\n",
      "\n",
      "--- Processing Scenario: 60d_10p_emotional_eating_unadjusted ---\n",
      "  Running Cox model for scenario: 60d_10p_emotional_eating_unadjusted\n",
      "    Duration: 60d_10p_emotional_eating_unadjusted_duration, Event: 10%_wl_achieved\n",
      "    Predictors: ['emotional_eating_yn']\n",
      "    Data shape: (1664, 3)\n",
      "    Checking proportional hazards assumption...\n",
      "Proportional hazard assumption looks okay.\n",
      "    Model fitting and assumption check complete.\n",
      "\n",
      "--- Processing Scenario: 60d_10p_emotional_eating_value_unadjusted ---\n",
      "  Running Cox model for scenario: 60d_10p_emotional_eating_value_unadjusted\n",
      "    Duration: 60d_10p_emotional_eating_value_unadjusted_duration, Event: 10%_wl_achieved\n",
      "    Predictors: ['emotional_eating_value_likert']\n",
      "    Data shape: (1664, 3)\n",
      "    Checking proportional hazards assumption...\n",
      "Proportional hazard assumption looks okay.\n",
      "    Model fitting and assumption check complete.\n",
      "\n",
      "--- Processing Scenario: 60d_10p_quantity_control_unadjusted ---\n",
      "  Running Cox model for scenario: 60d_10p_quantity_control_unadjusted\n",
      "    Duration: 60d_10p_quantity_control_unadjusted_duration, Event: 10%_wl_achieved\n",
      "    Predictors: ['quantity_control_likert']\n",
      "    Data shape: (1664, 3)\n",
      "    Checking proportional hazards assumption...\n",
      "Proportional hazard assumption looks okay.\n",
      "    Model fitting and assumption check complete.\n",
      "\n",
      "--- Processing Scenario: 60d_10p_impulse_control_unadjusted ---\n",
      "  Running Cox model for scenario: 60d_10p_impulse_control_unadjusted\n",
      "    Duration: 60d_10p_impulse_control_unadjusted_duration, Event: 10%_wl_achieved\n",
      "    Predictors: ['impulse_control_likert']\n",
      "    Data shape: (1664, 3)\n",
      "    Checking proportional hazards assumption...\n",
      "Proportional hazard assumption looks okay.\n",
      "    Model fitting and assumption check complete.\n",
      "\n",
      "--- Processing Scenario: 60d_10p_hunger_adjusted ---\n",
      "  Running Cox model for scenario: 60d_10p_hunger_adjusted\n",
      "    Duration: 60d_10p_hunger_adjusted_duration, Event: 10%_wl_achieved\n",
      "    Predictors: ['hunger_yn', 'age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi', 'height_m']\n",
      "    Data shape: (1664, 8)\n",
      "    Checking proportional hazards assumption...\n",
      "The ``p_value_threshold`` is set at 0.05. Even under the null hypothesis of no violations, some\n",
      "covariates will be below the threshold by chance. This is compounded when there are many covariates.\n",
      "Similarly, when there are lots of observations, even minor deviances from the proportional hazard\n",
      "assumption will be flagged.\n",
      "\n",
      "With that in mind, it's best to use a combination of statistical tests and visual tests to determine\n",
      "the most serious violations. Produce visual plots using ``check_assumptions(..., show_plots=True)``\n",
      "and looking for non-constant lines. See link [A] below for a full example.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>null_distribution</th>\n",
       "      <td>chi squared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>degrees_of_freedom</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>&lt;lifelines.CoxPHFitter: fitted with 1664 total...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_name</th>\n",
       "      <td>proportional_hazard_test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>test_statistic</th>\n",
       "      <th>p</th>\n",
       "      <th>-log2(p)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">age</th>\n",
       "      <th>km</th>\n",
       "      <td>12.63</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>11.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>13.79</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>12.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">baseline_bmi</th>\n",
       "      <th>km</th>\n",
       "      <td>3.39</td>\n",
       "      <td>0.07</td>\n",
       "      <td>3.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>2.93</td>\n",
       "      <td>0.09</td>\n",
       "      <td>3.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">baseline_weight_kg</th>\n",
       "      <th>km</th>\n",
       "      <td>2.60</td>\n",
       "      <td>0.11</td>\n",
       "      <td>3.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>2.20</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">height_m</th>\n",
       "      <th>km</th>\n",
       "      <td>3.97</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>3.53</td>\n",
       "      <td>0.06</td>\n",
       "      <td>4.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">hunger_yn</th>\n",
       "      <th>km</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">sex_f</th>\n",
       "      <th>km</th>\n",
       "      <td>40.27</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>32.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>43.70</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>34.61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{tabular}{llrrr}\n",
       " &  & test_statistic & p & -log2(p) \\\\\n",
       "\\multirow[c]{2}{*}{age} & km & 12.63 & 0.00 & 11.36 \\\\\n",
       " & rank & 13.79 & 0.00 & 12.26 \\\\\n",
       "\\multirow[c]{2}{*}{baseline_bmi} & km & 3.39 & 0.07 & 3.93 \\\\\n",
       " & rank & 2.93 & 0.09 & 3.52 \\\\\n",
       "\\multirow[c]{2}{*}{baseline_weight_kg} & km & 2.60 & 0.11 & 3.22 \\\\\n",
       " & rank & 2.20 & 0.14 & 2.85 \\\\\n",
       "\\multirow[c]{2}{*}{height_m} & km & 3.97 & 0.05 & 4.43 \\\\\n",
       " & rank & 3.53 & 0.06 & 4.05 \\\\\n",
       "\\multirow[c]{2}{*}{hunger_yn} & km & 0.32 & 0.57 & 0.80 \\\\\n",
       " & rank & 0.75 & 0.39 & 1.38 \\\\\n",
       "\\multirow[c]{2}{*}{sex_f} & km & 40.27 & 0.00 & 32.08 \\\\\n",
       " & rank & 43.70 & 0.00 & 34.61 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "<lifelines.StatisticalResult: proportional_hazard_test>\n",
       " null_distribution = chi squared\n",
       "degrees_of_freedom = 1\n",
       "             model = <lifelines.CoxPHFitter: fitted with 1664 total observations, 1118 right-censored observations>\n",
       "         test_name = proportional_hazard_test\n",
       "\n",
       "---\n",
       "                         test_statistic      p  -log2(p)\n",
       "age                km             12.63 <0.005     11.36\n",
       "                   rank           13.79 <0.005     12.26\n",
       "baseline_bmi       km              3.39   0.07      3.93\n",
       "                   rank            2.93   0.09      3.52\n",
       "baseline_weight_kg km              2.60   0.11      3.22\n",
       "                   rank            2.20   0.14      2.85\n",
       "height_m           km              3.97   0.05      4.43\n",
       "                   rank            3.53   0.06      4.05\n",
       "hunger_yn          km              0.32   0.57      0.80\n",
       "                   rank            0.75   0.39      1.38\n",
       "sex_f              km             40.27 <0.005     32.08\n",
       "                   rank           43.70 <0.005     34.61"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Variable 'age' failed the non-proportional test: p-value is 0.0002.\n",
      "\n",
      "   Advice 1: the functional form of the variable 'age' might be incorrect. That is, there may be\n",
      "non-linear terms missing. The proportional hazard test used is very sensitive to incorrect\n",
      "functional forms. See documentation in link [D] below on how to specify a functional form.\n",
      "\n",
      "   Advice 2: try binning the variable 'age' using pd.cut, and then specify it in `strata=['age',\n",
      "...]` in the call in `.fit`. See documentation in link [B] below.\n",
      "\n",
      "   Advice 3: try adding an interaction term with your time variable. See documentation in link [C]\n",
      "below.\n",
      "\n",
      "\n",
      "2. Variable 'sex_f' failed the non-proportional test: p-value is <5e-05.\n",
      "\n",
      "   Advice: with so few unique values (only 2), you can include `strata=['sex_f', ...]` in the call\n",
      "in `.fit`. See documentation in link [E] below.\n",
      "\n",
      "3. Variable 'height_m' failed the non-proportional test: p-value is 0.0462.\n",
      "\n",
      "   Advice 1: the functional form of the variable 'height_m' might be incorrect. That is, there may\n",
      "be non-linear terms missing. The proportional hazard test used is very sensitive to incorrect\n",
      "functional forms. See documentation in link [D] below on how to specify a functional form.\n",
      "\n",
      "   Advice 2: try binning the variable 'height_m' using pd.cut, and then specify it in\n",
      "`strata=['height_m', ...]` in the call in `.fit`. See documentation in link [B] below.\n",
      "\n",
      "   Advice 3: try adding an interaction term with your time variable. See documentation in link [C]\n",
      "below.\n",
      "\n",
      "\n",
      "---\n",
      "[A]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html\n",
      "[B]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Bin-variable-and-stratify-on-it\n",
      "[C]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Introduce-time-varying-covariates\n",
      "[D]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Modify-the-functional-form\n",
      "[E]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Stratification\n",
      "\n",
      "    Model fitting and assumption check complete.\n",
      "\n",
      "--- Processing Scenario: 60d_10p_satiety_adjusted ---\n",
      "  Running Cox model for scenario: 60d_10p_satiety_adjusted\n",
      "    Duration: 60d_10p_satiety_adjusted_duration, Event: 10%_wl_achieved\n",
      "    Predictors: ['satiety_yn', 'age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi', 'height_m']\n",
      "    Data shape: (1664, 8)\n",
      "    Checking proportional hazards assumption...\n",
      "The ``p_value_threshold`` is set at 0.05. Even under the null hypothesis of no violations, some\n",
      "covariates will be below the threshold by chance. This is compounded when there are many covariates.\n",
      "Similarly, when there are lots of observations, even minor deviances from the proportional hazard\n",
      "assumption will be flagged.\n",
      "\n",
      "With that in mind, it's best to use a combination of statistical tests and visual tests to determine\n",
      "the most serious violations. Produce visual plots using ``check_assumptions(..., show_plots=True)``\n",
      "and looking for non-constant lines. See link [A] below for a full example.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>null_distribution</th>\n",
       "      <td>chi squared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>degrees_of_freedom</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>&lt;lifelines.CoxPHFitter: fitted with 1664 total...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_name</th>\n",
       "      <td>proportional_hazard_test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>test_statistic</th>\n",
       "      <th>p</th>\n",
       "      <th>-log2(p)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">age</th>\n",
       "      <th>km</th>\n",
       "      <td>12.86</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>11.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>13.99</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>12.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">baseline_bmi</th>\n",
       "      <th>km</th>\n",
       "      <td>3.70</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>3.25</td>\n",
       "      <td>0.07</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">baseline_weight_kg</th>\n",
       "      <th>km</th>\n",
       "      <td>2.89</td>\n",
       "      <td>0.09</td>\n",
       "      <td>3.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>2.50</td>\n",
       "      <td>0.11</td>\n",
       "      <td>3.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">height_m</th>\n",
       "      <th>km</th>\n",
       "      <td>4.32</td>\n",
       "      <td>0.04</td>\n",
       "      <td>4.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>3.90</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">satiety_yn</th>\n",
       "      <th>km</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">sex_f</th>\n",
       "      <th>km</th>\n",
       "      <td>40.26</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>32.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>43.54</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>34.49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{tabular}{llrrr}\n",
       " &  & test_statistic & p & -log2(p) \\\\\n",
       "\\multirow[c]{2}{*}{age} & km & 12.86 & 0.00 & 11.54 \\\\\n",
       " & rank & 13.99 & 0.00 & 12.41 \\\\\n",
       "\\multirow[c]{2}{*}{baseline_bmi} & km & 3.70 & 0.05 & 4.20 \\\\\n",
       " & rank & 3.25 & 0.07 & 3.80 \\\\\n",
       "\\multirow[c]{2}{*}{baseline_weight_kg} & km & 2.89 & 0.09 & 3.49 \\\\\n",
       " & rank & 2.50 & 0.11 & 3.13 \\\\\n",
       "\\multirow[c]{2}{*}{height_m} & km & 4.32 & 0.04 & 4.73 \\\\\n",
       " & rank & 3.90 & 0.05 & 4.37 \\\\\n",
       "\\multirow[c]{2}{*}{satiety_yn} & km & 0.00 & 0.97 & 0.05 \\\\\n",
       " & rank & 0.00 & 0.98 & 0.03 \\\\\n",
       "\\multirow[c]{2}{*}{sex_f} & km & 40.26 & 0.00 & 32.06 \\\\\n",
       " & rank & 43.54 & 0.00 & 34.49 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "<lifelines.StatisticalResult: proportional_hazard_test>\n",
       " null_distribution = chi squared\n",
       "degrees_of_freedom = 1\n",
       "             model = <lifelines.CoxPHFitter: fitted with 1664 total observations, 1118 right-censored observations>\n",
       "         test_name = proportional_hazard_test\n",
       "\n",
       "---\n",
       "                         test_statistic      p  -log2(p)\n",
       "age                km             12.86 <0.005     11.54\n",
       "                   rank           13.99 <0.005     12.41\n",
       "baseline_bmi       km              3.70   0.05      4.20\n",
       "                   rank            3.25   0.07      3.80\n",
       "baseline_weight_kg km              2.89   0.09      3.49\n",
       "                   rank            2.50   0.11      3.13\n",
       "height_m           km              4.32   0.04      4.73\n",
       "                   rank            3.90   0.05      4.37\n",
       "satiety_yn         km              0.00   0.97      0.05\n",
       "                   rank            0.00   0.98      0.03\n",
       "sex_f              km             40.26 <0.005     32.06\n",
       "                   rank           43.54 <0.005     34.49"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Variable 'age' failed the non-proportional test: p-value is 0.0002.\n",
      "\n",
      "   Advice 1: the functional form of the variable 'age' might be incorrect. That is, there may be\n",
      "non-linear terms missing. The proportional hazard test used is very sensitive to incorrect\n",
      "functional forms. See documentation in link [D] below on how to specify a functional form.\n",
      "\n",
      "   Advice 2: try binning the variable 'age' using pd.cut, and then specify it in `strata=['age',\n",
      "...]` in the call in `.fit`. See documentation in link [B] below.\n",
      "\n",
      "   Advice 3: try adding an interaction term with your time variable. See documentation in link [C]\n",
      "below.\n",
      "\n",
      "\n",
      "2. Variable 'sex_f' failed the non-proportional test: p-value is <5e-05.\n",
      "\n",
      "   Advice: with so few unique values (only 2), you can include `strata=['sex_f', ...]` in the call\n",
      "in `.fit`. See documentation in link [E] below.\n",
      "\n",
      "3. Variable 'baseline_bmi' failed the non-proportional test: p-value is 0.0543.\n",
      "\n",
      "   Advice 1: the functional form of the variable 'baseline_bmi' might be incorrect. That is, there\n",
      "may be non-linear terms missing. The proportional hazard test used is very sensitive to incorrect\n",
      "functional forms. See documentation in link [D] below on how to specify a functional form.\n",
      "\n",
      "   Advice 2: try binning the variable 'baseline_bmi' using pd.cut, and then specify it in\n",
      "`strata=['baseline_bmi', ...]` in the call in `.fit`. See documentation in link [B] below.\n",
      "\n",
      "   Advice 3: try adding an interaction term with your time variable. See documentation in link [C]\n",
      "below.\n",
      "\n",
      "\n",
      "4. Variable 'height_m' failed the non-proportional test: p-value is 0.0377.\n",
      "\n",
      "   Advice 1: the functional form of the variable 'height_m' might be incorrect. That is, there may\n",
      "be non-linear terms missing. The proportional hazard test used is very sensitive to incorrect\n",
      "functional forms. See documentation in link [D] below on how to specify a functional form.\n",
      "\n",
      "   Advice 2: try binning the variable 'height_m' using pd.cut, and then specify it in\n",
      "`strata=['height_m', ...]` in the call in `.fit`. See documentation in link [B] below.\n",
      "\n",
      "   Advice 3: try adding an interaction term with your time variable. See documentation in link [C]\n",
      "below.\n",
      "\n",
      "\n",
      "---\n",
      "[A]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html\n",
      "[B]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Bin-variable-and-stratify-on-it\n",
      "[C]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Introduce-time-varying-covariates\n",
      "[D]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Modify-the-functional-form\n",
      "[E]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Stratification\n",
      "\n",
      "    Model fitting and assumption check complete.\n",
      "\n",
      "--- Processing Scenario: 60d_10p_emotional_eating_adjusted ---\n",
      "  Running Cox model for scenario: 60d_10p_emotional_eating_adjusted\n",
      "    Duration: 60d_10p_emotional_eating_adjusted_duration, Event: 10%_wl_achieved\n",
      "    Predictors: ['emotional_eating_yn', 'age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi', 'height_m']\n",
      "    Data shape: (1664, 8)\n",
      "    Checking proportional hazards assumption...\n",
      "The ``p_value_threshold`` is set at 0.05. Even under the null hypothesis of no violations, some\n",
      "covariates will be below the threshold by chance. This is compounded when there are many covariates.\n",
      "Similarly, when there are lots of observations, even minor deviances from the proportional hazard\n",
      "assumption will be flagged.\n",
      "\n",
      "With that in mind, it's best to use a combination of statistical tests and visual tests to determine\n",
      "the most serious violations. Produce visual plots using ``check_assumptions(..., show_plots=True)``\n",
      "and looking for non-constant lines. See link [A] below for a full example.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>null_distribution</th>\n",
       "      <td>chi squared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>degrees_of_freedom</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>&lt;lifelines.CoxPHFitter: fitted with 1664 total...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_name</th>\n",
       "      <td>proportional_hazard_test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>test_statistic</th>\n",
       "      <th>p</th>\n",
       "      <th>-log2(p)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">age</th>\n",
       "      <th>km</th>\n",
       "      <td>12.24</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>11.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>13.33</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>11.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">baseline_bmi</th>\n",
       "      <th>km</th>\n",
       "      <td>3.44</td>\n",
       "      <td>0.06</td>\n",
       "      <td>3.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>3.03</td>\n",
       "      <td>0.08</td>\n",
       "      <td>3.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">baseline_weight_kg</th>\n",
       "      <th>km</th>\n",
       "      <td>2.63</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>2.27</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">emotional_eating_yn</th>\n",
       "      <th>km</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">height_m</th>\n",
       "      <th>km</th>\n",
       "      <td>4.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>3.62</td>\n",
       "      <td>0.06</td>\n",
       "      <td>4.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">sex_f</th>\n",
       "      <th>km</th>\n",
       "      <td>38.97</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>31.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>42.13</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>33.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{tabular}{llrrr}\n",
       " &  & test_statistic & p & -log2(p) \\\\\n",
       "\\multirow[c]{2}{*}{age} & km & 12.24 & 0.00 & 11.06 \\\\\n",
       " & rank & 13.33 & 0.00 & 11.90 \\\\\n",
       "\\multirow[c]{2}{*}{baseline_bmi} & km & 3.44 & 0.06 & 3.98 \\\\\n",
       " & rank & 3.03 & 0.08 & 3.61 \\\\\n",
       "\\multirow[c]{2}{*}{baseline_weight_kg} & km & 2.63 & 0.10 & 3.25 \\\\\n",
       " & rank & 2.27 & 0.13 & 2.92 \\\\\n",
       "\\multirow[c]{2}{*}{emotional_eating_yn} & km & 0.01 & 0.93 & 0.11 \\\\\n",
       " & rank & 0.07 & 0.79 & 0.33 \\\\\n",
       "\\multirow[c]{2}{*}{height_m} & km & 4.01 & 0.05 & 4.47 \\\\\n",
       " & rank & 3.62 & 0.06 & 4.13 \\\\\n",
       "\\multirow[c]{2}{*}{sex_f} & km & 38.97 & 0.00 & 31.11 \\\\\n",
       " & rank & 42.13 & 0.00 & 33.45 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "<lifelines.StatisticalResult: proportional_hazard_test>\n",
       " null_distribution = chi squared\n",
       "degrees_of_freedom = 1\n",
       "             model = <lifelines.CoxPHFitter: fitted with 1664 total observations, 1118 right-censored observations>\n",
       "         test_name = proportional_hazard_test\n",
       "\n",
       "---\n",
       "                          test_statistic      p  -log2(p)\n",
       "age                 km             12.24 <0.005     11.06\n",
       "                    rank           13.33 <0.005     11.90\n",
       "baseline_bmi        km              3.44   0.06      3.98\n",
       "                    rank            3.03   0.08      3.61\n",
       "baseline_weight_kg  km              2.63   0.10      3.25\n",
       "                    rank            2.27   0.13      2.92\n",
       "emotional_eating_yn km              0.01   0.93      0.11\n",
       "                    rank            0.07   0.79      0.33\n",
       "height_m            km              4.01   0.05      4.47\n",
       "                    rank            3.62   0.06      4.13\n",
       "sex_f               km             38.97 <0.005     31.11\n",
       "                    rank           42.13 <0.005     33.45"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Variable 'age' failed the non-proportional test: p-value is 0.0003.\n",
      "\n",
      "   Advice 1: the functional form of the variable 'age' might be incorrect. That is, there may be\n",
      "non-linear terms missing. The proportional hazard test used is very sensitive to incorrect\n",
      "functional forms. See documentation in link [D] below on how to specify a functional form.\n",
      "\n",
      "   Advice 2: try binning the variable 'age' using pd.cut, and then specify it in `strata=['age',\n",
      "...]` in the call in `.fit`. See documentation in link [B] below.\n",
      "\n",
      "   Advice 3: try adding an interaction term with your time variable. See documentation in link [C]\n",
      "below.\n",
      "\n",
      "\n",
      "2. Variable 'sex_f' failed the non-proportional test: p-value is <5e-05.\n",
      "\n",
      "   Advice: with so few unique values (only 2), you can include `strata=['sex_f', ...]` in the call\n",
      "in `.fit`. See documentation in link [E] below.\n",
      "\n",
      "3. Variable 'height_m' failed the non-proportional test: p-value is 0.0452.\n",
      "\n",
      "   Advice 1: the functional form of the variable 'height_m' might be incorrect. That is, there may\n",
      "be non-linear terms missing. The proportional hazard test used is very sensitive to incorrect\n",
      "functional forms. See documentation in link [D] below on how to specify a functional form.\n",
      "\n",
      "   Advice 2: try binning the variable 'height_m' using pd.cut, and then specify it in\n",
      "`strata=['height_m', ...]` in the call in `.fit`. See documentation in link [B] below.\n",
      "\n",
      "   Advice 3: try adding an interaction term with your time variable. See documentation in link [C]\n",
      "below.\n",
      "\n",
      "\n",
      "---\n",
      "[A]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html\n",
      "[B]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Bin-variable-and-stratify-on-it\n",
      "[C]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Introduce-time-varying-covariates\n",
      "[D]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Modify-the-functional-form\n",
      "[E]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Stratification\n",
      "\n",
      "    Model fitting and assumption check complete.\n",
      "\n",
      "--- Processing Scenario: 60d_10p_emotional_eating_value_adjusted ---\n",
      "  Running Cox model for scenario: 60d_10p_emotional_eating_value_adjusted\n",
      "    Duration: 60d_10p_emotional_eating_value_adjusted_duration, Event: 10%_wl_achieved\n",
      "    Predictors: ['emotional_eating_value_likert', 'age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi', 'height_m']\n",
      "    Data shape: (1664, 8)\n",
      "    Checking proportional hazards assumption...\n",
      "The ``p_value_threshold`` is set at 0.05. Even under the null hypothesis of no violations, some\n",
      "covariates will be below the threshold by chance. This is compounded when there are many covariates.\n",
      "Similarly, when there are lots of observations, even minor deviances from the proportional hazard\n",
      "assumption will be flagged.\n",
      "\n",
      "With that in mind, it's best to use a combination of statistical tests and visual tests to determine\n",
      "the most serious violations. Produce visual plots using ``check_assumptions(..., show_plots=True)``\n",
      "and looking for non-constant lines. See link [A] below for a full example.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>null_distribution</th>\n",
       "      <td>chi squared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>degrees_of_freedom</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>&lt;lifelines.CoxPHFitter: fitted with 1664 total...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_name</th>\n",
       "      <td>proportional_hazard_test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>test_statistic</th>\n",
       "      <th>p</th>\n",
       "      <th>-log2(p)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">age</th>\n",
       "      <th>km</th>\n",
       "      <td>12.36</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>11.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>13.55</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>12.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">baseline_bmi</th>\n",
       "      <th>km</th>\n",
       "      <td>3.40</td>\n",
       "      <td>0.07</td>\n",
       "      <td>3.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>2.98</td>\n",
       "      <td>0.08</td>\n",
       "      <td>3.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">baseline_weight_kg</th>\n",
       "      <th>km</th>\n",
       "      <td>2.61</td>\n",
       "      <td>0.11</td>\n",
       "      <td>3.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>2.26</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">emotional_eating_value_likert</th>\n",
       "      <th>km</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">height_m</th>\n",
       "      <th>km</th>\n",
       "      <td>4.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>3.62</td>\n",
       "      <td>0.06</td>\n",
       "      <td>4.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">sex_f</th>\n",
       "      <th>km</th>\n",
       "      <td>39.14</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>31.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>42.32</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>33.58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{tabular}{llrrr}\n",
       " &  & test_statistic & p & -log2(p) \\\\\n",
       "\\multirow[c]{2}{*}{age} & km & 12.36 & 0.00 & 11.15 \\\\\n",
       " & rank & 13.55 & 0.00 & 12.07 \\\\\n",
       "\\multirow[c]{2}{*}{baseline_bmi} & km & 3.40 & 0.07 & 3.94 \\\\\n",
       " & rank & 2.98 & 0.08 & 3.57 \\\\\n",
       "\\multirow[c]{2}{*}{baseline_weight_kg} & km & 2.61 & 0.11 & 3.24 \\\\\n",
       " & rank & 2.26 & 0.13 & 2.91 \\\\\n",
       "\\multirow[c]{2}{*}{emotional_eating_value_likert} & km & 0.02 & 0.90 & 0.16 \\\\\n",
       " & rank & 0.04 & 0.85 & 0.24 \\\\\n",
       "\\multirow[c]{2}{*}{height_m} & km & 4.01 & 0.05 & 4.46 \\\\\n",
       " & rank & 3.62 & 0.06 & 4.13 \\\\\n",
       "\\multirow[c]{2}{*}{sex_f} & km & 39.14 & 0.00 & 31.24 \\\\\n",
       " & rank & 42.32 & 0.00 & 33.58 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "<lifelines.StatisticalResult: proportional_hazard_test>\n",
       " null_distribution = chi squared\n",
       "degrees_of_freedom = 1\n",
       "             model = <lifelines.CoxPHFitter: fitted with 1664 total observations, 1118 right-censored observations>\n",
       "         test_name = proportional_hazard_test\n",
       "\n",
       "---\n",
       "                                    test_statistic      p  -log2(p)\n",
       "age                           km             12.36 <0.005     11.15\n",
       "                              rank           13.55 <0.005     12.07\n",
       "baseline_bmi                  km              3.40   0.07      3.94\n",
       "                              rank            2.98   0.08      3.57\n",
       "baseline_weight_kg            km              2.61   0.11      3.24\n",
       "                              rank            2.26   0.13      2.91\n",
       "emotional_eating_value_likert km              0.02   0.90      0.16\n",
       "                              rank            0.04   0.85      0.24\n",
       "height_m                      km              4.01   0.05      4.46\n",
       "                              rank            3.62   0.06      4.13\n",
       "sex_f                         km             39.14 <0.005     31.24\n",
       "                              rank           42.32 <0.005     33.58"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Variable 'age' failed the non-proportional test: p-value is 0.0002.\n",
      "\n",
      "   Advice 1: the functional form of the variable 'age' might be incorrect. That is, there may be\n",
      "non-linear terms missing. The proportional hazard test used is very sensitive to incorrect\n",
      "functional forms. See documentation in link [D] below on how to specify a functional form.\n",
      "\n",
      "   Advice 2: try binning the variable 'age' using pd.cut, and then specify it in `strata=['age',\n",
      "...]` in the call in `.fit`. See documentation in link [B] below.\n",
      "\n",
      "   Advice 3: try adding an interaction term with your time variable. See documentation in link [C]\n",
      "below.\n",
      "\n",
      "\n",
      "2. Variable 'sex_f' failed the non-proportional test: p-value is <5e-05.\n",
      "\n",
      "   Advice: with so few unique values (only 2), you can include `strata=['sex_f', ...]` in the call\n",
      "in `.fit`. See documentation in link [E] below.\n",
      "\n",
      "3. Variable 'height_m' failed the non-proportional test: p-value is 0.0453.\n",
      "\n",
      "   Advice 1: the functional form of the variable 'height_m' might be incorrect. That is, there may\n",
      "be non-linear terms missing. The proportional hazard test used is very sensitive to incorrect\n",
      "functional forms. See documentation in link [D] below on how to specify a functional form.\n",
      "\n",
      "   Advice 2: try binning the variable 'height_m' using pd.cut, and then specify it in\n",
      "`strata=['height_m', ...]` in the call in `.fit`. See documentation in link [B] below.\n",
      "\n",
      "   Advice 3: try adding an interaction term with your time variable. See documentation in link [C]\n",
      "below.\n",
      "\n",
      "\n",
      "---\n",
      "[A]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html\n",
      "[B]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Bin-variable-and-stratify-on-it\n",
      "[C]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Introduce-time-varying-covariates\n",
      "[D]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Modify-the-functional-form\n",
      "[E]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Stratification\n",
      "\n",
      "    Model fitting and assumption check complete.\n",
      "\n",
      "--- Processing Scenario: 60d_10p_quantity_control_adjusted ---\n",
      "  Running Cox model for scenario: 60d_10p_quantity_control_adjusted\n",
      "    Duration: 60d_10p_quantity_control_adjusted_duration, Event: 10%_wl_achieved\n",
      "    Predictors: ['quantity_control_likert', 'age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi', 'height_m']\n",
      "    Data shape: (1664, 8)\n",
      "    Checking proportional hazards assumption...\n",
      "The ``p_value_threshold`` is set at 0.05. Even under the null hypothesis of no violations, some\n",
      "covariates will be below the threshold by chance. This is compounded when there are many covariates.\n",
      "Similarly, when there are lots of observations, even minor deviances from the proportional hazard\n",
      "assumption will be flagged.\n",
      "\n",
      "With that in mind, it's best to use a combination of statistical tests and visual tests to determine\n",
      "the most serious violations. Produce visual plots using ``check_assumptions(..., show_plots=True)``\n",
      "and looking for non-constant lines. See link [A] below for a full example.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>null_distribution</th>\n",
       "      <td>chi squared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>degrees_of_freedom</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>&lt;lifelines.CoxPHFitter: fitted with 1664 total...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_name</th>\n",
       "      <td>proportional_hazard_test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>test_statistic</th>\n",
       "      <th>p</th>\n",
       "      <th>-log2(p)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">age</th>\n",
       "      <th>km</th>\n",
       "      <td>11.69</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>10.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>12.84</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>11.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">baseline_bmi</th>\n",
       "      <th>km</th>\n",
       "      <td>3.66</td>\n",
       "      <td>0.06</td>\n",
       "      <td>4.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>3.22</td>\n",
       "      <td>0.07</td>\n",
       "      <td>3.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">baseline_weight_kg</th>\n",
       "      <th>km</th>\n",
       "      <td>2.81</td>\n",
       "      <td>0.09</td>\n",
       "      <td>3.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>2.43</td>\n",
       "      <td>0.12</td>\n",
       "      <td>3.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">height_m</th>\n",
       "      <th>km</th>\n",
       "      <td>4.26</td>\n",
       "      <td>0.04</td>\n",
       "      <td>4.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>3.86</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">quantity_control_likert</th>\n",
       "      <th>km</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">sex_f</th>\n",
       "      <th>km</th>\n",
       "      <td>40.31</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>32.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>43.57</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>34.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{tabular}{llrrr}\n",
       " &  & test_statistic & p & -log2(p) \\\\\n",
       "\\multirow[c]{2}{*}{age} & km & 11.69 & 0.00 & 10.64 \\\\\n",
       " & rank & 12.84 & 0.00 & 11.52 \\\\\n",
       "\\multirow[c]{2}{*}{baseline_bmi} & km & 3.66 & 0.06 & 4.16 \\\\\n",
       " & rank & 3.22 & 0.07 & 3.78 \\\\\n",
       "\\multirow[c]{2}{*}{baseline_weight_kg} & km & 2.81 & 0.09 & 3.41 \\\\\n",
       " & rank & 2.43 & 0.12 & 3.07 \\\\\n",
       "\\multirow[c]{2}{*}{height_m} & km & 4.26 & 0.04 & 4.68 \\\\\n",
       " & rank & 3.86 & 0.05 & 4.34 \\\\\n",
       "\\multirow[c]{2}{*}{quantity_control_likert} & km & 0.32 & 0.57 & 0.81 \\\\\n",
       " & rank & 0.41 & 0.52 & 0.94 \\\\\n",
       "\\multirow[c]{2}{*}{sex_f} & km & 40.31 & 0.00 & 32.10 \\\\\n",
       " & rank & 43.57 & 0.00 & 34.51 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "<lifelines.StatisticalResult: proportional_hazard_test>\n",
       " null_distribution = chi squared\n",
       "degrees_of_freedom = 1\n",
       "             model = <lifelines.CoxPHFitter: fitted with 1664 total observations, 1118 right-censored observations>\n",
       "         test_name = proportional_hazard_test\n",
       "\n",
       "---\n",
       "                              test_statistic      p  -log2(p)\n",
       "age                     km             11.69 <0.005     10.64\n",
       "                        rank           12.84 <0.005     11.52\n",
       "baseline_bmi            km              3.66   0.06      4.16\n",
       "                        rank            3.22   0.07      3.78\n",
       "baseline_weight_kg      km              2.81   0.09      3.41\n",
       "                        rank            2.43   0.12      3.07\n",
       "height_m                km              4.26   0.04      4.68\n",
       "                        rank            3.86   0.05      4.34\n",
       "quantity_control_likert km              0.32   0.57      0.81\n",
       "                        rank            0.41   0.52      0.94\n",
       "sex_f                   km             40.31 <0.005     32.10\n",
       "                        rank           43.57 <0.005     34.51"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Variable 'age' failed the non-proportional test: p-value is 0.0003.\n",
      "\n",
      "   Advice 1: the functional form of the variable 'age' might be incorrect. That is, there may be\n",
      "non-linear terms missing. The proportional hazard test used is very sensitive to incorrect\n",
      "functional forms. See documentation in link [D] below on how to specify a functional form.\n",
      "\n",
      "   Advice 2: try binning the variable 'age' using pd.cut, and then specify it in `strata=['age',\n",
      "...]` in the call in `.fit`. See documentation in link [B] below.\n",
      "\n",
      "   Advice 3: try adding an interaction term with your time variable. See documentation in link [C]\n",
      "below.\n",
      "\n",
      "\n",
      "2. Variable 'sex_f' failed the non-proportional test: p-value is <5e-05.\n",
      "\n",
      "   Advice: with so few unique values (only 2), you can include `strata=['sex_f', ...]` in the call\n",
      "in `.fit`. See documentation in link [E] below.\n",
      "\n",
      "3. Variable 'height_m' failed the non-proportional test: p-value is 0.0390.\n",
      "\n",
      "   Advice 1: the functional form of the variable 'height_m' might be incorrect. That is, there may\n",
      "be non-linear terms missing. The proportional hazard test used is very sensitive to incorrect\n",
      "functional forms. See documentation in link [D] below on how to specify a functional form.\n",
      "\n",
      "   Advice 2: try binning the variable 'height_m' using pd.cut, and then specify it in\n",
      "`strata=['height_m', ...]` in the call in `.fit`. See documentation in link [B] below.\n",
      "\n",
      "   Advice 3: try adding an interaction term with your time variable. See documentation in link [C]\n",
      "below.\n",
      "\n",
      "\n",
      "---\n",
      "[A]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html\n",
      "[B]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Bin-variable-and-stratify-on-it\n",
      "[C]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Introduce-time-varying-covariates\n",
      "[D]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Modify-the-functional-form\n",
      "[E]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Stratification\n",
      "\n",
      "    Model fitting and assumption check complete.\n",
      "\n",
      "--- Processing Scenario: 60d_10p_impulse_control_adjusted ---\n",
      "  Running Cox model for scenario: 60d_10p_impulse_control_adjusted\n",
      "    Duration: 60d_10p_impulse_control_adjusted_duration, Event: 10%_wl_achieved\n",
      "    Predictors: ['impulse_control_likert', 'age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi', 'height_m']\n",
      "    Data shape: (1664, 8)\n",
      "    Checking proportional hazards assumption...\n",
      "The ``p_value_threshold`` is set at 0.05. Even under the null hypothesis of no violations, some\n",
      "covariates will be below the threshold by chance. This is compounded when there are many covariates.\n",
      "Similarly, when there are lots of observations, even minor deviances from the proportional hazard\n",
      "assumption will be flagged.\n",
      "\n",
      "With that in mind, it's best to use a combination of statistical tests and visual tests to determine\n",
      "the most serious violations. Produce visual plots using ``check_assumptions(..., show_plots=True)``\n",
      "and looking for non-constant lines. See link [A] below for a full example.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>null_distribution</th>\n",
       "      <td>chi squared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>degrees_of_freedom</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>&lt;lifelines.CoxPHFitter: fitted with 1664 total...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_name</th>\n",
       "      <td>proportional_hazard_test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>test_statistic</th>\n",
       "      <th>p</th>\n",
       "      <th>-log2(p)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">age</th>\n",
       "      <th>km</th>\n",
       "      <td>12.44</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>11.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>13.52</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>12.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">baseline_bmi</th>\n",
       "      <th>km</th>\n",
       "      <td>3.38</td>\n",
       "      <td>0.07</td>\n",
       "      <td>3.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>2.91</td>\n",
       "      <td>0.09</td>\n",
       "      <td>3.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">baseline_weight_kg</th>\n",
       "      <th>km</th>\n",
       "      <td>2.59</td>\n",
       "      <td>0.11</td>\n",
       "      <td>3.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>2.19</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">height_m</th>\n",
       "      <th>km</th>\n",
       "      <td>3.96</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>3.52</td>\n",
       "      <td>0.06</td>\n",
       "      <td>4.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">impulse_control_likert</th>\n",
       "      <th>km</th>\n",
       "      <td>1.44</td>\n",
       "      <td>0.23</td>\n",
       "      <td>2.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>1.64</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">sex_f</th>\n",
       "      <th>km</th>\n",
       "      <td>40.88</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>32.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>44.23</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>34.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{tabular}{llrrr}\n",
       " &  & test_statistic & p & -log2(p) \\\\\n",
       "\\multirow[c]{2}{*}{age} & km & 12.44 & 0.00 & 11.22 \\\\\n",
       " & rank & 13.52 & 0.00 & 12.05 \\\\\n",
       "\\multirow[c]{2}{*}{baseline_bmi} & km & 3.38 & 0.07 & 3.92 \\\\\n",
       " & rank & 2.91 & 0.09 & 3.51 \\\\\n",
       "\\multirow[c]{2}{*}{baseline_weight_kg} & km & 2.59 & 0.11 & 3.22 \\\\\n",
       " & rank & 2.19 & 0.14 & 2.84 \\\\\n",
       "\\multirow[c]{2}{*}{height_m} & km & 3.96 & 0.05 & 4.43 \\\\\n",
       " & rank & 3.52 & 0.06 & 4.04 \\\\\n",
       "\\multirow[c]{2}{*}{impulse_control_likert} & km & 1.44 & 0.23 & 2.11 \\\\\n",
       " & rank & 1.64 & 0.20 & 2.32 \\\\\n",
       "\\multirow[c]{2}{*}{sex_f} & km & 40.88 & 0.00 & 32.53 \\\\\n",
       " & rank & 44.23 & 0.00 & 34.99 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "<lifelines.StatisticalResult: proportional_hazard_test>\n",
       " null_distribution = chi squared\n",
       "degrees_of_freedom = 1\n",
       "             model = <lifelines.CoxPHFitter: fitted with 1664 total observations, 1118 right-censored observations>\n",
       "         test_name = proportional_hazard_test\n",
       "\n",
       "---\n",
       "                             test_statistic      p  -log2(p)\n",
       "age                    km             12.44 <0.005     11.22\n",
       "                       rank           13.52 <0.005     12.05\n",
       "baseline_bmi           km              3.38   0.07      3.92\n",
       "                       rank            2.91   0.09      3.51\n",
       "baseline_weight_kg     km              2.59   0.11      3.22\n",
       "                       rank            2.19   0.14      2.84\n",
       "height_m               km              3.96   0.05      4.43\n",
       "                       rank            3.52   0.06      4.04\n",
       "impulse_control_likert km              1.44   0.23      2.11\n",
       "                       rank            1.64   0.20      2.32\n",
       "sex_f                  km             40.88 <0.005     32.53\n",
       "                       rank           44.23 <0.005     34.99"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Variable 'age' failed the non-proportional test: p-value is 0.0002.\n",
      "\n",
      "   Advice 1: the functional form of the variable 'age' might be incorrect. That is, there may be\n",
      "non-linear terms missing. The proportional hazard test used is very sensitive to incorrect\n",
      "functional forms. See documentation in link [D] below on how to specify a functional form.\n",
      "\n",
      "   Advice 2: try binning the variable 'age' using pd.cut, and then specify it in `strata=['age',\n",
      "...]` in the call in `.fit`. See documentation in link [B] below.\n",
      "\n",
      "   Advice 3: try adding an interaction term with your time variable. See documentation in link [C]\n",
      "below.\n",
      "\n",
      "\n",
      "2. Variable 'sex_f' failed the non-proportional test: p-value is <5e-05.\n",
      "\n",
      "   Advice: with so few unique values (only 2), you can include `strata=['sex_f', ...]` in the call\n",
      "in `.fit`. See documentation in link [E] below.\n",
      "\n",
      "3. Variable 'height_m' failed the non-proportional test: p-value is 0.0465.\n",
      "\n",
      "   Advice 1: the functional form of the variable 'height_m' might be incorrect. That is, there may\n",
      "be non-linear terms missing. The proportional hazard test used is very sensitive to incorrect\n",
      "functional forms. See documentation in link [D] below on how to specify a functional form.\n",
      "\n",
      "   Advice 2: try binning the variable 'height_m' using pd.cut, and then specify it in\n",
      "`strata=['height_m', ...]` in the call in `.fit`. See documentation in link [B] below.\n",
      "\n",
      "   Advice 3: try adding an interaction term with your time variable. See documentation in link [C]\n",
      "below.\n",
      "\n",
      "\n",
      "---\n",
      "[A]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html\n",
      "[B]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Bin-variable-and-stratify-on-it\n",
      "[C]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Introduce-time-varying-covariates\n",
      "[D]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Modify-the-functional-form\n",
      "[E]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Stratification\n",
      "\n",
      "    Model fitting and assumption check complete.\n",
      "\n",
      "--- Aggregating and Saving Results ---\n",
      "Saving aggregated results to table: cox_regression_results\n",
      "Results saved successfully.\n",
      "========== Cox Regression Pipeline Finished ==========\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# Cox Regression Pipeline Module\n",
    "####\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "# Ensure paper1_directory is defined in a previous cell\n",
    "# Example: paper1_directory = r\"C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\"\n",
    "if 'paper1_directory' not in locals() and 'paper1_directory' not in globals():\n",
    "    raise NameError(\"'paper1_directory' variable not defined. Please ensure it is defined in a previous cell.\")\n",
    "\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.exceptions import ConvergenceError\n",
    "import matplotlib.pyplot as plt # For saving plots\n",
    "import numpy as np\n",
    "\n",
    "# --- CONFIGURATION --\n",
    "\n",
    "# Adjust these paths as needed\n",
    "# --- MODIFIED: Use the paper1_directory VARIABLE ---\n",
    "DB_PATH = os.path.join(paper1_directory, 'survival_analysis.sqlite') # Path to the SQLite database\n",
    "ASSUMPTION_PLOT_DIR = os.path.join(paper1_directory, 'cox_assumption_plots')\n",
    "\n",
    "INPUT_TABLE_NAME = \"sa_input_table\" # Your wide input table\n",
    "OUTPUT_TABLE_NAME = \"cox_regression_results\" # Table to store summary results\n",
    "\n",
    "# Ensure the plot directory exists\n",
    "os.makedirs(ASSUMPTION_PLOT_DIR, exist_ok=True)\n",
    "\n",
    "# Define the scenarios to run\n",
    "# IMPORTANT: Ensure column names for duration/event/predictors/covariates match your sa_input_table EXACTLY.\n",
    "# IMPORTANT: Ensure categorical variables are numerically encoded (0/1) in sa_input_table.\n",
    "# --- MODIFIED: Updated variable names ---\n",
    "SCENARIOS = [\n",
    "     \n",
    "    # 60d10p, 6 VARS ONE BY ONE, UNADJUSTED\n",
    "    \n",
    "    {\n",
    "        'name': '60d_10p_hunger_unadjusted', # Descriptive name for the analysis run\n",
    "        'event_type': 'target',                # 'target' or 'dropout'\n",
    "        'target_perc': 10,                     # % WL target (used if event_type='target')\n",
    "        'time_window': 60,                     # Time window in days (used if event_type='dropout')\n",
    "        'predictors': ['hunger_yn'], # List of main predictors\n",
    "        'covariates': [] #'age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi', 'height_m' # List of adjustment covariates\n",
    "    },\n",
    "    {\n",
    "        'name': '60d_10p_satiety_unadjusted',\n",
    "        'event_type': 'target',\n",
    "        'target_perc': 10, \n",
    "        'time_window': 60,\n",
    "        'predictors': ['satiety_yn'],\n",
    "        'covariates': [] #'age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi', 'height_m'\n",
    "    },\n",
    "    {\n",
    "        'name': '60d_10p_emotional_eating_unadjusted',\n",
    "        'event_type': 'target',\n",
    "        'target_perc': 10,\n",
    "        'time_window': 60,\n",
    "        'predictors': ['emotional_eating_yn'],\n",
    "        'covariates': [] #'age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi', 'height_m'\n",
    "    },\n",
    "    {\n",
    "        'name': '60d_10p_emotional_eating_value_unadjusted',\n",
    "        'event_type': 'target',\n",
    "        'target_perc': 10,\n",
    "        'time_window': 60,\n",
    "        'predictors': ['emotional_eating_value_likert'],\n",
    "        'covariates': [] #'age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi', 'height_m'\n",
    "    },\n",
    "    {\n",
    "        'name': '60d_10p_quantity_control_unadjusted',\n",
    "        'event_type': 'target',\n",
    "        'target_perc': 10,\n",
    "        'time_window': 60,\n",
    "        'predictors': ['quantity_control_likert'],\n",
    "        'covariates': [] #'age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi', 'height_m'\n",
    "    },\n",
    "    {\n",
    "        'name': '60d_10p_impulse_control_unadjusted',\n",
    "        'event_type': 'target',\n",
    "        'target_perc': 10,\n",
    "        'time_window': 60,\n",
    "        'predictors': ['impulse_control_likert'],\n",
    "        'covariates': [] #'age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi', 'height_m'\n",
    "    },\n",
    "\n",
    "    # 60d10p, 6 VARS ONE BY ONE, ADJUSTED\n",
    "\n",
    "    {\n",
    "        'name': '60d_10p_hunger_adjusted', # Descriptive name for the analysis run\n",
    "        'event_type': 'target',                # 'target' or 'dropout'\n",
    "        'target_perc': 10,                     # % WL target (used if event_type='target')\n",
    "        'time_window': 60,                     # Time window in days (used if event_type='dropout')\n",
    "        'predictors': ['hunger_yn'], # List of main predictors\n",
    "        'covariates': ['age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi', 'height_m'] #'age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi', 'height_m' # List of adjustment covariates\n",
    "    },\n",
    "    {\n",
    "        'name': '60d_10p_satiety_adjusted',\n",
    "        'event_type': 'target',\n",
    "        'target_perc': 10, \n",
    "        'time_window': 60,\n",
    "        'predictors': ['satiety_yn'],\n",
    "        'covariates': ['age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi', 'height_m'] #'age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi', 'height_m'\n",
    "    },\n",
    "    {\n",
    "        'name': '60d_10p_emotional_eating_adjusted',\n",
    "        'event_type': 'target',\n",
    "        'target_perc': 10,\n",
    "        'time_window': 60,\n",
    "        'predictors': ['emotional_eating_yn'],\n",
    "        'covariates': ['age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi', 'height_m'] #'age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi', 'height_m'\n",
    "    },\n",
    "    {\n",
    "        'name': '60d_10p_emotional_eating_value_adjusted',\n",
    "        'event_type': 'target',\n",
    "        'target_perc': 10,\n",
    "        'time_window': 60,\n",
    "        'predictors': ['emotional_eating_value_likert'],\n",
    "        'covariates': ['age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi', 'height_m'] #'age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi', 'height_m'\n",
    "    },\n",
    "    {\n",
    "        'name': '60d_10p_quantity_control_adjusted',\n",
    "        'event_type': 'target',\n",
    "        'target_perc': 10,\n",
    "        'time_window': 60,\n",
    "        'predictors': ['quantity_control_likert'],\n",
    "        'covariates': ['age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi', 'height_m'] #'age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi', 'height_m'\n",
    "    },\n",
    "    {\n",
    "        'name': '60d_10p_impulse_control_adjusted',\n",
    "        'event_type': 'target',\n",
    "        'target_perc': 10,\n",
    "        'time_window': 60,\n",
    "        'predictors': ['impulse_control_likert'],\n",
    "        'covariates': ['age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi', 'height_m'] #'age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi', 'height_m'\n",
    "    },\n",
    "\n",
    "\n",
    "    # --- Add more scenarios as needed ---\n",
    "    # Example: Dropout model\n",
    "    # {\n",
    "    #     'name': '60d_dropout_emotional_eating_unadjusted',\n",
    "    #     'event_type': 'dropout',\n",
    "    #     'target_perc': None, # Not relevant for dropout event type\n",
    "    #     'time_window': 60,\n",
    "    #     'predictors': ['emotional_eating_yn'],\n",
    "    #     'covariates': [] #'age', 'sex_f', 'baseline_weight_kg', 'baseline_bmi', 'height_m' # No covariates for unadjusted model\n",
    "    # },\n",
    "]\n",
    "\n",
    "# --- HELPERS ---\n",
    "\n",
    "def load_data(db_path, table_name):\n",
    "    \"\"\"Loads data from the specified SQLite database table.\"\"\"\n",
    "    print(f\"Loading data from {db_path}, table {table_name}...\")\n",
    "    if not os.path.exists(db_path):\n",
    "        raise FileNotFoundError(f\"Database file not found at {db_path}\")\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n",
    "        print(f\"Loaded {len(df)} rows and {len(df.columns)} columns.\")\n",
    "        # --- NEW: Add check for empty DataFrame ---\n",
    "        if df.empty:\n",
    "             print(f\"Warning: Loaded DataFrame from {table_name} is empty.\")\n",
    "        return df\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"SQLite error loading data: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred loading data: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "def preprocess_data(df, scenarios):\n",
    "    \"\"\"Handles final data preparation specifically for Cox models.\"\"\"\n",
    "    print(\"Preprocessing data for Cox models...\")\n",
    "    if df.empty:\n",
    "         print(\"  Input DataFrame is empty. Skipping preprocessing.\")\n",
    "         return df # Return empty df\n",
    "\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # --- Data Type Conversion ---\n",
    "    # Identify all unique predictor/covariate columns needed across scenarios\n",
    "    all_model_cols = set()\n",
    "    for scenario in scenarios:\n",
    "        all_model_cols.update(scenario['predictors'])\n",
    "        all_model_cols.update(scenario['covariates'])\n",
    "\n",
    "    # --- REMOVED: Categorical map and conversion loop ---\n",
    "    # categorical_map = { ... }\n",
    "    # for col, mapping in categorical_map.items(): ...\n",
    "\n",
    "    # Ensure boolean columns used as events are 0/1 integers\n",
    "    event_cols_to_check = set()\n",
    "    for scenario in scenarios:\n",
    "         if scenario['event_type'] == 'target':\n",
    "              event_col = f\"{scenario['target_perc']}%_wl_achieved\"\n",
    "              event_cols_to_check.add(event_col)\n",
    "         elif scenario['event_type'] == 'dropout':\n",
    "              event_col = f\"{scenario['time_window']}d_dropout\"\n",
    "              event_cols_to_check.add(event_col)\n",
    "\n",
    "    for col in event_cols_to_check:\n",
    "         if col in df_processed.columns:\n",
    "              # --- MODIFIED: Check if column is NOT 0/1 already ---\n",
    "              if not df_processed[col].isin([0, 1, np.nan]).all():\n",
    "                   print(f\"  Warning: Event column '{col}' contains values other than 0, 1, or NaN. Attempting conversion.\")\n",
    "                   # Try converting boolean True/False to 1/0\n",
    "                   if df_processed[col].dtype == 'bool':\n",
    "                        print(f\"    Converting boolean event column '{col}' to 0/1 integer.\")\n",
    "                        df_processed[col] = df_processed[col].astype(int)\n",
    "                   else:\n",
    "                        # Attempt numeric conversion, coercing errors\n",
    "                        original_nan_count = df_processed[col].isnull().sum()\n",
    "                        df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')\n",
    "                        # Check if conversion failed or resulted in non 0/1 values\n",
    "                        if not df_processed[col].isin([0, 1, np.nan]).all():\n",
    "                             print(f\"    ERROR: Event column '{col}' could not be reliably converted to 0/1. Cox model may fail.\")\n",
    "                        elif df_processed[col].isnull().sum() > original_nan_count:\n",
    "                             print(f\"    Warning: Conversion of event column '{col}' introduced NaNs.\")\n",
    "\n",
    "              # --- Optional: Convert float 0.0/1.0 to integer ---\n",
    "              elif pd.api.types.is_float_dtype(df_processed[col]) and df_processed[col].dropna().isin([0.0, 1.0]).all():\n",
    "                   print(f\"  Converting float event column '{col}' (containing only 0.0/1.0) to integer.\")\n",
    "                   # Use nullable integer type Int64 to handle potential NaNs gracefully\n",
    "                   df_processed[col] = df_processed[col].astype('Int64')\n",
    "\n",
    "         else:\n",
    "              print(f\"  Warning: Event column '{col}' specified in scenarios but not found in DataFrame.\")\n",
    "\n",
    "\n",
    "    # Ensure duration columns are numeric\n",
    "    duration_cols_to_check = set()\n",
    "    for scenario in scenarios:\n",
    "         if scenario['event_type'] == 'target':\n",
    "              duration_col = f\"days_to_{scenario['target_perc']}%_wl\"\n",
    "              duration_cols_to_check.add(duration_col)\n",
    "         elif scenario['event_type'] == 'dropout':\n",
    "              # Assuming 'total_followup_days' is the primary duration for dropout\n",
    "              duration_cols_to_check.add('total_followup_days')\n",
    "              # Add specific window days if used, e.g., f'days_to_{scenario[\"time_window\"]}d_measurement'\n",
    "              # duration_cols_to_check.add(f'days_to_{scenario[\"time_window\"]}d_measurement')\n",
    "\n",
    "\n",
    "    for col in duration_cols_to_check:\n",
    "         if col in df_processed.columns:\n",
    "              if not pd.api.types.is_numeric_dtype(df_processed[col]):\n",
    "                   print(f\"  Attempting to convert duration column '{col}' to numeric.\")\n",
    "                   original_nan_count = df_processed[col].isnull().sum()\n",
    "                   df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')\n",
    "                   if df_processed[col].isnull().sum() > original_nan_count:\n",
    "                        print(f\"    Warning: Conversion of duration column '{col}' introduced NaNs.\")\n",
    "              # --- NEW: Check for negative durations ---\n",
    "              if pd.api.types.is_numeric_dtype(df_processed[col]) and (df_processed[col] < 0).any():\n",
    "                   print(f\"  Warning: Negative values detected in duration column '{col}'. These might cause issues.\")\n",
    "         else:\n",
    "              print(f\"  Warning: Duration column '{col}' specified in scenarios but not found in DataFrame.\")\n",
    "\n",
    "\n",
    "    # --- Missing Value Handling (Example: Simple Listwise Deletion per model later) ---\n",
    "    # No global handling here; will be done per model based on required columns.\n",
    "    # You could add imputation here if preferred, but listwise deletion per model is often safer initially.\n",
    "    print(\"Preprocessing complete.\")\n",
    "    return df_processed\n",
    "\n",
    "# --- COX REGRESSION MODULE ---\n",
    "\n",
    "def run_cox_model(df_subset, duration_col, event_col, predictor_cols, scenario_name, plot_dir):\n",
    "    \"\"\"\n",
    "    Fits a Cox model, checks assumptions, and extracts results.\n",
    "\n",
    "    Args:\n",
    "        df_subset (pd.DataFrame): DataFrame containing only necessary columns and no NaNs for the model.\n",
    "        duration_col (str): Name of the duration column.\n",
    "        event_col (str): Name of the event column (0/1).\n",
    "        predictor_cols (list): List of predictor and covariate column names.\n",
    "        scenario_name (str): Name of the scenario for labeling outputs.\n",
    "        plot_dir (str): Directory to save assumption plots.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing model summary, concordance, and assumption results.\n",
    "              Returns None if the model fails to converge or df_subset is too small.\n",
    "    \"\"\"\n",
    "    print(f\"  Running Cox model for scenario: {scenario_name}\")\n",
    "    print(f\"    Duration: {duration_col}, Event: {event_col}\")\n",
    "    print(f\"    Predictors: {predictor_cols}\")\n",
    "    print(f\"    Data shape: {df_subset.shape}\")\n",
    "\n",
    "    # --- MODIFIED: Stricter check for sufficient data relative to predictors ---\n",
    "    min_subjects_per_predictor = 10\n",
    "    required_subjects = min_subjects_per_predictor * len(predictor_cols) if predictor_cols else 10\n",
    "    if df_subset.shape[0] < max(10, required_subjects) or df_subset.shape[1] < 2:\n",
    "         print(f\"    Warning: Insufficient data ({df_subset.shape[0]} subjects) for {len(predictor_cols)} predictors. Need at least {max(10, required_subjects)}. Skipping model.\")\n",
    "         return None\n",
    "    if not predictor_cols:\n",
    "         print(\"    Warning: No predictors specified. Skipping model.\")\n",
    "         return None\n",
    "    # --- NEW: Check if event column has variance ---\n",
    "    if event_col in df_subset and df_subset[event_col].nunique() < 2:\n",
    "         print(f\"    Warning: Event column '{event_col}' has no variation (all 0s or all 1s). Skipping model.\")\n",
    "         return None\n",
    "    # --- NEW: Check if duration column has variance ---\n",
    "    if duration_col in df_subset and df_subset[duration_col].nunique() < 2:\n",
    "         print(f\"    Warning: Duration column '{duration_col}' has no variation. Skipping model.\")\n",
    "         return None\n",
    "\n",
    "\n",
    "    cph = CoxPHFitter()\n",
    "    results = {'scenario_name': scenario_name}\n",
    "\n",
    "    try:\n",
    "        # Use formula for cleaner specification if predictor_cols is not empty\n",
    "        # --- MODIFIED: Ensure backticks handle various column names ---\n",
    "        formula = \" + \".join([f\"`{str(col).replace('`', '')}`\" for col in predictor_cols])\n",
    "        cph.fit(df_subset, duration_col=duration_col, event_col=event_col, formula=formula)\n",
    "\n",
    "        # Extract Results\n",
    "        results['summary'] = cph.summary.reset_index().rename(columns={'covariate': 'Variable'}) # Get summary table\n",
    "        results['concordance'] = cph.concordance_index_\n",
    "        results['log_likelihood'] = cph.log_likelihood_\n",
    "        results['n_subjects'] = cph.durations.shape[0]\n",
    "        results['n_events'] = int(cph.event_observed.sum()) # Ensure integer\n",
    "\n",
    "\n",
    "        # Check Proportional Hazards Assumption\n",
    "        print(\"    Checking proportional hazards assumption...\")\n",
    "        try:\n",
    "            # Setting show_plots=False prevents plots from displaying in notebook during check\n",
    "            # --- MODIFIED: Pass dataframe subset used for fitting ---\n",
    "            assumption_results = cph.check_assumptions(df_subset[[duration_col, event_col] + predictor_cols], show_plots=False, p_value_threshold=0.05)\n",
    "            assumption_summary = {}\n",
    "            violated_vars = []\n",
    "            # --- MODIFIED: Safer iteration over assumption results ---\n",
    "            for result_tuple in assumption_results:\n",
    "                 if len(result_tuple) >= 4: # Ensure tuple has expected elements\n",
    "                     var = result_tuple[0]\n",
    "                     test_stat = result_tuple[1]\n",
    "                     p_val = result_tuple[2]\n",
    "                     is_proportional = result_tuple[3]\n",
    "                     assumption_summary[f\"{var}_ph_test_stat\"] = test_stat\n",
    "                     assumption_summary[f\"{var}_ph_p_value\"] = p_val\n",
    "                     assumption_summary[f\"{var}_is_proportional\"] = is_proportional\n",
    "                     if not is_proportional:\n",
    "                          violated_vars.append(var)\n",
    "                 else:\n",
    "                      print(f\"    Warning: Unexpected format in assumption results tuple: {result_tuple}\")\n",
    "\n",
    "\n",
    "            results['assumption_summary'] = assumption_summary\n",
    "            results['ph_assumption_violated'] = len(violated_vars) > 0\n",
    "            results['ph_violated_variables'] = \", \".join(violated_vars) if violated_vars else None\n",
    "\n",
    "            # Optionally, save Schoenfeld residual plots for violated variables\n",
    "            if violated_vars:\n",
    "                print(f\"    PH Assumption Violated for: {violated_vars}. Saving plots...\")\n",
    "                for var in violated_vars:\n",
    "                     try:\n",
    "                          fig, ax = plt.subplots(figsize=(8, 5))\n",
    "                          # --- MODIFIED: Use plot_partial_effects_on_outcome for better visualization ---\n",
    "                          # cph.plot_covariate_groups(var, values=np.percentile(df_subset[var].dropna(), [25, 75]), ax=ax) # Old method\n",
    "                          cph.plot_partial_effects_on_outcome(covariates=var, values=np.percentile(df_subset[var].dropna(), [10, 50, 90]), cmap='coolwarm', ax=ax)\n",
    "                          plot_filename = os.path.join(plot_dir, f\"{scenario_name}_{var}_partial_effects.png\") # Updated filename\n",
    "                          plt.tight_layout() # Adjust layout\n",
    "                          plt.savefig(plot_filename)\n",
    "                          plt.close(fig) # Close the figure to prevent display in notebook\n",
    "                          print(f\"      Saved plot: {plot_filename}\")\n",
    "                     except Exception as plot_err:\n",
    "                          print(f\"      Error generating/saving plot for {var}: {plot_err}\")\n",
    "\n",
    "\n",
    "        except ValueError as e_assump_val:\n",
    "             # Catch specific ValueError often related to singular matrix in PH test\n",
    "             print(f\"    Error during assumption check (likely data issue, e.g., low variance): {e_assump_val}\")\n",
    "             results['assumption_summary'] = {'error': f\"ValueError: {e_assump_val}\"}\n",
    "             results['ph_assumption_violated'] = None\n",
    "             results['ph_violated_variables'] = None\n",
    "        except Exception as e_assump:\n",
    "            print(f\"    Error during assumption check: {e_assump}\")\n",
    "            results['assumption_summary'] = {'error': str(e_assump)}\n",
    "            results['ph_assumption_violated'] = None\n",
    "            results['ph_violated_variables'] = None\n",
    "\n",
    "        print(\"    Model fitting and assumption check complete.\")\n",
    "\n",
    "    except ConvergenceError as e_conv:\n",
    "        print(f\"    ERROR: Cox model failed to converge for scenario {scenario_name}. Skipping. Error: {e_conv}\")\n",
    "        return None # Indicate failure\n",
    "    except Exception as e_fit:\n",
    "        print(f\"    ERROR: Failed to fit Cox model for scenario {scenario_name}. Error: {e_fit}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc()) # Uncomment for detailed traceback\n",
    "        return None # Indicate failure\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# --- ORCHESTRATION & EXECUTION ---\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate the Cox regression pipeline.\"\"\"\n",
    "    print(\"========== Starting Cox Regression Pipeline ==========\")\n",
    "    all_results_summary = [] # To store key summary stats from each model\n",
    "\n",
    "    try:\n",
    "        # 1. Load Data\n",
    "        df_raw = load_data(DB_PATH, INPUT_TABLE_NAME)\n",
    "        if df_raw.empty:\n",
    "             print(\"ERROR: Input data table is empty. Stopping pipeline.\")\n",
    "             return # Stop if no data loaded\n",
    "\n",
    "        # 2. Preprocess Data\n",
    "        df_analysis = preprocess_data(df_raw, SCENARIOS)\n",
    "        if df_analysis.empty:\n",
    "             print(\"ERROR: Data preprocessing resulted in an empty DataFrame. Stopping pipeline.\")\n",
    "             return # Stop if preprocessing failed\n",
    "\n",
    "        # 3. Loop through scenarios and run models\n",
    "        for scenario in SCENARIOS:\n",
    "            scenario_name = scenario['name']\n",
    "            print(f\"\\n--- Processing Scenario: {scenario_name} ---\")\n",
    "\n",
    "            # Determine duration and event columns based on event_type\n",
    "            duration_col_orig = None # Original duration (e.g., days_to_X%_wl)\n",
    "            event_col = None\n",
    "            if scenario['event_type'] == 'target':\n",
    "                if scenario['target_perc'] is None:\n",
    "                     print(\"  Error: 'target_perc' must be specified for event_type 'target'. Skipping.\")\n",
    "                     continue\n",
    "                duration_col_orig = f\"days_to_{scenario['target_perc']}%_wl\"\n",
    "                event_col = f\"{scenario['target_perc']}%_wl_achieved\"\n",
    "            elif scenario['event_type'] == 'dropout':\n",
    "                if scenario['time_window'] is None:\n",
    "                     print(\"  Error: 'time_window' must be specified for event_type 'dropout'. Skipping.\")\n",
    "                     continue\n",
    "                # Decide which duration column to use for dropout analysis\n",
    "                # Option 1: Use total follow-up time\n",
    "                duration_col_orig = 'total_followup_days'\n",
    "                # Option 2: Use time to a specific measurement (if available)\n",
    "                # duration_col_orig = f'days_to_{scenario[\"time_window\"]}d_measurement'\n",
    "                event_col = f\"{scenario['time_window']}d_dropout\"\n",
    "            else:\n",
    "                print(f\"  Error: Invalid event_type '{scenario['event_type']}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Define all columns needed for this specific model\n",
    "            predictor_cols = scenario['predictors'] + scenario['covariates']\n",
    "            # --- MODIFICATION START ---\n",
    "            # Define base columns needed BEFORE creating the specific duration\n",
    "            base_cols_needed = [event_col] + predictor_cols\n",
    "            if scenario['event_type'] == 'target':\n",
    "                base_cols_needed.append(duration_col_orig) # Need the time-to-event column\n",
    "                if 'total_followup_days' not in df_analysis.columns:\n",
    "                     print(\"  Error: 'total_followup_days' column is required for censoring in target events but not found. Skipping.\")\n",
    "                     continue\n",
    "                base_cols_needed.append('total_followup_days') # Need total followup for censoring\n",
    "            elif scenario['event_type'] == 'dropout':\n",
    "                 base_cols_needed.append(duration_col_orig) # Need the duration column for dropout (e.g., total_followup_days)\n",
    "\n",
    "            # Check if all necessary base columns exist\n",
    "            missing_cols = [col for col in base_cols_needed if col not in df_analysis.columns]\n",
    "            if missing_cols:\n",
    "                print(f\"  Error: Missing required columns for this scenario: {missing_cols}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Select initial subset with base columns\n",
    "            df_model_subset_initial = df_analysis[base_cols_needed].copy()\n",
    "\n",
    "            # Create the duration column specifically for the model\n",
    "            duration_for_model_col = f\"{scenario_name}_duration\" # Unique name for temp duration\n",
    "\n",
    "            if scenario['event_type'] == 'target':\n",
    "                 # Use time-to-event if event occurred (1), otherwise use total follow-up time\n",
    "                 df_model_subset_initial[duration_for_model_col] = np.where(\n",
    "                      df_model_subset_initial[event_col] == 1,\n",
    "                      df_model_subset_initial[duration_col_orig],       # Use days_to_X%_wl\n",
    "                      df_model_subset_initial['total_followup_days'] # Use total_followup_days\n",
    "                 )\n",
    "                 # Ensure the original duration_col (days_to_X%_wl) isn't NaN if event is 1\n",
    "                 check_event1_nan_duration = df_model_subset_initial[\n",
    "                     (df_model_subset_initial[event_col] == 1) &\n",
    "                     (df_model_subset_initial[duration_col_orig].isnull())\n",
    "                 ].shape[0]\n",
    "                 if check_event1_nan_duration > 0:\n",
    "                      print(f\"  Error: {check_event1_nan_duration} rows have event=1 but NaN in duration column '{duration_col_orig}'. Check data generation. Skipping.\")\n",
    "                      continue\n",
    "\n",
    "            elif scenario['event_type'] == 'dropout':\n",
    "                 # Duration is already set (e.g., total_followup_days)\n",
    "                 df_model_subset_initial[duration_for_model_col] = df_model_subset_initial[duration_col_orig]\n",
    "\n",
    "            # Now, define columns to keep for the final model subset\n",
    "            final_model_cols = [duration_for_model_col, event_col] + predictor_cols\n",
    "\n",
    "            # Drop rows ONLY if event or predictors are NaN (duration_for_model_col should be populated now)\n",
    "            cols_to_check_for_na = [event_col] + predictor_cols\n",
    "            # Also check the final duration column for NaNs, just in case\n",
    "            if duration_for_model_col not in cols_to_check_for_na:\n",
    "                 cols_to_check_for_na.append(duration_for_model_col)\n",
    "            df_model_subset = df_model_subset_initial[final_model_cols].dropna(subset=cols_to_check_for_na)\n",
    "\n",
    "            # --- MODIFICATION END ---\n",
    "\n",
    "\n",
    "            # Check if duration is always positive and event is 0/1 AFTER dropping NaNs\n",
    "            # --- MODIFIED: Check the new duration column and require > 0 ---\n",
    "            if duration_for_model_col in df_model_subset and (df_model_subset[duration_for_model_col] <= 0).any():\n",
    "                 # Cox model requires positive duration\n",
    "                 print(f\"  Warning: Non-positive values found in effective duration column '{duration_for_model_col}' after dropping NaNs. Removing these rows.\")\n",
    "                 original_rows = df_model_subset.shape[0]\n",
    "                 df_model_subset = df_model_subset[df_model_subset[duration_for_model_col] > 0]\n",
    "                 print(f\"    Removed {original_rows - df_model_subset.shape[0]} rows with non-positive duration.\")\n",
    "\n",
    "\n",
    "            if event_col in df_model_subset and not df_model_subset[event_col].isin([0, 1]).all():\n",
    "                 print(f\"  Error: Event column '{event_col}' contains values other than 0 or 1 after dropping NaNs. Skipping.\")\n",
    "                 continue\n",
    "\n",
    "\n",
    "            # Run the Cox model\n",
    "            # --- MODIFIED: Pass the new duration column name ---\n",
    "            model_output = run_cox_model(\n",
    "                df_model_subset,\n",
    "                duration_for_model_col, # Use the corrected duration column\n",
    "                event_col,\n",
    "                predictor_cols, # Pass the original list of predictors/covariates\n",
    "                scenario_name,\n",
    "                ASSUMPTION_PLOT_DIR\n",
    "            )\n",
    "\n",
    "            # Collect results if model ran successfully\n",
    "            if model_output and 'summary' in model_output:\n",
    "                # Extract key info from the model summary DataFrame for aggregation\n",
    "                summary_df = model_output['summary']\n",
    "                # --- MODIFIED: Safer extraction using .get() with defaults ---\n",
    "                for _, row in summary_df.iterrows():\n",
    "                    variable_name = row.get('Variable', 'UNKNOWN')\n",
    "                    all_results_summary.append({\n",
    "                        'scenario_name': scenario_name,\n",
    "                        'variable': variable_name,\n",
    "                        'hazard_ratio': row.get('exp(coef)'),\n",
    "                        'ci_lower': row.get('exp(coef) lower 95%'),\n",
    "                        'ci_upper': row.get('exp(coef) upper 95%'),\n",
    "                        'p_value': row.get('p'),\n",
    "                        'z_score': row.get('z'), # Added z-score\n",
    "                        # 'log_likelihood_ratio_p': row.get('log(p)'), # This is usually the p-value for the variable, not LRT\n",
    "                        'concordance': model_output.get('concordance'),\n",
    "                        'n_subjects': model_output.get('n_subjects'),\n",
    "                        'n_events': model_output.get('n_events'),\n",
    "                        'log_likelihood': model_output.get('log_likelihood'), # Overall model log-likelihood\n",
    "                        'ph_assumption_violated': model_output.get('ph_assumption_violated'),\n",
    "                        'ph_violated_variables': model_output.get('ph_violated_variables'),\n",
    "                        # Add specific assumption p-values if needed\n",
    "                        f\"{variable_name}_ph_p_value\": model_output.get('assumption_summary', {}).get(f\"{variable_name}_ph_p_value\")\n",
    "                    })\n",
    "            else:\n",
    "                 print(f\"  Scenario {scenario_name} did not produce valid results.\")\n",
    "\n",
    "\n",
    "        # 4. Save Aggregated Results\n",
    "        if all_results_summary:\n",
    "            print(\"\\n--- Aggregating and Saving Results ---\")\n",
    "            results_df = pd.DataFrame(all_results_summary)\n",
    "\n",
    "            conn_out = None\n",
    "            try:\n",
    "                conn_out = sqlite3.connect(DB_PATH) # Connect to the main DB to save results\n",
    "                print(f\"Saving aggregated results to table: {OUTPUT_TABLE_NAME}\")\n",
    "                results_df.to_sql(OUTPUT_TABLE_NAME, conn_out, if_exists='replace', index=False)\n",
    "                conn_out.commit()\n",
    "                print(\"Results saved successfully.\")\n",
    "            except sqlite3.Error as e:\n",
    "                print(f\"SQLite error saving results: {e}\")\n",
    "                if conn_out: conn_out.rollback()\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred saving results: {e}\")\n",
    "                if conn_out: conn_out.rollback()\n",
    "            finally:\n",
    "                if conn_out:\n",
    "                    conn_out.close()\n",
    "        else:\n",
    "            print(\"\\nNo valid model results were generated to save.\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"ERROR: Database error during setup - {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: An unexpected error occurred in the main pipeline - {e}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "    finally:\n",
    "        print(\"========== Cox Regression Pipeline Finished ==========\")\n",
    "\n",
    "# --- Execute the main function ---\n",
    "# Ensure this cell is run within the notebook context where 'paper1_directory' is defined\n",
    "if __name__ == \"__main__\":\n",
    "     # Check again if running as script vs notebook cell\n",
    "     if 'paper1_directory' not in locals() and 'paper1_directory' not in globals():\n",
    "          print(\"ERROR: 'paper1_directory' variable not defined. Cannot run main().\")\n",
    "          # Or define a default path if running as a standalone script\n",
    "          # paper1_directory = '.' # Example: Use current directory\n",
    "     else:\n",
    "          main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique observations in the weight_gain_cause column:\n",
      "comer mal, estress\n",
      "EMBARAZO \n",
      "Ansiedad y estrés \n",
      "PICOTEA EN EL RESTAURANTE SOLO HACE 1 COMIDA AL DIA. \n",
      "Embarazo, falta motivación...\n",
      "NO PODER MOVERSE POR DOLOR LUMBARES\n",
      "ANSIEDAD POR PROBLEMAS LABORALES\n",
      "DEPRESION\n",
      "ROTURA DE MENISCO, DETUBO EJERCICIO Y NO SE HA SABIDO CONTROLAR + MUERTE DE SU PADRE\n",
      "Estrés en el trabajo y horarios cambiantes \n",
      "EDAD, MENOS ACTIVIDAD FISICA\n",
      "No se cuida con cantidades. \n",
      "Estrés y poco ejercicio\n",
      "QUARENTENA\n",
      "malos hábitos alimentarios\n",
      "COMER MAL\n",
      "PROBLEMAS PERSONALES, DESORDEN\n",
      "ANSIEDAD, MURIO SU MADRE HACE UN AÑO\n",
      "menopausia \n",
      "MUCHAS ANSIEDAD POR TEMAS PERSONALES\n",
      "EMBARAZOS\n",
      "TRABAJA EN ESTACION DE SKI\n",
      "PICOTEO\n",
      "DEPOIS DA PANDEMIA COMEÇOU A AUMENTAR PESO GRADUALMENTE. SEDENTARISMO.\n",
      "JÁ FEZ LEV, DIETA 3 PASSOS, HERBALIFE, SEM RESULTADOS A MEDIO-LONGO PRAZO.\n",
      "ESTAVA ACOMPANHADA (TRABALHO), ACHEI QUE NÃO ESTAVA CONFORTÁVEL NA PORMENORIZAÇÃO DESTE TEMA E NÃO O EXPLOREI MAIS.\n",
      "desorden horario, come mas por las noches durante el dia se restringe\n",
      "ESTA DE BAJA\n",
      "NO HACE MANTENIMIENTO\n",
      "CON LOS PROBLEMAS, ES INCAPAZ DE SEGUIR LA DIETA Y ACABA DESCONTROLANDO\n",
      "Tubo problemas de depresión en el pasado, ahora esta más estable y animada.\n",
      "DEJO DE FUMAR Y COGIÓ PESO DES E ENTONCES.\n",
      "Problemas familiares y mucha ansiedad\n",
      "DEIXOU DE TREINAR\n",
      "MUITO TRABALHO E REALMENTE EM EXCESSO. DEPOIS AO FINAL DA NOITE SÓ LHE APETECE \"PORCARIA\" E COME UM \"MIMINHO\"\n",
      "Ansiedad\n",
      "desajustes hormonales\n",
      "No cuidarse, la vida social \n",
      "PANDEMIA\n",
      "picotea\n",
      "volver a los hábitos anteriores, picoteo y festividades\n",
      "MUCHO REPOSO POR PROBLEMA CON OPERACION DE CATARATAS, LESION EN LA ESPALDA\n",
      "VERANO\n",
      "MENOPAUSIA\n",
      "Influyen emociones y dificultad física. \n",
      "Pandemia no se ha cuidado \n",
      "COMER MUCHO, MUCHOS ACTOS SOCIALES\n",
      "PROGRESIVO\n",
      "COMENTA QUE SE SALE DE LA RUTINA\n",
      "OPERACION UTERO DE URGENCIA MÁS TTO HORMONAL - EMBARAZO\n",
      "MUCHA ANSIEDAD POR CORTICOIDES\n",
      "SEDENTARISMO, DEJA DEPORTE Y MALOS HABITOS \n",
      "menopausia + toma vino habitualmente\n",
      "le cuesta el mantenimiento, vuelve a habitos anteriores\n",
      "le quitaron el útero\n",
      "malos hábitos\n",
      "HIZO PRONOKAL PARA TTO DE FERTILIDAD , Y NO HA HECHO MANTENIMIENTO\n",
      "ABRIU UM CAFÉ BIOLOGICO COM BOLOS E ASSIM E METEU-SE A COMER AQUELAS GOSTOSURAS\n",
      "Pandemia\n",
      "DICE QUE LLEVA TODO LA VIDA A DIETA\n",
      "OBJ: SALUD\n",
      "PANDEMIA, STRESS, SEDENTARISMO\n",
      "Cantidades\n",
      "Malos hábitos.\n",
      "Ansiedad sobre todo por las tardes.\n",
      "No desayuno ni media mañana. \n",
      "UN POCO TODO\n",
      "DESCONTROL GENERAL Y MUCHOS FACTORES\n",
      "picoteo por las tardes frutos secos, queso, patatas fritas - desorden de comidas por picoteo\n",
      "ESTILO DE VIDA, MALOS HÁBITOS, DIVORCIO + CAMBIO RESIDENCIA\n",
      "Antidepresivos \n",
      "PICOTEA\n",
      "come por ansiedad, come muy rápido\n",
      "ANSIEDAD\n",
      "CAUSAS PERSONALES Y DE TRABAJO \n",
      "22kg, pandemia,  ha tenido mucha ansiedad. \n",
      "MALOS HABITOS ALIMENTARIOS. MUCHO CAFE, POCAS COMIDAS, EN LA HORA DE LA MERIENDA HACE ATRACONES (ANSIEDAD)\n",
      "ELLA QUIERE LLEGAR A 54KG\n",
      "ACCIDENTE EN OCTUBRE Y HA ESTADO DE REPOSO\n",
      "EMBARAZO\n",
      "AUMENTO PROGRESIVO, DESORGANIZACION\n",
      "MALOS HÁBITOS, HORARIOS\n",
      "COMPROMISOS\n",
      "MENOPAUSIA\n",
      "HACE 2 AÑOS SE DESMADRÓ - MUCHO STRESS \n",
      "ANSIEDAD, PANDEMIA, MUERTE DE SU PADRE\n",
      "DESCONTROL, ANSIEDAD CON LA COMIDA \n",
      "Depresión\n",
      "NO PUEDE HACER AF, AHORA HA RETOMADO (HACE 2 MESES)\n",
      "SEDENTARISMO \n",
      "MUCHA ANSIEDAD\n",
      "SE MANTUVO BIEN HASTA EL 2021\n",
      "MUCHO ESTRES EN EL TRABAJO\n",
      "Menopausia\n",
      "PORQUE CUANDO SE DEJA, NO SABE PARAR. \n",
      "PICOTEO POR LAS TARDES, ESPEOR MOMENTO\n",
      "trabajo sedentario, comer fuera\n",
      "MUCHO DESORDEN, LA NOCHES PICOTEA MUCHO\n",
      "NO MANTENIMIENTO, DESORDEN ABSOLUTO, MALOS HABITOS\n",
      "INSOMNIO - LE PROVOCA ATRACONES NOCTURNOS\n",
      "ansiedad por picoteo por todo - toma ansiolíticos\n",
      "ESTRES POR TRABAJO, ANSIEDAD POR COMER DULCE O MALCOMER\n",
      "COMER MAL\n",
      "ADICTA A LA COCA ZERO\n",
      "REDUCE EJERCICIO POR OPERACIONES DE RODILLA Y CAIDAS \n",
      "CONFINAMIENTO Y TRABAJA EN LA SANIDAD PROBLEMAS DE ANSIEDAD. \n",
      "Dejadez y tendencia a engordar + menopausia\n",
      "EMBARAZO, ESTRES \n",
      "MENOPAUSIA\n",
      "ANSIEDAD, NO EJERCICIO\n",
      "Estrés en el trabajo...\n",
      "hija con alteraciones cromosómicas y le genera mucha ansiedad\n",
      "DESORDEN AL COMER POR PROBLEMAS\n",
      "ACHA QUE É EMOCIONAL. TEM TIDO UNS PROBLEMAS PRIVADOS (PAI FALECEU E SEPAROU-SE) E EMPRESARIAL (EMPRESA NÃO VAI BEM) E DEPOIS DA GRAVIDEZ NOTA QUE MUDOU MUITO\n",
      "embarazos pandemia\n",
      "sedentarismo, no cuidar la alimentacion\n",
      "COMER MÁS\n",
      "HACE 5 AÑOS MURIÓ SU MARIDO Y COGIÓ PESO\n",
      "FIBROMIALGIA\n",
      "MENOPAUSIA\n",
      "HIPOTIROIDISMO\n",
      "por la noche tiene mas ansiedad\n",
      "CUANDO MAS HAMBRE TIENE ES POR LA MAÑANA\n",
      "Vuelta a los antiguos hábitos\n",
      "Ansiedad, ovarios poliquísticos\n",
      "HA IDO COGIENDO POCO A POCO DESDE OCTUBRE, PORQUE SE FUE OLVIDANDO DE LAS PAUTAS. CENAS FUERA DE CASA, CELEBRACIONES.\n",
      "ENTROU EM DEPRESSÃO EM 2021 E COMEÇOU A AUMENTAR PESO GRADUALMENTE DEVIDO A COMPULSÃO ALIMENTAR. ATUALMENTE JÁ NÃO TRABALHA POR TURNOS (HORÁRIO 10-19H00).\n",
      "NO HACER MANTENIMIENTO, FALTA DE TIEMPO \n",
      "Cx espalda - mucho reposo y medicación y corticoides, muchas migrañas - 1 año sin regla (menopausia)\n",
      "LE GUSTAN MUCHO LOS DULCES, SI EMPIEZA A TOMARLO SE DESCONTROLA\n",
      "FEZ UMA ROTURA DE LIGAMENTOS GRAVE E FOI OPERADA E FICOU ENTÃO 1 ANO PARADA SEM QUALQUER ATIVIDADE FISICA\n",
      "MARIDO É PRESIDENTE DA CAMARA E TEM DE IR JANTAR A MUITAS FESTAS E ASSIM E SÃO SÓ COISAS BOAS : AÇORDA , DOCES , ETC . MUITO PÃO. NESTA ALTURA TAMBÉM PARTOU AS CAMINHADAS O QUE NÃO AJUDOU TAMBÉM.\n",
      "Estres en el trabajo\n",
      "REFERE QUE ATÉ MANTEVE O PESO MAS DEPOIS DESCONTROLOU-SE COM OS FILHOS SAIREM DE CASA PARA IR PARA FACULDADE E COM FÉRIAS E MUDANÇA DE TRABALHO\n",
      "DURANTE ALGUM TEMPO CONSEGUIU MANTER A ALIMENTAÇÃO MAS DEPOIS TEVE MUITAS SITUAÇÕES DE STRESS QUE LHE DEU PARA COMER\n",
      "NO MANTENIMIENTO, CAMBIOS EN LA VIDA, DESORDEN\n",
      "NO MANTENIMIENTO, COMIDA RAPIDA, STRESS LE PROVOCA PICAR DULCE\n",
      "Deja de fumar \n",
      "NO HIZO MANTENIMIENTO\n",
      "SIEMPRE ANSIEDAD,LE DA POR COMER, SE DESPIERTA POR LA NOCHE PARA PICAR\n",
      "REFERE QUE HÁ 2 MESES ESTAGNOU NO PESO E QUE DEIXOU DE EMAGRECER\n",
      "La hace o perfecto o nada en lo absoluto. \n",
      "\n",
      "ANSIEDAD, DESCONTROL COMIDAS, DORMIR POCO\n",
      "picoteo\n",
      "NO HA HECHO MANTENIMIENTO, POR OPERACION MUCHO REPOSO\n",
      "DESAJUSTE HORARIO\n",
      "menopausia\n",
      "TIROIDES\n",
      "NO MANTENIMIENTO\n",
      "REFERE QUE O NATAL E A CABEÇA NÃO FORAM SEUS AMIGOS\n",
      "DIVORCIO\n",
      "TIENE ATRACONES CUANDO ESTA ESTRESADA, ES POR TEMPORADAS\n",
      "ansiedad por muerte de familiar\n",
      "siempre ha tenido tendencia a subir de peso\n",
      "post embarazo después de 2 niños, diabetes gestacional \n",
      "Cambio de vida a vida sedentaria\n",
      "TELETRABAJO- SEDENTARISMO- COME MAS-PICOTEO- NO DEPORTE\n",
      "NO CAMBIO DE HABITOS, COMER LO Q LE GUSTA\n",
      "COME MUCHO EL FIN DE SEMANA\n",
      "SEDENTARISMO\n",
      "DESCONTROL DE LAS COMIDAS\n",
      "PICOTEO\n",
      "DEJADEZ, SEDENTARISMO, CAMBIOS HORMONALES, ANSIEDAD, DEPRESION\n",
      "embarazos, menopausia\n",
      "EDAD\n",
      "SEDENTARISMO\n",
      "MENOPAUSIA\n",
      "STRESS\n",
      "LA HIZO HACE UNOS 12 AÑOS\n",
      "EN POCO TIEMPO\n",
      "INACTIVIDAD\n",
      "Menopausia\n",
      "Malos habitos alimentarios\n",
      "Zaro actividad fisica\n",
      "MALA ALIMENTACION\n",
      "No hay cambios en su vida no hay motivos \n",
      "Menopausia, sedentaria\n",
      "SIEMPRE DIETA RESTRICTIVA PERO DESDE EL VERANO HASTA AHORA HA COMIDO SIN TANTO CONTROL\n",
      "MENOPAUSIA\n",
      "EDAD\n",
      "MENOPAUSIA, ESTRES EN CASA \n",
      "COME 1 VEZ AL DIA\n",
      "TODO UN POCO\n",
      "LE  GUSTA COMER\n",
      "OBJETIVO: TALLA 42-44\n",
      "SIEMPRE HA COMIDO MUCHO\n",
      "NUNCA HA SIDO DELGADA\n",
      "PREMENOPAUSIA, DEJAR DE FUMAR , CAMBIO DE TURNO\n",
      "ESTRÉS, ANSIEDAD POR EL TRABAJO. HA PASADO A SER PROFE DE GUARDERIA A PROFE DE UNI\n",
      "SEDENTARIA DESDE HACE 4 AÑOS\n",
      "Menopausia y ansiedad\n",
      "\n",
      "cambio en el estilo de vida \n",
      "PANDEMIA UNICAMENTE, UN POCO MÁS DE ANSIEDAD\n",
      "PROBLEMAS FAMILIARES: DETECTARON UN CÁNCER A SU HIJO Y YA TODO SE FUE AL TRASTE COMENTA\n",
      "menopausia, malos habitos\n",
      "MENOPAUSIA PRECOZ, TRATAMIENTO DE FERTILIDAD\n",
      "ANSIEDAD, DESORGANIZADA EN SUS COMIDAS\n",
      "vida social muy ajetreada\n",
      "FRACTURA DE L1.\n",
      "Confinamiento, \n",
      "embarazo y malos  hábitos\n",
      "Comer mal\n",
      "Dos comidas al día solamente\n",
      "mucho estrés\n",
      "mala alimentación\n",
      "embarazo,come mucho  productos con azúcar\n",
      "ansiedad por comer , come mucha cantidad\n",
      "Pastillas para quedar embarazada\n",
      "HACE 7 AÑOS LE SOBRABAN 10KG, Y DESPUES POCO A POCO HA IDO PONIENDO PESO\n",
      "COVID, ANSEIDAD Y FALTA DE MOVILIDAD\n",
      "ENFERMEDAD DE SU MADRE, PANDEMIA\n",
      "ESTRÉS, PANDEMIA, MUERTE PADRE\n",
      "Malos hábitos alimentarios\n",
      "EMBARAZOS,COME MUCHO EN RESTAURANTES POR TRABAJO\n",
      "Difícil de controlar algunas noches a nivel emocional. \n",
      "EMBARAZO, REDUCE ACTIVIDAD\n",
      "DESPUES DE CONFINAMIENTO, CAMBIO DE HABITOS, DEJÓ DE TRABAJAR\n",
      "Ansiedad + Dejar de fumar\n",
      "Sedentarismo\n",
      "Confinamiento\n",
      "Sendentarismo a raiz de la pandemia + lesión\n",
      "VIDA SOCIAL\n",
      "DEJAR DE FUMAR HACE 2.5 AÑOS\n",
      "TROCANTERITIS - REPOSO, MENOPAUSIA\n",
      "EDAD, TRABAJO SEDENTARIO (12H SENTADA)\n",
      "POR CANSANCIO NO COME\n",
      "COME FUERA Y VIAJA MUCHO POR TRABAJO\n",
      "2 perdidas de embarazo y le cuesta perder peso\n",
      "ESTEVE DE FÉRIAS E ATÉ FICOU COM O MESMO PESO QUE TINHA ANTES DE IR DE FÉRIAS MAS A DRA. RECOMENDOU FAZER 1 SEMANA DE P1 E DEPOIS PASSA AO P2 PARA VER SE PERDE MAIS UM POUCO\n",
      "EMBARAZOS, CONFINAMIENTO\n",
      "picar entre horas, vida social, poca AF\n",
      "Menopausia y exceso de pan. Sedentarismo.\n",
      "HACE 2 TOMAS AL DIA - HORARIOS DIVERSOS - AZAFATAS VUELO\n",
      "Con el tiempo ha ido cogiendo peso por sedentarismo + malos hábitos alimentarios\n",
      "LESIÓN \n",
      "endometriosis - tratamiento hormonalque le ha dado mucha ansiedad\n",
      "TENDENCIA NATURAL\n",
      "FOME EMOCIONAL\n",
      "A SEGUIR TER FICADO GRÁVIDA. GANHOU ESTE PESO PÓS GRAVIDEZ. DEIXA DE COMER QUANDO ESTÁ MAIS STRESSADA E ASSIM. \n",
      "EMBARAZO, ABORTO Y OTRAS COSAS QUE LE INFLUYEN EN EL PESO. NO TIENE HAMBRE PERO COME MAL. \n",
      "Le gusta mucho el dulce y comer\n",
      "Malos hábitos\n",
      "BALON GASTRICO\n",
      "SE ROMPIO EL PERONE Y REPOSO UNOS MESES\n",
      "Ser madre\n",
      "la vez anterior llego a 91 kg, pero por estres, come por ansiedad y por eso subio peso\n",
      "STRESS, ANSIEDAD, TRABAJAR FUERA DE CASA\n",
      "Deja de fumar y está lesionado\n",
      "A raiz de la pandemia + ansiedad\n",
      "PESO MINIMO EM IDADE ADULTA 58KG (HÁ CERCA DE 26 ANOS)\n",
      "PESO MAX EM IDADE ADULTA = 86KG (HÁ CERCA DE 5 ANOS QUANDO DEIXOU DE FUMAR)\n",
      "PESO HABITUAL RONDA OS 70KG NO ÚLTIMO ANO.\n",
      "DIZ-ME QUE TODA A VIDA VIVEU EM DIETA, MAS POR VEZES ENTRA EM DESIQUILIBRIO ALIMENTAR E É QUANDO AUMENTA DE PESO.\n",
      "STODA LA VIDA CON SOBREPESO, COMER\n",
      "Mala alimentación\n",
      "Embarazo hace 10 años\n",
      "+ Sedentarismo por problemas fisicos\n",
      "+ Se ha puesto el DIU\n",
      "CAMBIO DE TRABAJO, PANDEMIA. COME POR ABURRIMIENTO. COME DESORDENADO\n",
      "PICOTEA MUCHO, MALA ALIMENTACION\n",
      "subida de peso por problemas familiares.\n",
      "DIZ QUE TEM GORDURA CONCENTRADA NO ABDOMEN E COSTAS.\n",
      "MÃE DE 2 FILHOS (3 E 5 ANOS DE IDADE), POUCO DESCANSO (PRIVAÇÃO DE SONO). TAMBÉM ME DIZ QUE SOFREU ABORTO EM AGOSTO/2021 E EMOCIONALMENTE TAMBÉM DESCOMPENSOU NA ALIMENTAÇÃO. DIZ QUE NÃO COME \"MAL\", MAS PECA NAS QUANTIDADES.\n",
      "HABITOS ALIMENTARES\n",
      "FAZIA MUITO DESPORTO E QUANDO FOI PARA A FACULDADE DEIXOU DE FAZER DESPORTO\n",
      "PROBLEMAS FISICOS. FOI OPERADA A UM PE E COMEÇOU A GANHAR PESO. JÁ FOI OPERADA 4 OU 5 X. FOI TAMBÉM OPERADA A UMA HERNIA ABDOMINAL.\n",
      "TEM METABOLISMO LENTO\n",
      "ALIMENTAÇÃO + SEDENTARISMO + EMOCIONAL\n",
      "FEZ TTO DE FERTILIZAÇÃO IN VITRO - DESCONTAVA AQUI NA COMIDA E A NIVEL EMOCIONAL FOI COMPLICADO\n",
      "EMBARAZO Y PANDEMIA (ESTRES)\n",
      "ESTRES EN EL TRABAJO, CAMBIOS HORMONALES\n",
      "SE SALTA COMIDAS. TUBO UN ABORTO MOTIVO PRINCIPAL POR EL  QUE EMPEZÓ A GANAR PESO. \n",
      "MENOPAUSA\n",
      "HA IDO AUMENTANDO PROGRESIVAMENTE DE MANERA ANUAL. VIAJA MUCHO. LLEGA A LA CENA CON  MUCHA ANSIEDAD\n",
      "PREMENOPAUSIA - DESAJUSTE HORMONAL CON SOFOCOS\n",
      "Toda la vida ha ido subiendo y bajando\n",
      "Dejar de fumar\n",
      "AUMENTO DE PESO A RAIZ DEL EMBARAZO\n",
      "MALOS HABITOS \n",
      "MATERNIDAD FALTA DE CUIDADOS DESPUES. \n",
      "OBJ PASO 1 15KG\n",
      "TRABAJO NUEVO MUY ESTRESANTE, DE MUCHO DEPORTE A SEDENTARISMO\n",
      "SE DESRAGULA CON LA ALIMENTACION. NO HA COMIDO BIEN NI HA HECHO EJERCICIO. \n",
      "HA DEJADO DE HACER DEPORTE POR LO QUE HA SUBIDO DE PESO\n",
      "MALA ALIMENTACION, MUERTE DE FAMILIARES \n",
      "A PARTIR DE COVID GANÓ PASO Y A PERDIDO DISCIPLINA\n",
      "HA AUMENTADO DE PESO POR SEDENTARISMO, MENOPAUSIA...\n",
      "ENGORDOU HÁ CERCA DE 5 ANOS. DEIXOU DE FUMAR. ENTRETANTO CASOU E QUERIA MOSTRAR OS DOTES ALIMENTARES AO MARIDO. REFERE QUE O IRMÃO APARECEU MORTO E ISSO ABALOU-A MUITO. REFERE QUE PASSOU MUITA FOME QUANDO ERA PEQUENA E AGORA NOTA QUE NÃO TEM CONTROLO NA COMIDA PORQUE ENQUANTO TIVER COMIDA COME\n",
      "SE HA DESCUIDADO Y HA SUBIDO 15Kg APROX. \n",
      "MALOS HÁBITOS ALIMENTICIOS (dulce) Y COME MUY RAPIDO\n",
      "DESIQUILIBRO ALIMENTAR \n",
      "FALTA DE ROTINAS \n",
      "PICOTEA MUCHO. EL FIN DE SEMANA ES CUANDO MEJOR LO HACES (3 COMIDAS/DÍA)\n",
      "LUXACION HOMBRO\n",
      "PROGRESIVO, IR DEJANDOSE CON EL PASO DEL TIEMPO\n",
      "Hace años estaba obesa, hizo Naturhouse y perdio mucho peso pero ahora, desde hace un año, ha empezado a subir y aunque coma bien, no para de subir\n",
      "PREMENOPAUSIA\n",
      "LESION\n",
      "Dejar de fumar\n",
      "MENOPAUSIA \n",
      "SAUDÁVEL.\n",
      "PESO HABITUAL RONDA OS 89-90KG. NOS ULTIMOS 6 MESES AUMENTOU CERCA DE 10KG. DIZ-ME TER PESO IO-IO. FAZ MUITAS DIETAS MAS PERDE-SE PELO MEIO. DEPOIS DE FAZER PRONOKAL TEVE 2 FILHOS E NUNCA MAIS RECUPEROU O PESO.\n",
      "MUDANÇA DE CIDADE RECENTE - CASTELO BRANCO. TEM AO SEU CUIDADO OS 2 FILHOS DE 4 E 6 ANOS, O SEU MARIDO AINDA NÃO CONSEGUIU TRANSFERENCIA DE TRABALHO E POR ISSO ESTÁ MAIS SOBRECARREGADA EM CASA. DESIQUILIBRIO ALIMENTAR. COMPENSAÇÃO EMOCIONAL NA COMIDA.\n",
      "\n",
      "Deja de fumar\n",
      "Ansiedad\n",
      "COME MAS LOS ULTIMOS AÑOS \n",
      "HIPOTIROIDISMO\n",
      "DEIXOU DE JOGAR FUTEBOL E TORNOU-SE MUITO SEDENTÁRIO.\n",
      "PARA ALÉM DISTO DEPOIS DE JOGAR FUTEBOL FICOU COM UMA DEPRESSÃO NERVOSA E TOMOU MEDICAÇÃO\n",
      "ANSIEDAD Y EMBARAZOS \n",
      "TODA LA VIDA, EXFUMADORA \n",
      "2 GRAVIDEZES EM QUE AUMENTOU DE PESO ATÉ AOS 90KG E DEIXOU DE FUMAR. NA 1º GRAVIDEZ ATÉ CONSEGUIU VOLTAR QUASE AO SEU PESO MAS NA SEGUNDA GRAVIDEZ JUNTAMENTE COM O TER DEIXADO DE FUMAR NÃO CONSEGUIU \n",
      "A CAUSA DEL TRABAJO, COORDINADORA DE MERCADONA\n",
      "SE HA DEJADO Y DESCUIDADO, DEJÓ DE FUMAR HACE UN PAR DE AÑOS\n",
      "CONFINAMENTO E COM A IDADE NOTA QUE TEM MAIS DIFICULDADE EM EMAGRECER. NO FINAL DIZ QUE TAMBÉM É DO COMODISMO\n",
      "HACE 15 AÑOS HIZO LA DIETA\n",
      "FIN DE SEMANA - VIDA SOCIAL\n",
      "HIPOTIROIDES ANTES EL EMBARAZO PERO AHORA BIEN\n",
      "ha estado haciendo erasmus en italia. \n",
      "3 EMBARAZOS \n",
      "AUMENTO PREVIO EMBARAZO + POST EMBARAZO\n",
      "Por el verano\n",
      "Le gusta comer mucho dulce, galletas, bizcocho, \n",
      "SEDENTARISMO, DESORDEN HORARIO\n",
      "A RAÍZ DE PANDEMIA + VERANO PORQUE TENIA ANSIEDAD (esta en paro) Y SE ABURRÍA\n",
      "Le cuesta mantener el peso. \n",
      "Los años \n",
      "DEJO DE FUMAR\n",
      "POCO A POCO A PARTIR DE LOS 40 AÑOS\n",
      "CONFINAMENTO - MUITO TEMPO EM CASA E TER PARADO A AF. ACHA QUE NÃO COME MAL MAS GOSTA DE DOCES E NÃO TEM TIDO O CUIDADO DE EVITAR OS DOCES.\n",
      "COMIAN MAL CON EVENTOS \n",
      "ANSIEDAD\n",
      "VERANO + COVID\n",
      "HABITOS EN GENERAL. CONFINAMIENTO, ESTILO DE VIDA, MENOPAUSIA \n",
      "COMER A MAIS\n",
      "CONFINAMENTO + DIVÓRCIO\n",
      "JUBILADA EN LA PANDEMIA Y AUMENTO MUCHO PESO\n",
      "\n",
      "\n",
      "\n",
      "menopausia, dejar de fumar...\n",
      "PESO HABITUAL RONDAVA OS 65KG\n",
      "COMEÇOU A GANHAR PESO NA ALTURA DA PANDEMIA. TAMBÉM DEIXOU DE FUMAR NESTA ALTURA. ELEVADA INGESTÃO DE PRODUTOS AÇUCARADOS. HISTÓRICO MARCADO DE DIETAS IO-IO.\n",
      "LOS ULTIMOS 2 AÑOS, SU PADRE FALLECIÓ Y MUCHA ANSIEDAD\n",
      "MUCHO DESORDEN\n",
      "CAMBIO DE HÁBITOS DURANTE LOS ULTIMOS 5 AÑOS + SOCIABILIZACION\n",
      "STRESS, VIAJA MUCHO, COMER MUY DESORDENADO\n",
      "MATERNIDAD \n",
      "TIENEN MUCHA VIDA SOCIAL TOMA CERVEZA Y MARTINI, \n",
      "SEMPRE TEVE EXCESSO DE PESO. NÃO TEM PESO HABITUAL. PESO MUITO IO-IO, EM PROCESSOS DE EMAGRECIMENTO CONTÍNUOS. BANDA GÁSTRICA EM 2009 (CHEGOU AOS 73KG), REMOVEU EM FEVEREIRO DE 2022 DEVIDO A ESOFAGITE DE REFLUXO.\n",
      "Ansiedad\n",
      "Tendencia a coger peso\n",
      "Dejar de fumar\n",
      "Dos embarazos\n",
      "Embarazos ha notado cambios y le cuesta perder ese peso. Le gusta dulce \n",
      "2 GRAVIDEZ + DESCONTROLO EMOCIONAL QUE A LEVOU A COMPENSAR COM A COMIDA\n",
      "NO FINAL DA 1ª GRAVIDEZ FEZ DIETA 3 PASSOS E RECUPEROU O PESO. APÓS A 2ª GRAVIDEZ NUNCA CONSEGUIU BAIXAR DOS 60KG FICANDO DEPOIS COM OS 64KG E DEPOIS COM OS PROBLEMAS NO TRABALHO GANHOU MAIS 4 KG. TEVE TAMBÉM O FALECIMENTO DOS PAIS QUE TAMBÉM NÃO AJUDOU. \n",
      "LE CUESTA MANTENER UN PESO ESTABLE. TIENE MUCHOS ANSIEDAD \n",
      "Dos embarazos\n",
      "Estrés\n",
      "Siempre ha sido \"gordita\"\n",
      "Sedentarismo\n",
      "Malos hábitos\n",
      "Estrés en el trabajo\n",
      "DEPRESIÓN Y ANSIEDAD \n",
      "ANSIEDADE - MEDICADA\n",
      "NOTA QUE FICA COM ANSIAS DE COMER DOCES\n",
      "DEIXOU DE FAZER DESPORTO\n",
      "EM OUT/2022 INICIOU ACOMPANHAMENTO POR NUTRICIONISTA NA CUF (PESO INICIAL 120KG), PERDENDO ATÉ AGORA CERCA DE 9KG.\n",
      "O PESO HABITUAL MAIS RECENTE RONDAVA OS 120KG. DESDE OS 17 ANOS QUE NÃO PESA QUE MENOS QUE 100KG.\n",
      "ESTRES Y ANSIEDAD\n",
      "ANSIEDAD, ESTRES , FALTA DE MOVILIDAD \n",
      "LE GUSTA COMER BIEN, DURANTE UN TIEMPO SEDENTARISMO\n",
      "HIZO LA DIETA HACE 10 AÑOS\n",
      "PESO HABITUAL RONDA OS 78KG\n",
      "COMEÇOU A AUMENTAR PESO DE HÁ 1 ANO PARA AGORA. DESLEIXO ALIMENTAR.\n",
      "JÁ PERDEU 4KG NO ULTIMO MÊS COM CUIDADOS ALIMENTARES PRÓPRIOS.\n",
      "\n",
      "MUDANÇA P SUIÇA E AUMENTO DE 16KG NUM ANO. TB TEVE BURNOUT QUANDO COMEÇOU A AUMENTAR EM 2021. DEIXOU TABACO EM 2020 E TB ASSOCIA UM POUCO A ISSO O AUMENTO DE PESO. DOCES\n",
      "PROBLEMAS PERSONALES MUERTE DE UN FAMILIAR \n",
      "ANSIEDAD \n",
      "PAROU DE FUMAR HA 2 ANIOS E TEVE DE PARAR AF HA 6 MESES PORQUE TORCEU OS 2 PÉS (INFLITRAÇOES COM CORTISONA E GEL HIALURONICO). DIFICL DE PARAR DE COMER MESMO SABENDO QUE TA CHEIA E PODE FICAR MAL. APTENCIA PARA DOCES. FAZ PA, ALM E JANTAR. COME MT AO JANTAR PORQUE PASSA H SEM COMER A TARDE. AS VEZES N ALMOÇA TB\n",
      "Malos hábitos. Dos años sin trabajar por la pandemia, ansiedad y depresión.\n",
      "TODA SU VIDA HA TENIDO SOBREPESO. \n",
      "comer mucho desorden, estrés\n",
      "la comida más grande es la cena\n",
      "EMBARAZO, BUENO HABITOS Y COME SANO\n",
      "TIENE ANTOJOS \n",
      "DIAGNOSTICADA COM LIPEDEMA - NOTA QUE QUANDO AS PERNAS FICARAM MAIS INFLAMADAS O PESO SUBIU\n",
      "PICOTEA ENTRE HORAS, MUCHO DESORDEN HORARIO\n",
      "MALOS HÁBITOS\n",
      "MENOPAUSIA\n",
      "Hacia mucho deporte y por lesión\n",
      "Come con estrés y fuera de casa por trabajo \n",
      "TRABAJO MAS SEDENTARIO, ANSIEDAD\n",
      "Influyen las emociones a la hora de comer\n",
      "HIZO LA DIETA HACE MUCHOS AÑOS\n",
      "DESARREGLOS EN LA ALIMENTACION, SEDENTARISMO, PROBLEMA DE ESPALDA\n",
      "CANCER DE MAMA HACE 6 AÑOS, MEDICACION, DEPRESION\n",
      "AUN NO TIENE ALTA ONCOLOGICA\n",
      "EMBARAZO SEGUIDOS \n",
      "sedentarismo\n",
      "COMER DE MAS, SEDENTARISMO\n",
      "SIEMPRE SOBREPESO, HACE 2 AÑOS DUELO, LA ECHARON DEL TRABAJO\n",
      "EDAD, MALA ALIMENTACION\n",
      "SEDENTARISMO + NOTA QUE TEM MAIS DIFICULDADE EM PERDER PESO\n",
      "MENOPAUSA + IDADE\n",
      "VIDA MAIS SEDENTÁRIA\n",
      "COVID \n",
      "EMBARAZO, VIDA SEDENTARIA, ANSIEDAD \n",
      "ESTÁ A TOMAR ANTIDEPRESSIVOS + VITAMINAS. DIZ QUE ANSIEDADE E NERVOSISMO DÁ-LHE PARA COMER SÓ PORCARIAS. \n",
      "malos hábitos, picotea últimamente\n",
      "PAULATINO, CADA AÑO IBA SUBIENDO DE PESO\n",
      "TRABAJO SEDENTARIO\n",
      "DESORDEN PICOTEO\n",
      "ABORTO Y A RAÍZ DE AHÍ MALOS HABITOS ALIMENTARIOS\n",
      "PICOTEA ENTTRE HORAS: GALLETAS, CHOCOLATE\n",
      "MENOPAUSIA, STRESS, COME DESORGANIZADO\n",
      "PESO HABITUAL RONDAVA OS 65-68KG\n",
      "DEPOIS DE SER MÃE (HÁ 11 ANOS) ESTABILIZOU NOS 70KG; NÃO ATRIBUI NENHUMA CAUSA AO EXCESSO DO PESO, DIZ QUE SEMPRE FOI MAGRA E TEVE PERCENTIL ABAIXO DO PESO EM CRIANÇA.\n",
      "sedentaria, se salta comidas\n",
      "HIPOTIROIDISMO HACE 3 AÑOS\n",
      "ANSIEDAD - COMER DULCE\n",
      "TRABALHO MUITO SEDENTÁRIO. DEIXOU DE PRATICAR DESPORTO COM OS AMIGOS (JOGAVA FUTEBOL 2X/SEMANA). ESTÁ A CONSTRUIR A SUA CASA E DIZ QUE DESDE QUE INICIOU COME MAIS. \n",
      "ACHA QUE TEM ALGUMA CARGA GENÉTICA PARA TER MAIS PESO PORQUE O PAI ERA OBESO E DIABETICO TIPO 1. SEMPRE TEVE CUIDADO COM DOCES. AGORA DESDE O INICIO DA PANDEMIA QUE ESTÁ MAIS PARADA E NOTA QUE ANTES CONSEGUIA EMAGRECER E MANTÊ-LO E ULTIMAMENTE NÃO CONSEGUE PERDER E MUITO MENOS MANTER\n",
      "VIAJA MUCHO, SEDENTARISMO, JETLAG\n",
      "DEJAR DE FUMAR\n",
      "FALTA DE CUIDADO ALIMENTAR\n",
      "VIDA MAIS SEDENTÁRIA\n",
      "MENOPAUSIA\n",
      "HA HECHO VARIOS TIPOS DE DIETA (KETO, HERBALIFE,...), ATRACONES....\n",
      "MENOPAUSIA, CAMBIO HORMONAL\n",
      "AUMENTO DE PESO LENTO Y GRADUAL\n",
      "EMBARAZOS\n",
      "PROBLEMA FAMILIAR, COMIDAS DESORGANIZADAS\n",
      "ha comido mal durante muchos años. Ha hecho otras dietas. \n",
      "HACE TIEMPO HACIENDO DIETA Y NO CONSIGUE BAJAR DE PESO\n",
      "AUMENTO MUY PROGRESIVO DESDE HACE 2 AÑOS\n",
      "DEPRESION \n",
      "FOME EMOCIONAL, GULA.\n",
      "PESO ACIMA DOS 70KG JÁ HÁ UNS 3 ANOS\n",
      "PESO HABITUAL 68-69KG.\n",
      "GRAVIDEZ - AUMENTOU DE PESO E NÃO CONSEGUIU PERDER \n",
      "DESORDEN Y MAL COMEDORA, NO LE GUSTAN LAS VERDURAS, NO TOMA LEGUMBRES\n",
      "Otras dietas\n",
      "Situación familiar complicada a nivel anímico\n",
      "mucho picoteo por aburrimiento\n",
      "METABOLISMO LENTO + OBSTIPADA + SEDENTÁRIA + É PREGUIÇOSA A BEBER ÁGUA\n",
      "Descontrol horario + gran comedor\n",
      "Medico que trabaja en urgencias y sus horarios son indefinidos \n",
      "Rotura de meniscos y tendones por accedente de tráfico. \n",
      "NÃO FAZ IDEIA. DIZ QUE JÁ É DOUTORADA EM DIETAS E O PROBLEMA É O IO IO CONSTANTE DE PESO. VAI FAZER ANALISES PARA PERCEBER O QUE SE PASSA MAS JÁ SABE QUE TEM UM QUISTO NA TIROIDE, TRABALHO MUITO SEDENTÁRIO. \n",
      "DESDE LOS 12 AÑOS HA TENIDO PROBLEMAS DE PESO.\n",
      "COME DE FORMA DESEQUILIBRADA\n",
      "MALA RELACIÓN CON LA COMIDA\n",
      "MUCHO CAFES DESCAFEINADOS, MUCHO DESPORDEN HORARIOS - CENA MUCHA ANSIEDAD Y SE DA EL ATRACON - POR EL DULCE\n",
      "trastornos por atracon - ingiere cantidades muy grandes a diario\n",
      "COMER NÃO EM QUANTIDADE MAS EM QUALIDADE. COMER MUITO PÃO E DOCES . MUITO GULOSA\n",
      "PANDEMIA MAS DIZ QUE TEM ABUSADO NA ALIMENTAÇÃO E NÃO TEM FEITO AF\n",
      "NÃO QUIS TRAÇAR OBJETIVOS QUE NÃO CONSEGUE CUMPRIR. QUER IR FAZENDO O TTO SEM EXPECTATIVAS CONCRETAS DE PERDA.\n",
      "PESO HABITUAL TEM RONDADO O PESO ATUAL\n",
      "PESO MINIMO EM IDADE ADULTA 90KG\n",
      "PESO HABITUAL 86KG\n",
      "PESO MÁXIMO=118KG\n",
      "COMER MUCHO Y NO MOVERSE NADA\n",
      "come segun ella bien, se siente saciada\n",
      "UMA ALTURA EM QUE ESTEVE MUITO SEDENTÁRIA - ESTAVA A FAZER TESE DE MESTRADO - 2 ANOS\n",
      "En la familia todo gira en torno a la comida\n",
      "Al venir a vivir a España, menos acceso a fruta y verdura pq están mas malas.\n",
      "Otras dietas sin mantenimiento.\n",
      "MUCHO STRESS\n",
      "DEPOIS DO NASCIMENTO DO ULTIMO FILHO (HÁ 11 ANOS) COMEÇOU A DESCAMBAR E COM A MENOPAUSA NÃO AJUDOU\n",
      "Sobrepeso toda la vida. \n",
      "Ha hecho mil dietas\n",
      "Febrero ha tenido un pico de estres laboral\n",
      "NÃO SABE. ACHA QUE SÃO VÁRIOS FATORES. MUITO STRESS E COM MUITOS PRAZOS NO TRABALHO E NOTA QUE A COMIDA É UM POUCO O SEU REFUGIO\n",
      "ALIMENTAÇÃO DESIQUILIBRADA. SEDENTARISMO. STRESS LABORAL.\n",
      "INICIOU DIETA NO ANO PASSADO POR AUTO RECREAÇÃO PERDENDO JÁ 24KG QUE NÃO VOLTOU A RECUPERAR. GOSTAVA DE PERDER MAIS 30KG.\n",
      "SEMPRE FOI MAGRINHA (PESO HABITUAL 58-60KG)\n",
      "PESO MAX = PESO ATUAL =90KG\n",
      "COMEÇOU A AUMENTAR PESO QUANDO ENTROU NA FACULDADE E QUANDO DEIXOU DE FAZER DESPORTO (GINÁSTICA DE COMPETIÇÃO - TREINOS 3H/DIA).\n",
      "TRABALHA ENTRE 10-12H/DIA.\n",
      "Sedentarismo, menopausia, tiene picos de ansiedad, desorden en las comidas por el trabajo. \n",
      "DESDE LA IQ\n",
      "TRABAJA DESDE 6.30H A 19.30H!!!!\n",
      "tiene un restaurante suyo - suele comer en el restaurante\n",
      "En general un poco de todo, mal comer picotea entre horas \n",
      "Le gusta mucho la lecha 1 lt  de leche al día semi \n",
      "COMER DESORDENADAMENTE\n",
      "2 EMBARAZOS Y ANSIEDAD CON LA COMIDA 7 MESES DE LA ULTIMA \n",
      "OVARIOS POLIQUISTICOS, RESISTENCIA A LA INSULINA. \n",
      "STRESS + COMIDA + COMPENSAÇÕES COM A COMIDA\n",
      "GOSTA DE COMER E AO FIM DE SEMANA BEBE UM VINHO E/OU CERVEJA\n",
      "CONFINAMIENTO - COGIÓ 17 KG\n",
      "NUNCA HA PESADO TANTO COMO AHORA\n",
      "TIENE UNA VIDA MUY ESTRESANTE, Y CUANDO LLEGA LA NOCHE COME MUCHISIMO\n",
      "CAMBIO DE PAREJA, \n",
      "POR LA PANDEMIA, ANTES IBA AL GYM\n",
      "NÃO SABE BEM. DIZ QUE É MUITO STRESSADA E ANSIOSA E NÃO TEM FOME MAS SIM VONTADE DE COMER\n",
      "PESO HABITUAL 76-80KG\n",
      "PESO MÁXIMO 80KG\n",
      "PESO MIN 60KG \n",
      "QUANDO ENGRAVIDOU AUMENTOU 25KGS E NUNCA MAIS RECUPEROU. 2 GRAVIDEZES.\n",
      "DESCONTROLO HORÁRIO E CANSAÇO. FALTA DE DESCANSO IMPULSIONA MAUS HÁBITOS ALIMENTARES.\n",
      "MUCHA ANSIEDAD POR LA NOCHE\n",
      "ES MADRE SOLTERA DE 2 NIÑOS DE 6 AÑOS\n",
      "Nunca ha estado en el peso ideal \n",
      "mucho picoteo por las noches (vino, jamón, queso, patatas)\n",
      "PANDEMIA COM AUMENTO DO SEDENTARISMO\n",
      "ANSIEDAD POR MENOPAUSIA\n",
      "PESO MÁXIMO EM IDADE ADULTA 82KG (MUITO RECENTEMENTE ANTES DE INICIAR ACOMPANHAMENTO DE NUTRICIONISTA)\n",
      "PESO HABITUAL RONDA 68-70KG\n",
      "COMEÇOU A AUMENTAR DE PESO GRADUALMENTE DESDE HÁ 3 ANOS ATRÁS. DESIQUILIBRIO ALIMENTAR. DEFICIENTE FRACIONAMENTO ALIMENTAR.\n",
      "IDADE + MENOPAUSA\n",
      "ESTRES COME LO QUE LE APETECE FUERA DE TIEMPO. \n",
      "PROGRESIVO DESDE HACE 3 AÑOS, DEJÓ LA AF. TRABAJA CON MÁS GUARDIAS Y COME DE GLOVO\n",
      "SEMPRE TEVE EXCESSO DE PESO\n",
      "PESO MINIMO EM IDADE ADULTA 70KGS. ALTERAÇÕES DE ROTINA, MUDANÇAS DE TRABALHO.\n",
      "VÁRIAS TENTATIVAS DE DIETA SEM SUCESSO. FAZ PSICOTERAPIA PARA TRATAR COMPULSÃO ALIMENTAR.\n",
      "comer capricho en las meriendas, comer fuera de casa a diario y elegir por placer\n",
      "Muere un familiar y esta cuidado a otro\n",
      "SU OBJ ES 76KG, 67KG ES EL DE LA DRA\n",
      "NO MANTENIMIENTO, DEJAR DE FUMAR HACE 2 AÑOS\n",
      "Menopausia\n",
      "Estres\n",
      "Estrés, trabaja sedentario. \n",
      "NÃO FAZ IDEIA. FOI UM AUMENTO DE PESO GRADUAL DESDE OS 18 ANOS\n",
      "IDADE + TIROIDE + FALTA DE AF\n",
      "PESO HABITUAL 64-65KG\n",
      "DIZ SER MUITO ANSIOSA E POR ISSO TEM EPISÓDIOS DE COMPULSÃO ALIMENTAR FREQUENTES (GERALMENTE AO FINAL DO DIA - ENTRE AS 17H-20H00).\n",
      "COMER MAL, SIN HORARIOS, \n",
      "Embarazo\n",
      "Perdió mucho con Naturhouse\n",
      "Dos embarazos\n",
      "Malos hábitos\n",
      "Ansiedad\n",
      "Depresión\n",
      "Después del embarazo subió mucho de peso, quiere perder hasta llegar a los 80\n",
      "SEDENTARIO Y ANSIEDAD\n",
      "Abortos y embarazos \n",
      "Hipotiroidismo y premenopausia falta de ejercicio. Tiene alimentación saludable. \n",
      "Cambio de hábitos en salud y ejercicio. \n",
      "Mucha vida social y ansiedad\n",
      "VIDA SEDENTÁRIA + NÃO TER HORAS REGULARES PARA AS REFEIÇÕES. FOI MÃE HÁ 4 ANOS E NÃO CONSEGUIU BAIXAR MAIS\n",
      "PICOTEO FRUTOS SECOS, YOGURES, DESPUES DE CENAR LECHE CON CACAO\n",
      "FAZIA CAMINHADAS 8KM A 12KM TODOS OS DIAS ANTES DA PANDEMIA E DEPOIS DA PANDEMIA FICOU MAIS SEDENTÁRIO\n",
      "PICOTEO POR LAS TARDES - FRUTOS SECOS, PATATAS - ANSIEDAD POR COMER DE CARA A LA MAÑANA\n",
      "Cambio de rutina por el trabajo\n",
      "COME MUITO E TEM MUITA FOME. ADORA DOCES E É MUITO SEDENTÁRIA\n",
      "Menopausia y desestabilización de la tiroides.\n",
      "Fanta o coca cola el fin de semana \n",
      "AUMENTO PRGRASIVO DESDE LA PANDEMIA\n",
      "AUMENTO PROGRESIVO - TERCER EMBARAZO SUBIO MUCHO DE PESO - NO SE HA CUIDADO NUNCA Y COME CANTIDADES\n",
      "Le gusta comer y beber vino \n",
      "Siempre sobrepeso\n",
      "Sedentarismo 9h sentada\n",
      "Malos hábitos - hambre emociona- picoteo\n",
      "Pandemia, dejo de hacer ejercicio. \n",
      "Viaja mucho\n",
      "Malos hábitos\n",
      "Perimenopausia\n",
      "2 EMBARAZOS \n",
      "menopausia, mucho stress\n",
      "Desorden de comidas\n",
      "ANTES HACIA MUCHO DEPORTE, COME MUCHO Y LO QUE LE GUSTA \n",
      "Vida sedentaria \n",
      "PROGRESIVO: 2 EMBARAZOS DESCONTRALADOS\n",
      "REGLA IRREGULARES\n",
      "SEDENTARISMO - DEIXOU DE FAZER DESPORTO (HÁ 10 ANOS) + 2 GRAVIDEZES (1 5 ANOS E OUTRA DE 8 ANOS)\n",
      "GRAVIDEZ  (MENINO - 17 MESES) E REGRESSO AO TRABALHO\n",
      "PREMENOPAUSIA\n",
      "SEDENTARISMO + MALOS HORARIOS\n",
      "\n",
      "Sedentarismo, inicio de relación \n",
      "AUMENTO PROGRESIVO - DIETAS ANTERIORES CECTOGÉNICAS\n",
      "ATIVIDADE LABORAL - PROFESSOR UNIVERSITÁRIO - NÃO RESPEITA MUITO OS HORÁRIOS DE REFEIÇÕES E CUIDADO ALIMENTAR + ALCOOL AO JANTAR\n",
      "CORTISOL ALTO DESDE HACE 6 AÑOS, MUCHO ESTRES PERO NO LE AFECTA A LA MANERA DE COMER\n",
      "No sabe decirme\n",
      "MALCOMER\n",
      "HA PROBADO MUCHAS DIETAS, PERO LE CUESTA DESPUES HACE EL MANTENIMIENTO\n",
      "MUITO STRESS + IDADE \n",
      "ansiedad momento\n",
      "MORAVA FORA E NAS MUDANÇAS PARA PORTUGAL E MUDANÇA DE ROTINAS E SÓ AÍ GANHOU 10 KG E DEPOIS ENGRAVIDOU E CHEGOU AOS 95KG\n",
      "O SISTEMA NERVOSO SÓ LHE DÁ PARA COMER PORCARIAS\n",
      "FOI MÃE HÁ 4 ANOS MAS O PESO DESCAMBOU E TEM SIDO MUITO DIFICIL BAIXAR O PESO. ACHA QUE TAMBÉM ESTÁ NUMA FASE DE PRÉ-MENOPAUSA E DEPOIS PARECE QUE NÃO AJUDA\n",
      "NÃO SABE MESMO PORQUE DIZ QUE ATÉ TEM CUIDADOS ALIMENTARES MAS A DRA. FEZ ANÁLISES A NIVEL ENDÓCRINO ESTÁ TUDO BEM\n",
      "Embarazo hace 5 años + pandemia\n",
      "\n",
      "Muchas horas delante del PC\n",
      "Poco tiempo deporte\n",
      "Comidas fuera de casa\n",
      "malos habitos, desorden alimentario. Descontrol\n",
      "Come y no hace ejercicio \n",
      "Le cuesta desayunar y hace ayuno intermitente \n",
      "TIROIDITE DE HASHIMOTO E DEPOIS COM ESTA ATRITE REUMATOIDE NÃO AJUDOU E NÃO CONSEGUIU SAIR DESTE PESO\n",
      "MENOPAUSA (ACHA QUE CONTRIBUIU UM BOCADINHO) E NÃO SABE BEM O PORQUÊ\n",
      "MUITO TEMPO SENTADO EM CASA. ALGUMAS LIMITAÇÕES TAMBÉM A FAZER AF POR PROBLEMAS NA ANCA QUE IMPOSSIBILITA MUITAS AF.\n",
      "GRAVIDEZ\n",
      "NÃO SABE DIZER\n",
      "Mala alimentación, familia con sobrepeso. Desordenada no come en toda la mañana y come una vez o 2 a día. Cocina para la familia \n",
      "Endometriosis + histerectonia.\n",
      "3 invitro + embarazo \n",
      "Protesis de cadera, ya que antes hacía mucho deporte\n",
      "Comida rápida por falta de tiempo\n",
      "Mucho hidrato \n",
      "Dejar de fumar\n",
      "Premenopausia\n",
      "IDADE + COMER MUITO TARDE + SEDENTARISMO + GOSTA DE COMER\n",
      "FILHOS + ESTILO DE VIDA + DEIXOU DE PRATICAR DESPORTO\n",
      "Evolución del día a día \n",
      "Poco a poco ha aumentado de peso\n",
      "FOI JOGADOR DE FUTEBOL MAS COM 14 ANOS SOUBE QUE TINHA UM PROBLEMA NO CORAÇÃO E TEVE DE PARAR COM O EXERCICIO FISICO E DEPOIS COMO NÃO TINHA GRANDE CUIDADO NA ALIMENTAÇÃO FOI AUMENTANDO O PESO. \n",
      "Varias causas, deja de fumar, se casa, trabaja en eventos y teletrabajo\n",
      "INICIO DE VIDA PROFISSIONAL. FALTA DE TEMPO PARA FAZER AF + STRESS NO TRABALHO COM UMA ALIMENTAÇÃO POUCA CUIDADA\n",
      "PANDEMIA + DESLEIXO + SEDENTARISMO (TRABALHO)\n",
      "NOTA QUE ESTÁ MUITO INFLAMADA E MUITO INCHADA\n",
      "SEDENTARISMO (TELE TRABALHO) + ALIMENTAÇÃO\n",
      "COM A PANDEMIA HABITUOU-SE A ESTAR MUITO TEMPO SENTADA + IDADE\n",
      "DESDE OS 27 ANOS NOTOU QUE COMEÇOU A GANHAR 1KG POR ANO MAS ACHA QUE AGORA ULTIMAMENTE CHEGAR AOS 85KG FOI MAIS RÁPIDO\n",
      "IDADE + POUCA AF (PROBLEMA DE JOELHOS EM QUE TEVE DE PARAR COM A AF - TINHA MENOS 10KG/12KG) \n",
      "Trabajo (Come en restaurantes)\n",
      "Estrés laboral\n",
      "Ansiedad noches / No desayuna\n",
      "FOI ATLETA DE ALTA COMPETIÇÃO E QUANDO DEIXOU O DESPORTO NOTOU MUITAS MUDANÇAS. GOSTA MUITO DE COMER E DE IR COMER FORA. GRAVIDEZ - PERDEU LOGO 10KG NA SEMANA A SEGUIR AO PARTO E ATÉ CHEGOU AOS 68KG MAS DEPOIS COM A PANDEMIA O PESO DESCONTROLOU\n",
      "DEIXOU DE FUMAR HÁ 8 ANOS + MENOPAUSA + COME MUITA COISA QUE FAZ MAL. GOSTA MUITO DO PETISCO\n",
      "Deja de hacer ejercicio y comer desordenado. No influyen las emociones a la hora de comer \n",
      "Recupera peso en verano y luego le cuesta bajarlo \n",
      "ACHA QUE ESTE AUMENTO DE PESO FOI AUMENTO DE MM\n",
      "Trabajo sedentario \n",
      "UMA CIRURGIA DE REMOÇÃO DO UTERO EM 2021 E FOI A PARTIR DAÍ QUE O PESO FICOU DESCONTROLADO\n",
      "DESCONTROLO COMPLETO NA ALIMENTAÇÃO\n",
      "VIDA MUITO STRESS E COMPENSA AS EMOÇÕES COM A ALIMENTAÇÃO\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to the SQLite database\n",
    "db_path = 'C:\\\\Users\\\\Felhasználó\\\\Desktop\\\\Projects\\\\PNK_DB2\\\\paper1_emotional\\\\survival_analysis.sqlite'\n",
    "table_name = 'sa_input_table'\n",
    "\n",
    "# Query to fetch unique values from the weight_gain_cause column\n",
    "query = f\"SELECT DISTINCT weight_gain_cause FROM {table_name}\"\n",
    "\n",
    "# Execute the query and fetch results\n",
    "with sqlite3.connect(db_path) as conn:\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "    results = cursor.fetchall()\n",
    "\n",
    "# Extract and print the unique observations\n",
    "unique_weight_gain_causes = [row[0] for row in results if row[0] is not None]\n",
    "print(\"Unique observations in the weight_gain_cause column:\")\n",
    "for cause in unique_weight_gain_causes:\n",
    "    print(cause)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database exported to Excel at: C:\\Users\\Felhasználó\\Desktop\\Projects\\PNK_DB2\\paper1_emotional\\survival_analysis.xlsx\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the output Excel file path\n",
    "output_excel_path = os.path.join(paper1_directory, 'survival_analysis.xlsx')\n",
    "\n",
    "# Connect to the SQLite database\n",
    "with sqlite3.connect(DB_PATH) as conn:\n",
    "    # Get the list of all tables in the database\n",
    "    query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "    tables = pd.read_sql_query(query, conn)['name'].tolist()\n",
    "    \n",
    "    # Create a Pandas Excel writer\n",
    "    with pd.ExcelWriter(output_excel_path, engine='openpyxl') as writer:\n",
    "        # Loop through each table and save it as a sheet in the Excel file\n",
    "        for table in tables:\n",
    "            df = pd.read_sql_query(f\"SELECT * FROM {table}\", conn)\n",
    "            df.to_excel(writer, sheet_name=table, index=False)\n",
    "\n",
    "print(f\"Database exported to Excel at: {output_excel_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
